{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c876cf",
   "metadata": {},
   "source": [
    "# Curriculum_vs_NonCurriculum_MetaDrive_SB3_Experiments\n",
    "\n",
    "**Generated:** 2025-11-20T19:41:51.012806Z\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook contains a full, reproducible experiment pipeline for comparing **non-curriculum** vs **curriculum** reinforcement learning for autonomous driving using **MetaDrive** and **Stable Baselines3 (SB3)**. It includes:\n",
    "\n",
    "- Full environment factory and wrappers (including a discrete-action wrapper for DQN).\n",
    "- Exact stage definitions (C0..C3) and matching budgets.\n",
    "- Non-curriculum runner (train each target map separately for the same total sample budget).\n",
    "- Curriculum runner (C0→C1→C2→C3 sequential fine-tuning).\n",
    "- Evaluation harness (metrics logging, CSV saving, TensorBoard integration, video recording).\n",
    "- Hyperparameters and experiment folder conventions.\n",
    "\n",
    "**Note:** This notebook provides runnable code cells. It is designed to be run on a machine with MetaDrive installed and sufficient CPU/GPU. Heavy training cells are provided but **commented** or parametrized with small pilot budgets; uncomment or change budgets to run full experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b7750",
   "metadata": {},
   "source": [
    "## 0. Install dependencies (run once)\n",
    "\n",
    "Run in a terminal or notebook cell. If MetaDrive or SB3 is not installed, install them. On many systems you may need\n",
    "`pip install stable-baselines3[extra] gymnasium metadrive numpy pandas matplotlib tensorboard opencv-python`\n",
    "\n",
    "**Important:** Do not run this on environments without internet access; install packages beforehand if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3[extra] gymnasium metadrive numpy pandas matplotlib tensorboard opencv-python\n",
    "# If you already have the deps installed, comment the line above out.\n",
    "print('Install packages if needed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e161f",
   "metadata": {},
   "source": [
    "## 1. Imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fb189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SB3 imports\n",
    "from stable_baselines3 import PPO, SAC, DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# MetaDrive import guard (user must have installed)\n",
    "try:\n",
    "    from metadrive.envs.metadrive_env import MetaDriveEnv\n",
    "except Exception as e:\n",
    "    MetaDriveEnv = None\n",
    "    print('MetaDrive import failed - ensure metadrive is installed in the runtime if you plan to run MetaDrive experiments.')\n",
    "\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3da0a2",
   "metadata": {},
   "source": [
    "## 2. Experiment configuration (maps, stages, budgets, hyperparameters, folder layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Maps, stages and budgets\n",
    "STAGES = [\n",
    "    (\"C0_Straight\", \"Straight\", 0.0, 200_000),\n",
    "    (\"C1_Curve\",    \"Curve\",    0.0, 300_000),\n",
    "    (\"C2_Roundabout\",\"Roundabout\",0.0,400_000),\n",
    "    (\"C3_Dynamic\",  \"20-block\", 0.3, 400_000),\n",
    "]\n",
    "TOTAL_CURRICULUM_BUDGET = sum([s[3] for s in STAGES])\n",
    "print('Total curriculum budget (per algorithm) =', TOTAL_CURRICULUM_BUDGET)\n",
    "\n",
    "# Folder convention\n",
    "EXPERIMENT_ROOT = Path('experiments')\n",
    "EXPERIMENT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# Seeds and workers\n",
    "SEEDS = [0,1,2]\n",
    "N_ENVS = 8\n",
    "EVAL_FREQ = 10_000\n",
    "EVAL_EPISODES = 10\n",
    "\n",
    "# Held-out test map\n",
    "HELDOUT_MAP = (\"Fork\", 0.2)\n",
    "\n",
    "# Hyperparameters (consistent)\n",
    "HYPERS = {\n",
    "    'PPO': {\n",
    "        'policy':'MlpPolicy', 'policy_kwargs':{'net_arch':[64,64]}, 'learning_rate':3e-4,\n",
    "        'n_steps':2048, 'batch_size':64, 'n_epochs':10, 'gamma':0.99, 'clip_range':0.2\n",
    "    },\n",
    "    'SAC': {\n",
    "        'policy':'MlpPolicy', 'policy_kwargs':{'net_arch':[256,256]}, 'learning_rate':3e-4,\n",
    "        'batch_size':256, 'buffer_size':100_000, 'gamma':0.99\n",
    "    },\n",
    "    'DQN': {\n",
    "        'policy':'MlpPolicy', 'policy_kwargs':{'net_arch':[64,64]}, 'learning_rate':1e-4,'buffer_size':50_000,'batch_size':32,'train_freq':4\n",
    "    }\n",
    "}\n",
    "\n",
    "print('Config ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e299413",
   "metadata": {},
   "source": [
    "## 3. Environment wrappers\n",
    "Includes a discrete-action wrapper for DQN (maps discrete indices -> continuous steer/throttle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0cb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, mapping):\n",
    "        super().__init__(env)\n",
    "        self.mapping = mapping\n",
    "        self.action_space = spaces.Discrete(len(mapping))\n",
    "\n",
    "    def action(self, action):\n",
    "        return np.array(self.mapping[action], dtype=np.float32)\n",
    "\n",
    "print('Wrapper defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2b81f",
   "metadata": {},
   "source": [
    "## 4. Environment factory / vectorized env helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def make_metadrive_env(map_name, traffic_density=0.0, use_discrete=False, seed=0, render=False):\n",
    "    def _init():\n",
    "        cfg = {\n",
    "            'map': map_name,\n",
    "            'traffic_density': traffic_density,\n",
    "            'use_render': False,\n",
    "            'start_seed': seed,\n",
    "            'random_spawn': True,\n",
    "            'debug': False,\n",
    "        }\n",
    "        env = MetaDriveEnv(cfg)\n",
    "        # Optionally wrap for DQN\n",
    "        if use_discrete:\n",
    "            mapping = [(-1.0,0.0),(-1.0,0.3),(0.0,0.5),(1.0,0.3),(1.0,0.0)]\n",
    "            env = DiscreteActionWrapper(env, mapping)\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "def make_vec_env(map_name, traffic_density=0.0, n_envs=8, use_discrete=False, seed=0, parallel=False):\n",
    "    factories = [make_metadrive_env(map_name, traffic_density, use_discrete, seed+i) for i in range(n_envs)]\n",
    "    if parallel:\n",
    "        return SubprocVecEnv(factories)\n",
    "    else:\n",
    "        return DummyVecEnv(factories)\n",
    "\n",
    "print('Env factory ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d56696",
   "metadata": {},
   "source": [
    "## 5. Custom callback to log per-eval metrics (success rate, collisions, speed etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to compute and append evaluation metrics to CSV on each eval.\n",
    "    Assumes the eval_env yields episode info dicts with keys 'success','collision','avg_speed','traffic_violation'.\n",
    "    \"\"\"\n",
    "    def __init__(self, eval_env, out_csv, eval_episodes=10, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.out_csv = out_csv\n",
    "        self.eval_episodes = eval_episodes\n",
    "        self._cols = ['timestamp','total_timesteps','mean_reward','std_reward','success_rate','collision_rate','avg_speed','traffic_violations']\n",
    "        if not os.path.exists(out_csv):\n",
    "            pd.DataFrame(columns=self._cols).to_csv(out_csv, index=False)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def on_training_end(self):\n",
    "        pass\n",
    "\n",
    "    def record_eval(self, model, total_timesteps):\n",
    "        # run evaluation episodes and compute metrics\n",
    "        rewards = []\n",
    "        successes = []\n",
    "        collisions = []\n",
    "        speeds = []\n",
    "        traffic_viol = []\n",
    "        for ep in range(self.eval_episodes):\n",
    "            obs, _ = self.eval_env.reset()\n",
    "            done = False\n",
    "            ep_r = 0.0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = self.eval_env.step(action)\n",
    "                ep_r += reward\n",
    "                done = terminated or truncated\n",
    "            rewards.append(ep_r)\n",
    "            info_ep = info if isinstance(info, dict) else {}\n",
    "            # fallback if env does not provide keys\n",
    "            successes.append(info_ep.get('success', 1.0 if ep_r>0 else 0.0))\n",
    "            collisions.append(info_ep.get('collision', 0.0))\n",
    "            speeds.append(info_ep.get('avg_speed', info_ep.get('speed', 0.0)))\n",
    "            traffic_viol.append(info_ep.get('traffic_violation', 0.0))\n",
    "\n",
    "        mean_r = np.mean(rewards)\n",
    "        std_r = np.std(rewards)\n",
    "        success_rate = np.mean(successes)\n",
    "        collision_rate = np.mean(collisions)\n",
    "        avg_speed = np.mean(speeds)\n",
    "        tv = np.mean(traffic_viol)\n",
    "        row = {\n",
    "            'timestamp': time.time(), 'total_timesteps': total_timesteps, 'mean_reward': mean_r, 'std_reward': std_r,\n",
    "            'success_rate': success_rate, 'collision_rate': collision_rate, 'avg_speed': avg_speed, 'traffic_violations': tv\n",
    "        }\n",
    "        df = pd.read_csv(self.out_csv)\n",
    "        df = df.append(row, ignore_index=True)\n",
    "        df.to_csv(self.out_csv, index=False)\n",
    "        return row\n",
    "\n",
    "print('MetricsCallback defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303bc4f",
   "metadata": {},
   "source": [
    "## 6. Training functions: non-curriculum and curriculum runners\n",
    "\n",
    "These functions create models, attach callbacks, and run training. Each saves checkpoint, best-model and CSV metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ecb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(algo, env, hyperparams):\n",
    "    if algo == 'PPO':\n",
    "        model = PPO(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
    "                    policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
    "                    n_steps=hyperparams['n_steps'], batch_size=hyperparams['batch_size'], n_epochs=hyperparams['n_epochs'], gamma=hyperparams['gamma'])\n",
    "        return model\n",
    "    if algo == 'SAC':\n",
    "        model = SAC(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
    "                    policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
    "                    batch_size=hyperparams.get('batch_size',256), buffer_size=hyperparams.get('buffer_size',100000), gamma=hyperparams.get('gamma',0.99))\n",
    "        return model\n",
    "    if algo == 'DQN':\n",
    "        model = DQN(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
    "                    policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
    "                    buffer_size=hyperparams.get('buffer_size',50000), batch_size=hyperparams.get('batch_size',32), train_freq=hyperparams.get('train_freq',4))\n",
    "        return model\n",
    "    raise ValueError('Unknown algo')\n",
    "\n",
    "\n",
    "def train_noncurriculum(algo, map_name, traffic, total_timesteps, seed, n_envs=N_ENVS):\n",
    "    out_dir = EXPERIMENT_ROOT/f\"{algo}/noncurriculum/seed_{seed}/{map_name}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print('Training non-curriculum:', algo, map_name, 'seed', seed)\n",
    "\n",
    "    use_discrete = (algo=='DQN')\n",
    "    env = make_vec_env(map_name, traffic_density=traffic, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
    "    eval_env = make_vec_env(map_name, traffic_density=traffic, n_envs=1, use_discrete=use_discrete, seed=seed+100)\n",
    "    eval_env = eval_env.env_fns[0]() if hasattr(eval_env, 'env_fns') else eval_env\n",
    "\n",
    "    model = make_model(algo, env, HYPERS[algo])\n",
    "\n",
    "    # callbacks\n",
    "    eval_cb = EvalCallback(eval_env, best_model_save_path=str(out_dir/'best_model'), log_path=str(out_dir/'eval_logs'), eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "    ckpt_cb = CheckpointCallback(save_freq=EVAL_FREQ, save_path=str(out_dir/'checkpoints'), name_prefix='ckpt')\n",
    "    metrics_csv = out_dir/'metrics.csv'\n",
    "    metrics_cb = MetricsCallback(eval_env, str(metrics_csv), eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "    # train\n",
    "    model.learn(total_timesteps=total_timesteps, callback=[eval_cb, ckpt_cb])\n",
    "    model.save(str(out_dir/'model.zip'))\n",
    "\n",
    "    # held-out eval\n",
    "    held_map, held_traffic = HELDOUT_MAP\n",
    "    held_env = make_vec_env(held_map, traffic_density=held_traffic, n_envs=1, use_discrete=use_discrete, seed=seed+500)\n",
    "    held_env = held_env.env_fns[0]() if hasattr(held_env, 'env_fns') else held_env\n",
    "    mean_reward, std_reward = evaluate_policy(model, held_env, n_eval_episodes=100)\n",
    "    pd.DataFrame([{'mean_reward':mean_reward,'std_reward':std_reward}]).to_csv(out_dir/'heldout_metrics.csv', index=False)\n",
    "    print('Non-curriculum training complete and saved to', out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def train_curriculum(algo, seed, n_envs=N_ENVS):\n",
    "    out_base = EXPERIMENT_ROOT/f\"{algo}/curriculum/seed_{seed}\"\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "    print('Starting curriculum training for', algo, 'seed', seed)\n",
    "\n",
    "    use_discrete = (algo=='DQN')\n",
    "    # initialize model on C0\n",
    "    stage0 = STAGES[0]\n",
    "    _, map0, traffic0, steps0 = stage0\n",
    "    vec_env = make_vec_env(map0, traffic_density=traffic0, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
    "    model = make_model(algo, vec_env, HYPERS[algo])\n",
    "\n",
    "    cumulative = 0\n",
    "    for stage_name, map_name, traffic, stage_steps in STAGES:\n",
    "        out_dir = out_base/stage_name\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print('Curriculum stage:', stage_name, 'map:', map_name)\n",
    "\n",
    "        # replace env\n",
    "        try:\n",
    "            vec_env.close()\n",
    "        except:\n",
    "            pass\n",
    "        vec_env = make_vec_env(map_name, traffic_density=traffic, n_envs=n_envs, use_discrete=use_discrete, seed=seed+hash(stage_name)%100)\n",
    "        model.set_env(vec_env)\n",
    "\n",
    "        # eval env\n",
    "        eval_env = make_vec_env(map_name, traffic_density=traffic, n_envs=1, use_discrete=use_discrete, seed=seed+100)\n",
    "        eval_env = eval_env.env_fns[0]() if hasattr(eval_env, 'env_fns') else eval_env\n",
    "\n",
    "        eval_cb = EvalCallback(eval_env, best_model_save_path=str(out_dir/'best_model'), log_path=str(out_dir/'eval_logs'), eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
    "        ckpt_cb = CheckpointCallback(save_freq=EVAL_FREQ, save_path=str(out_dir/'checkpoints'), name_prefix='ckpt')\n",
    "\n",
    "        model.learn(total_timesteps=stage_steps, reset_num_timesteps=False, callback=[eval_cb, ckpt_cb])\n",
    "        model.save(str(out_dir/'stage_model.zip'))\n",
    "        cumulative += stage_steps\n",
    "\n",
    "    # heldout eval\n",
    "    held_map, held_traffic = HELDOUT_MAP\n",
    "    held_env = make_vec_env(held_map, traffic_density=held_traffic, n_envs=1, use_discrete=use_discrete, seed=seed+500)\n",
    "    held_env = held_env.env_fns[0]() if hasattr(held_env, 'env_fns') else held_env\n",
    "    mean_reward, std_reward = evaluate_policy(model, held_env, n_eval_episodes=100)\n",
    "    pd.DataFrame([{'mean_reward':mean_reward,'std_reward':std_reward}]).to_csv(out_base/'heldout_metrics.csv', index=False)\n",
    "\n",
    "    print('Curriculum finished and saved under', out_base)\n",
    "    return out_base\n",
    "\n",
    "print('Training functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83889cd2",
   "metadata": {},
   "source": [
    "## 7. Visualization helpers (plot metrics, learning curves, and display videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e24184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(csv_path, title=None):\n",
    "    if not os.path.exists(csv_path):\n",
    "        print('CSV not found:', csv_path); return\n",
    "    df = pd.read_csv(csv_path)\n",
    "    fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
    "    axs = axs.flatten()\n",
    "    axs[0].plot(df['total_timesteps'], df['mean_reward'], marker='o'); axs[0].set_title('Mean reward')\n",
    "    axs[1].plot(df['total_timesteps'], df['success_rate'], marker='o'); axs[1].set_title('Success rate')\n",
    "    axs[2].plot(df['total_timesteps'], df['collision_rate'], marker='o'); axs[2].set_title('Collision rate')\n",
    "    axs[3].plot(df['total_timesteps'], df['avg_speed'], marker='o'); axs[3].set_title('Avg speed')\n",
    "    if title: fig.suptitle(title)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "print('Plot helper ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f2875",
   "metadata": {},
   "source": [
    "## 8. Quick pilot run (toy budget) — uncomment to test\n",
    "\n",
    "Below is a small pilot run (very small budgets) to validate pipeline. **Do not use these tiny budgets for final experiments**. Use to test that your environment and SB3 setup works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilot example: run one tiny non-curriculum PPO for 2000 steps on Straight map\n",
    "# Uncomment to run\n",
    "\n",
    "# out = train_noncurriculum('PPO', 'Straight', 0.0, total_timesteps=2000, seed=0, n_envs=2)\n",
    "# print('Pilot saved at:', out)\n",
    "\n",
    "print('Pilot cell ready — uncomment to run a small test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6e5f6",
   "metadata": {},
   "source": [
    "## 9. Full experiment sweep orchestration (commented)\n",
    "\n",
    "This cell will launch the entire sweep: for each algorithm, for each seed, both non-curriculum and curriculum runs.\n",
    "**Careful**: this will run many long experiments — uncomment only when you are ready and have compute/time resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full sweep (uncomment to run)\n",
    "# algos = ['PPO','SAC','DQN']\n",
    "# for algo in algos:\n",
    "#     # non-curriculum: for each target map train for the total curriculum budget (to match sample budget)\n",
    "#     for seed in SEEDS:\n",
    "#         for (_, map_name, traffic, _) in STAGES:\n",
    "#             train_noncurriculum(algo, map_name, traffic, total_timesteps=TOTAL_CURRICULUM_BUDGET, seed=seed, n_envs=N_ENVS)\n",
    "#     # curriculum\n",
    "#     for seed in SEEDS:\n",
    "#         train_curriculum(algo, seed, n_envs=N_ENVS)\n",
    "\n",
    "print('Full sweep cell ready (commented)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8869d",
   "metadata": {},
   "source": [
    "## 10. Notes, reproducibility, and next steps\n",
    "\n",
    "- The notebook defines exact hyperparams, budgets, maps, seeds and logging conventions to ensure reproducible experiments.\n",
    "- Use Highway-Env for fast Phase 1 iteration; use MetaDrive for full MetaDrive experiments.\n",
    "- Always run a short pilot to measure `steps/sec` and set realistic budgets.\n",
    "- For visualization, use TensorBoard pointed at the `experiments` folder and the `plot_metrics()` helper.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**End of notebook.**\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
