{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15c876cf",
      "metadata": {
        "id": "15c876cf"
      },
      "source": [
        "# NonCurriculum_MetaDrive_SB3_Experiments\n",
        "\n",
        "This notebook contains a full, reproducible experiment pipeline for **non-curriculum** based reinforcement learning for autonomous driving using **MetaDrive** and **Stable Baselines3 (SB3)**. It includes:\n",
        "\n",
        "- Full environment factory and wrappers (including a discrete-action wrapper for DQN).\n",
        "- Exact stage definitions (C0..C3) and matching budgets.\n",
        "- Non-curriculum runner (train each target map separately for the same total sample budget).\n",
        "- Evaluation harness (metrics logging, CSV saving, TensorBoard integration, video recording).\n",
        "- Hyperparameters and experiment folder conventions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8b7750",
      "metadata": {
        "id": "8f8b7750"
      },
      "source": [
        "## 0. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y metadrive metadrive-simulator metadrive-simulator-py3-12 || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KG6c_nUg1EH",
        "outputId": "0f185698-778b-4070-f0e6-f3818f276b96"
      },
      "id": "9KG6c_nUg1EH",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping metadrive as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator-py3-12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bbe3fc75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe3fc75",
        "outputId": "053db2a5-dd25-4f69-c364-8cc790c6e4b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# !pip install stable-baselines3[extra] gymnasium metadrive numpy pandas matplotlib tensorboard opencv-python\n",
        "# !pip install stable-baselines3[extra] tensorboard opencv-python\n",
        "!pip install -q \"stable-baselines3[extra]\" \"metadrive-simulator-py3-12\" tensorboard opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import metadrive\n",
        "\n",
        "print(\"metadrive.__file__ :\", getattr(metadrive, \"__file__\", None))\n",
        "print(\"metadrive.__path__ :\", getattr(metadrive, \"__path__\", None))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDpNIf_ug8-1",
        "outputId": "74a2510b-900a-4254-fbef-bedb28247c94"
      },
      "id": "MDpNIf_ug8-1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadrive.__file__ : /usr/local/lib/python3.12/dist-packages/metadrive/__init__.py\n",
            "metadrive.__path__ : ['/usr/local/lib/python3.12/dist-packages/metadrive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/metadriverse/metadrive.git\n",
        "# %cd metadrive\n",
        "# !pip install -e .\n"
      ],
      "metadata": {
        "id": "vAvVvccaKQJQ"
      },
      "id": "vAvVvccaKQJQ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m metadrive.pull_asset\n"
      ],
      "metadata": {
        "id": "yD7V6MWtKbUn"
      },
      "id": "yD7V6MWtKbUn",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m metadrive.examples.profile_metadrive\n"
      ],
      "metadata": {
        "id": "-zr38W64KdZd"
      },
      "id": "-zr38W64KdZd",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content\n"
      ],
      "metadata": {
        "id": "Kf1vBH1BKfpt"
      },
      "id": "Kf1vBH1BKfpt",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import metadrive, inspect, os\n",
        "print(\"Using metadrive from:\", metadrive.__file__)\n",
        "print(\"Contents:\", os.listdir(os.path.dirname(metadrive.__file__)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXDForLMfJsl",
        "outputId": "f5f50ae8-d080-4765-997c-45c87c332b15"
      },
      "id": "ZXDForLMfJsl",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using metadrive from: /usr/local/lib/python3.12/dist-packages/metadrive/__init__.py\n",
            "Contents: ['component', 'utils', '__init__.py', '__pycache__', 'shaders', 'manager', 'pull_asset.py', 'render_pipeline', 'scenario', 'obs', 'constants.py', 'base_class', 'examples', 'third_party', 'engine', 'type.py', 'policy', 'envs', 'tests', 'version.py']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports"
      ],
      "metadata": {
        "id": "kKG8RN5zKYtb"
      },
      "id": "kKG8RN5zKYtb"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e81fb189",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e81fb189",
        "outputId": "976babec-dae4-4a05-fd19-6a86cd1041ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import logging\n",
        "\n",
        "# SB3 imports\n",
        "from metadrive.envs.metadrive_env import MetaDriveEnv\n",
        "from metadrive.envs.varying_dynamics_env import VaryingDynamicsEnv\n",
        "from stable_baselines3 import PPO, SAC, DQN\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# MetaDrive import guard\n",
        "# try:\n",
        "# from metadrive.envs.metadrive_env import MetaDriveEnv\n",
        "# except Exception as e:\n",
        "    # MetaDriveEnv = None\n",
        "    # print('MetaDrive import failed.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")                       # ignore everything\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", module=\"stable_baselines3\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n"
      ],
      "metadata": {
        "id": "PpACB8RcK8m8"
      },
      "id": "PpACB8RcK8m8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cc3da0a2",
      "metadata": {
        "id": "cc3da0a2"
      },
      "source": [
        "## 2. Experiment configuration\n",
        "setup: maps, stages, budgets, hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e386723b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e386723b",
        "outputId": "5855e707-8f12-47fd-f1a4-7e9683e8d556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total curriculum budget (per algorithm) = 650000\n"
          ]
        }
      ],
      "source": [
        "### Maps, stages and budgets\n",
        "# STAGES = [\n",
        "#     (\"C0_Straight\", \"Straight\", 0.0, 200_000),\n",
        "#     (\"C1_Curve\", \"Curve\", 0.0, 300_000),\n",
        "#     (\"C2_Roundabout\",\"Roundabout\",0.0,400_000),\n",
        "#     (\"C3_Dynamic\", \"20-block\", 0.3, 400_000),\n",
        "# ]\n",
        "\n",
        "### Maps, curriculum stages and budgets (with reward configs)\n",
        "\n",
        "STAGES = [\n",
        "    # C0: straight road, no traffic – just learn to go forward safely.\n",
        "    {\n",
        "        \"id\": \"C0\",\n",
        "        \"name\": \"C0_Straight\",\n",
        "        \"env_type\": \"general\",\n",
        "        \"map\": \"S\", # Straight\n",
        "        \"traffic\": 0.0,\n",
        "        \"budget\": 100_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,              # scale MetaDrive base reward\n",
        "            \"speed_w\": 0.05,            # weak speed shaping\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -5.0,\n",
        "            \"offroad_penalty\": -3.0,\n",
        "            \"traffic_violation_penalty\": -2.0,\n",
        "            \"success_bonus\": 10.0,\n",
        "            \"step_penalty\": -0.001,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C1: roundabout, no traffic – topology harder, still single-ego.\n",
        "    {\n",
        "        \"id\": \"C1\",\n",
        "        \"name\": \"C1_Roundabout\",\n",
        "        \"env_type\": \"general\",\n",
        "        \"map\": \"O\", # Roundabout\n",
        "        \"traffic\": 0.0,\n",
        "        \"budget\": 150_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.05,\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -6.0,  # slightly harsher for bad manoeuvres\n",
        "            \"offroad_penalty\": -4.0,\n",
        "            \"traffic_violation_penalty\": -3.0,\n",
        "            \"success_bonus\": 10.0,\n",
        "            \"step_penalty\": -0.005,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C2: 20-block PG map with **light traffic** – first exposure to traffic.\n",
        "    {\n",
        "        \"id\": \"C2\",\n",
        "        \"name\": \"C2_LightTraffic\",\n",
        "        \"map\": 10, # 20-block\n",
        "        \"traffic\": 0.05,               # light traffic\n",
        "        \"budget\": 200_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.08,            # encourage moving at speed in traffic\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -8.0,\n",
        "            \"offroad_penalty\": -6.0,\n",
        "            \"traffic_violation_penalty\": -4.0,\n",
        "            \"success_bonus\": 12.0,\n",
        "            \"step_penalty\": -0.01,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C3: same PG map with **dense traffic** – “multi-agent-ish” final stage.\n",
        "    # Still single learning ego, but many interacting vehicles (like CuRLA's\n",
        "    # higher-traffic final curriculum stage).\n",
        "    {\n",
        "        \"id\": \"C3\",\n",
        "        \"name\": \"C3_DenseTraffic\",\n",
        "        \"map\": 20,\n",
        "        \"traffic\": 0.30,               # dense traffic ≈ mild multi-agent\n",
        "        \"budget\": 200_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.10,\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -10.0, # strong safety pressure\n",
        "            \"offroad_penalty\": -8.0,\n",
        "            \"traffic_violation_penalty\": -5.0,\n",
        "            \"success_bonus\": 15.0,\n",
        "            \"step_penalty\": -0.05,\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "TOTAL_CURRICULUM_BUDGET = sum(s[\"budget\"] for s in STAGES)\n",
        "print(\"Total curriculum budget (per algorithm) =\", TOTAL_CURRICULUM_BUDGET)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Held-out 1: fixed 6-block map SCrRXO with medium traffic\n",
        "HELDOUT_SCENARIO_STAGE = {\n",
        "    \"id\": \"HELDOUT_SCENARIO\",\n",
        "    \"name\": \"HELDOUT_SCrRXO_MedTraffic\",\n",
        "    \"map\": \"SCrRXO\", # Straight -> Circular -> in-ramp -> out-ramp -> intersection -> roundabout\n",
        "    \"traffic\": 0.30, # medium-ish traffic\n",
        "    \"budget\": 0, # no training budget, eval only\n",
        "    \"reward\": STAGES[-1][\"reward\"],  # reuse hardest-stage shaping for comparability\n",
        "}\n",
        "\n",
        "# Held-out 2: VaryingDynamics environment (dynamics robustness test)\n",
        "VARYING_DYNAMICS_CONFIG = dict(\n",
        "    num_scenarios=100,\n",
        "    map=5,                  # small PG map\n",
        "    log_level=logging.ERROR,\n",
        "    # random_dynamics uses default ranges from docs; that's enough to show robustness\n",
        ")"
      ],
      "metadata": {
        "id": "B-HaQNeltf-R"
      },
      "id": "B-HaQNeltf-R",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder convention\n",
        "EXPERIMENT_ROOT = Path('experiments')\n",
        "EXPERIMENT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "# Seeds and workers\n",
        "SEEDS = [0]\n",
        "N_ENVS = 1 # should be 8 but metadrive issues\n",
        "EVAL_FREQ = 10_000\n",
        "EVAL_EPISODES = 5\n",
        "\n",
        "# Held-out test map\n",
        "# HELDOUT_MAP = (\"Fork\", 0.2)"
      ],
      "metadata": {
        "id": "w_ndV2-1wbQZ"
      },
      "id": "w_ndV2-1wbQZ",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "HYPERS = {\n",
        "    'PPO': {\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[64,64]}, # can do 64, 64 as well\n",
        "        'learning_rate':3e-4,\n",
        "        'n_steps':1024, # compute issue, can do 2048 if faster\n",
        "        'batch_size':64,\n",
        "        'n_epochs':10,\n",
        "        'gamma':0.99,\n",
        "        'clip_range':0.2\n",
        "    },\n",
        "    'SAC': {\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[256,256]},\n",
        "        'learning_rate':3e-4,\n",
        "        'batch_size':256,\n",
        "        'buffer_size':100_000,\n",
        "        'gamma':0.99\n",
        "    },\n",
        "    'DQN': { # Atari setup\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[64,64]},\n",
        "        'learning_rate':1e-4,\n",
        "        'buffer_size':100_000, # can do 50,000 if needed\n",
        "        'batch_size':32,\n",
        "        'train_freq':4\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "HjNj-DhhwaJR"
      },
      "id": "HjNj-DhhwaJR",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e299413",
      "metadata": {
        "id": "0e299413"
      },
      "source": [
        "Includes a discrete-action wrapper for DQN (maps discrete indices -> continuous steer/throttle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3e0cb97e",
      "metadata": {
        "id": "3e0cb97e"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class DiscreteActionWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env, mapping):\n",
        "        super().__init__(env)\n",
        "        self.mapping = mapping\n",
        "        self.action_space = spaces.Discrete(len(mapping))\n",
        "\n",
        "    def action(self, action):\n",
        "        return np.array(self.mapping[action], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CurriculumRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Stage-dependent reward shaping:\n",
        "    - Start from MetaDrive's base reward.\n",
        "    - Add speed term.\n",
        "    - Add collision / off-road / traffic-violation penalties.\n",
        "    - Add success bonus.\n",
        "\n",
        "    Uses the per-stage reward config from STAGES.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, reward_cfg):\n",
        "        super().__init__(env)\n",
        "        self.cfg = reward_cfg\n",
        "        self.max_speed = self.cfg.get(\"max_speed_kmh\", 80.0)\n",
        "\n",
        "    def step(self, action):\n",
        "        # MetaDrive uses Gymnasium API: obs, reward, terminated, truncated, info\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # --- speed term ---\n",
        "        # MetaDrive usually exposes speed either as 'speed' or 'velocity'\n",
        "        raw_speed = float(info.get(\"speed\", info.get(\"velocity\", 0.0)))\n",
        "        speed = max(0.0, min(raw_speed, self.max_speed))\n",
        "        speed_term = self.cfg.get(\"speed_w\", 0.0) * (speed / self.max_speed)\n",
        "\n",
        "        # --- start from scaled base reward + speed shaping ---\n",
        "        r = self.cfg.get(\"base_w\", 1.0) * base_r + speed_term\n",
        "\n",
        "        # --- per-step cost (encourage finishing sooner) ---\n",
        "        r += self.cfg.get(\"step_penalty\", 0.0)\n",
        "\n",
        "        # --- collision penalties ---\n",
        "        crashed = (\n",
        "            info.get(\"crash_vehicle\", False)\n",
        "            or info.get(\"crash_object\", False)\n",
        "            or info.get(\"crash_building\", False)\n",
        "        )\n",
        "        if crashed:\n",
        "            r += self.cfg.get(\"collision_penalty\", 0.0)\n",
        "        info[\"collision\"] = bool(crashed)\n",
        "\n",
        "        # --- off-road / traffic-violation penalties ---\n",
        "        offroad = info.get(\"out_of_road\", False)\n",
        "        if offroad:\n",
        "            r += self.cfg.get(\"offroad_penalty\", 0.0)\n",
        "\n",
        "        # generic \"traffic violation\" flag for your metrics callback\n",
        "        traffic_violation = bool(offroad or info.get(\"traffic_light_violation\", False))\n",
        "        if traffic_violation:\n",
        "            r += self.cfg.get(\"traffic_violation_penalty\", 0.0)\n",
        "        info[\"traffic_violation\"] = traffic_violation\n",
        "\n",
        "        # --- success bonus at terminal step ---\n",
        "        success = bool(info.get(\"arrive_dest\", False) or info.get(\"success\", False))\n",
        "        if terminated and success:\n",
        "            r += self.cfg.get(\"success_bonus\", 0.0)\n",
        "        info[\"success\"] = success\n",
        "\n",
        "        # For logging\n",
        "        info[\"avg_speed\"] = speed\n",
        "        info[\"shaped_reward\"] = r\n",
        "\n",
        "        return obs, r, terminated, truncated, info"
      ],
      "metadata": {
        "id": "wLTVGUAADX1r"
      },
      "id": "wLTVGUAADX1r",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "75c2b81f",
      "metadata": {
        "id": "75c2b81f"
      },
      "source": [
        "## 4. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Environment factory"
      ],
      "metadata": {
        "id": "nGXJIcxPxSfh"
      },
      "id": "nGXJIcxPxSfh"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "be7c260f",
      "metadata": {
        "id": "be7c260f"
      },
      "outputs": [],
      "source": [
        "# from functools import partial\n",
        "\n",
        "# def make_metadrive_env(map_name, traffic_density=0.0, use_discrete=False, seed=0, render=False):\n",
        "#     def _init():\n",
        "#         cfg = {\n",
        "#             'map': map_name,\n",
        "#             'traffic_density': traffic_density,\n",
        "#             'use_render': False,\n",
        "#             'start_seed': seed,\n",
        "#             'random_spawn': True,\n",
        "#             'debug': False,\n",
        "#         }\n",
        "#         env = MetaDriveEnv(cfg)\n",
        "#         # Optionally wrap for DQN\n",
        "#         if use_discrete:\n",
        "#             mapping = [(-1.0,0.0),(-1.0,0.3),(0.0,0.5),(1.0,0.3),(1.0,0.0)]\n",
        "#             env = DiscreteActionWrapper(env, mapping)\n",
        "#         return Monitor(env)\n",
        "#     return _init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_vec_env(map_name, traffic_density=0.0, n_envs=8, use_discrete=False, seed=0, parallel=False):\n",
        "#     factories = [make_metadrive_env(map_name, traffic_density, use_discrete, seed+i) for i in range(n_envs)]\n",
        "#     if parallel:\n",
        "#         return SubprocVecEnv(factories)\n",
        "#     else:\n",
        "#         return DummyVecEnv(factories)"
      ],
      "metadata": {
        "id": "Vz_Jdlgfw0a_"
      },
      "id": "Vz_Jdlgfw0a_",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "class MetaDriveGymCompatibilityWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Makes MetaDriveEnv follow the Gymnasium reset() and step() signature.\n",
        "    Removes unsupported arguments like options.\n",
        "    \"\"\"\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            obs, info = self.env.reset(seed=seed)\n",
        "        else:\n",
        "            obs, info = self.env.reset()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        # MetaDrive uses done only; Gymnasium expects (terminated, truncated)\n",
        "        terminated = done\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "vKLjHVxvkKCh"
      },
      "id": "vKLjHVxvkKCh",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import logging\n",
        "\n",
        "def make_metadrive_env(stage, use_discrete=False, seed=0, render=False):\n",
        "    \"\"\"\n",
        "    stage: one of the dicts from STAGES. Uses:\n",
        "        stage[\"map\"], stage[\"traffic\"], stage[\"reward\"]\n",
        "    \"\"\"\n",
        "    map_name = stage[\"map\"]\n",
        "    traffic_density = stage[\"traffic\"]\n",
        "    reward_cfg = stage[\"reward\"]\n",
        "\n",
        "    def _init():\n",
        "        cfg = {\n",
        "            \"map\": map_name,\n",
        "            \"traffic_density\": traffic_density,\n",
        "            \"use_render\": render,\n",
        "            \"start_seed\": seed,\n",
        "            # \"random_spawn\": True,\n",
        "            \"debug\": False,\n",
        "            \"log_level\": logging.ERROR,\n",
        "            # cap episode length so eval can't run forever\n",
        "            \"horizon\": 1000, # max steps per episode\n",
        "            \"truncate_as_terminate\": True,   # treat horizon as done\n",
        "        }\n",
        "        env = MetaDriveEnv(cfg)\n",
        "\n",
        "        # Fix reset() signature mismatch\n",
        "        env = MetaDriveGymCompatibilityWrapper(env)\n",
        "\n",
        "        # CuRLA-style stage-dependent reward shaping\n",
        "        env = CurriculumRewardWrapper(env, reward_cfg)\n",
        "\n",
        "        # discrete-action wrapper for DQN: [steering, throttle]\n",
        "        if use_discrete:\n",
        "            mapping = [\n",
        "              (-1.0, 0.0), # hard left, no throttle\n",
        "              (-1.0, 0.3), # left + some accel\n",
        "              (0.0, 0.5), # go straight, accel\n",
        "              (1.0, 0.3), # right + some accel\n",
        "              (1.0, 0.0), # hard right, no throttle\n",
        "            ]\n",
        "            env = DiscreteActionWrapper(env, mapping)\n",
        "\n",
        "        return Monitor(env)\n",
        "\n",
        "    return _init\n"
      ],
      "metadata": {
        "id": "sv070jNaDnyR"
      },
      "id": "sv070jNaDnyR",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_vec_env(stage, n_envs=8, use_discrete=False, seed=0, parallel=False):\n",
        "#     factories = [\n",
        "#         make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "#         for i in range(n_envs)\n",
        "#     ]\n",
        "#     if parallel:\n",
        "#         return SubprocVecEnv(factories)\n",
        "#     else:\n",
        "#         return DummyVecEnv(factories)\n",
        "\n",
        "# from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "\n",
        "# def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n",
        "#     \"\"\"\n",
        "#     Create vectorized MetaDrive envs.\n",
        "#     IMPORTANT: MetaDrive can only have one engine per process.\n",
        "#     So:\n",
        "#       - n_envs == 1  -> use DummyVecEnv (single process, single env)\n",
        "#       - n_envs > 1   -> use SubprocVecEnv (one env per process)\n",
        "#     \"\"\"\n",
        "#     if n_envs == 1:\n",
        "#         return DummyVecEnv([\n",
        "#             make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n",
        "#         ])\n",
        "#     else:\n",
        "#         env_fns = [\n",
        "#             make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "#             for i in range(n_envs)\n",
        "#         ]\n",
        "#         return SubprocVecEnv(env_fns)\n",
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n",
        "    \"\"\"\n",
        "    Create vectorized MetaDrive envs.\n",
        "    FIX: We must ALWAYS use SubprocVecEnv for MetaDrive.\n",
        "    Using DummyVecEnv (single process) prevents creating a second env\n",
        "    (like eval_env) because MetaDrive allows only one engine per process.\n",
        "    \"\"\"\n",
        "    # env_fns = [\n",
        "    #     make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "    #     for i in range(n_envs)\n",
        "    # ]\n",
        "\n",
        "    # # FORCE SubprocVecEnv even if n_envs=1\n",
        "    # return SubprocVecEnv(env_fns)\n",
        "\n",
        "    return DummyVecEnv([\n",
        "        make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n",
        "    ])"
      ],
      "metadata": {
        "id": "v9TCh0Q_DpL4"
      },
      "id": "v9TCh0Q_DpL4",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO/SAC use continous control (`Box`) from MetaDrive while DQN uses a manually defined discrete control space."
      ],
      "metadata": {
        "id": "dTkPoInXfp4F"
      },
      "id": "dTkPoInXfp4F"
    },
    {
      "cell_type": "code",
      "source": [
        "vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=False, seed=0)\n",
        "base_env = vec.envs[0]    # DummyVecEnv\n",
        "print(\"PPO/SAC action space:\", base_env.action_space)\n",
        "print(\"obs space:\", base_env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-OkesJEflDV",
        "outputId": "ed252f27-5b7a-4eaf-be65-34037aa715d6"
      },
      "id": "Q-OkesJEflDV",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPO/SAC action space: Box(-1.0, 1.0, (2,), float32)\n",
            "obs space: Box(-0.0, 1.0, (259,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=True, seed=0)\n",
        "base_env = vec.envs[0]    # DummyVecEnv\n",
        "print(\"PPO/SAC action space:\", base_env.action_space)\n",
        "print(\"obs space:\", base_env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA49LfETfmvF",
        "outputId": "d1e64acf-4bd4-43df-f385-06207c8c3eb2"
      },
      "id": "TA49LfETfmvF",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPO/SAC action space: Discrete(5)\n",
            "obs space: Box(-0.0, 1.0, (259,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_eval_vec_env(stage, use_discrete=False, seed=0):\n",
        "    \"\"\"\n",
        "    Eval env:\n",
        "    - Always uses SubprocVecEnv (even with 1 worker) so that MetaDriveEnv\n",
        "      is only ever created in subprocesses, not in the main process.\n",
        "    \"\"\"\n",
        "    env_fns = [make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)]\n",
        "    return SubprocVecEnv(env_fns)\n"
      ],
      "metadata": {
        "id": "hJa0pGDE4Z1J"
      },
      "id": "hJa0pGDE4Z1J",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34d56696",
      "metadata": {
        "id": "34d56696"
      },
      "source": [
        "### 4.2 log per-eval metrics (success rate, collisions, speed etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "35b0feb6",
      "metadata": {
        "id": "35b0feb6"
      },
      "outputs": [],
      "source": [
        "# class MetricsCallback(BaseCallback):\n",
        "#     \"\"\"\n",
        "#     Custom callback to compute and append evaluation metrics to CSV on each eval.\n",
        "#     Assumes the eval_env yields episode info dicts with keys 'success','collision','avg_speed','traffic_violation'.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, eval_env, out_csv, eval_episodes=10, verbose=0):\n",
        "#         super().__init__(verbose)\n",
        "#         self.eval_env = eval_env\n",
        "#         self.out_csv = out_csv\n",
        "#         self.eval_episodes = eval_episodes\n",
        "#         self._cols = ['timestamp','total_timesteps','mean_reward','std_reward','success_rate','collision_rate','avg_speed','traffic_violations']\n",
        "#         if not os.path.exists(out_csv):\n",
        "#             pd.DataFrame(columns=self._cols).to_csv(out_csv, index=False)\n",
        "\n",
        "#     def _on_step(self) -> bool:\n",
        "#         return True\n",
        "\n",
        "#     def on_training_end(self):\n",
        "#         pass\n",
        "\n",
        "#     def record_eval(self, model, total_timesteps):\n",
        "#         # run evaluation episodes and compute metrics\n",
        "#         rewards = []\n",
        "#         successes = []\n",
        "#         collisions = []\n",
        "#         speeds = []\n",
        "#         traffic_viol = []\n",
        "#         for ep in range(self.eval_episodes):\n",
        "#             obs, _ = self.eval_env.reset()\n",
        "#             done = False\n",
        "#             ep_r = 0.0\n",
        "#             while not done:\n",
        "#                 action, _ = model.predict(obs, deterministic=True)\n",
        "#                 obs, reward, terminated, truncated, info = self.eval_env.step(action)\n",
        "#                 ep_r += reward\n",
        "#                 done = terminated or truncated\n",
        "#             rewards.append(ep_r)\n",
        "#             info_ep = info if isinstance(info, dict) else {}\n",
        "#             # fallback if env does not provide keys\n",
        "#             successes.append(info_ep.get('success', 1.0 if ep_r>0 else 0.0))\n",
        "#             collisions.append(info_ep.get('collision', 0.0))\n",
        "#             speeds.append(info_ep.get('avg_speed', info_ep.get('speed', 0.0)))\n",
        "#             traffic_viol.append(info_ep.get('traffic_violation', 0.0))\n",
        "\n",
        "#         mean_r = np.mean(rewards)\n",
        "#         std_r = np.std(rewards)\n",
        "#         success_rate = np.mean(successes)\n",
        "#         collision_rate = np.mean(collisions)\n",
        "#         avg_speed = np.mean(speeds)\n",
        "#         tv = np.mean(traffic_viol)\n",
        "#         row = {\n",
        "#             'timestamp': time.time(), 'total_timesteps': total_timesteps, 'mean_reward': mean_r, 'std_reward': std_r,\n",
        "#             'success_rate': success_rate, 'collision_rate': collision_rate, 'avg_speed': avg_speed, 'traffic_violations': tv\n",
        "#         }\n",
        "#         df = pd.read_csv(self.out_csv)\n",
        "#         df = df.append(row, ignore_index=True)\n",
        "#         df.to_csv(self.out_csv, index=False)\n",
        "#         return row"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class MetricsCallback(BaseCallback):\n",
        "#     \"\"\"\n",
        "#     Run a short evaluation every eval_freq steps and log:\n",
        "#       - mean_reward\n",
        "#       - success_rate\n",
        "#       - collision_rate\n",
        "#       - traffic_violation_rate\n",
        "#       - avg_speed\n",
        "#       - avg_episode_length\n",
        "\n",
        "#     Saves to a CSV at csv_path.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, eval_env, csv_path, eval_freq=50_000, eval_episodes=10, verbose=0):\n",
        "#         super().__init__(verbose)\n",
        "#         self.eval_env = eval_env\n",
        "#         self.csv_path = csv_path\n",
        "#         self.eval_freq = eval_freq\n",
        "#         self.eval_episodes = eval_episodes\n",
        "\n",
        "#         # Create dir if needed\n",
        "#         os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "#         # Write header if file doesn't exist\n",
        "#         if not os.path.exists(self.csv_path):\n",
        "#             with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "#                 writer = csv.DictWriter(\n",
        "#                     f,\n",
        "#                     fieldnames=[\n",
        "#                         \"timesteps\",\n",
        "#                         \"mean_reward\",\n",
        "#                         \"success_rate\",\n",
        "#                         \"collision_rate\",\n",
        "#                         \"traffic_violation_rate\",\n",
        "#                         \"avg_speed\",\n",
        "#                         \"avg_episode_length\",\n",
        "#                     ],\n",
        "#                 )\n",
        "#                 writer.writeheader()\n",
        "\n",
        "#     def _on_step(self) -> bool:\n",
        "#         # Only evaluate every eval_freq calls\n",
        "#         if self.n_calls % self.eval_freq != 0:\n",
        "#             return True\n",
        "\n",
        "#         rewards = []\n",
        "#         successes = []\n",
        "#         collisions = []\n",
        "#         traffic_violations = []\n",
        "#         speeds = []\n",
        "#         ep_lengths = []\n",
        "\n",
        "#         for _ in range(self.eval_episodes):\n",
        "#             obs, _ = self.eval_env.reset()\n",
        "#             done = False\n",
        "#             truncated = False\n",
        "\n",
        "#             ep_reward = 0.0\n",
        "#             ep_success = False\n",
        "#             ep_collision = False\n",
        "#             ep_traffic_violation = False\n",
        "#             ep_steps = 0\n",
        "#             ep_speeds = []\n",
        "\n",
        "#             while not (done or truncated):\n",
        "#                 action, _ = self.model.predict(obs, deterministic=True)\n",
        "#                 obs, r, done, truncated, info = self.eval_env.step(action)\n",
        "\n",
        "#                 ep_reward += float(r)\n",
        "#                 ep_steps += 1\n",
        "\n",
        "#                 # flags from CurriculumRewardWrapper / MetaDrive info\n",
        "#                 if info.get(\"success\", False):\n",
        "#                     ep_success = True\n",
        "#                 if info.get(\"collision\", False):\n",
        "#                     ep_collision = True\n",
        "#                 if info.get(\"traffic_violation\", False):\n",
        "#                     ep_traffic_violation = True\n",
        "\n",
        "#                 if \"avg_speed\" in info:\n",
        "#                     ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "#                 elif \"speed\" in info:\n",
        "#                     ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "#             rewards.append(ep_reward)\n",
        "#             successes.append(1.0 if ep_success else 0.0)\n",
        "#             collisions.append(1.0 if ep_collision else 0.0)\n",
        "#             traffic_violations.append(1.0 if ep_traffic_violation else 0.0)\n",
        "#             ep_lengths.append(ep_steps)\n",
        "#             if ep_speeds:\n",
        "#                 speeds.append(sum(ep_speeds) / len(ep_speeds))\n",
        "\n",
        "#         mean_reward = float(sum(rewards) / len(rewards)) if rewards else 0.0\n",
        "#         success_rate = float(sum(successes) / len(successes)) if successes else 0.0\n",
        "#         collision_rate = float(sum(collisions) / len(collisions)) if collisions else 0.0\n",
        "#         traffic_violation_rate = float(sum(traffic_violations) / len(traffic_violations)) if traffic_violations else 0.0\n",
        "#         avg_speed = float(sum(speeds) / len(speeds)) if speeds else 0.0\n",
        "#         avg_episode_length = float(sum(ep_lengths) / len(ep_lengths)) if ep_lengths else 0.0\n",
        "\n",
        "#         row = {\n",
        "#             \"timesteps\": int(self.num_timesteps),\n",
        "#             \"mean_reward\": mean_reward,\n",
        "#             \"success_rate\": success_rate,\n",
        "#             \"collision_rate\": collision_rate,\n",
        "#             \"traffic_violation_rate\": traffic_violation_rate,\n",
        "#             \"avg_speed\": avg_speed,\n",
        "#             \"avg_episode_length\": avg_episode_length,\n",
        "#         }\n",
        "\n",
        "#         with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "#             writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "#             writer.writerow(row)\n",
        "\n",
        "#         if self.verbose > 0:\n",
        "#             print(f\"[Metrics] t={self.num_timesteps}  succ={success_rate:.2f}  \"\n",
        "#                   f\"coll={collision_rate:.2f}  len={avg_episode_length:.1f}\")\n",
        "\n",
        "#         return True"
      ],
      "metadata": {
        "id": "nqsUhrkZHHzw"
      },
      "id": "nqsUhrkZHHzw",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Run a short evaluation every eval_freq steps and log:\n",
        "      - mean_reward\n",
        "      - success_rate\n",
        "      - collision_rate\n",
        "      - traffic_violation_rate\n",
        "      - avg_speed\n",
        "      - avg_episode_length\n",
        "\n",
        "    Saves to a CSV at csv_path.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eval_env, csv_path, eval_freq=50_000, eval_episodes=10, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.eval_env = eval_env\n",
        "        self.csv_path = csv_path\n",
        "        self.eval_freq = eval_freq\n",
        "        self.eval_episodes = eval_episodes\n",
        "\n",
        "        # Create dir if needed\n",
        "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "        # Write header if file doesn't exist\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "                writer = csv.DictWriter(\n",
        "                    f,\n",
        "                    fieldnames=[\n",
        "                        \"timesteps\",\n",
        "                        \"mean_reward\",\n",
        "                        \"success_rate\",\n",
        "                        \"collision_rate\",\n",
        "                        \"traffic_violation_rate\",\n",
        "                        \"avg_speed\",\n",
        "                        \"avg_episode_length\",\n",
        "                    ],\n",
        "                )\n",
        "                writer.writeheader()\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Only evaluate every eval_freq calls\n",
        "        if self.n_calls % self.eval_freq != 0:\n",
        "            return True\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_successes = []\n",
        "        episode_collisions = []\n",
        "        episode_traffic_violations = []\n",
        "        episode_speeds = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for _ in range(self.eval_episodes):\n",
        "            # DummyVecEnv.reset() -> obs (no info, vec-batched)\n",
        "            obs = self.eval_env.reset()\n",
        "            done = False\n",
        "\n",
        "            ep_reward = 0.0\n",
        "            ep_success = False\n",
        "            ep_collision = False\n",
        "            ep_traffic_violation = False\n",
        "            ep_steps = 0\n",
        "            ep_speeds = []\n",
        "\n",
        "            while not done:\n",
        "                # obs shape: (n_envs, obs_dim); n_envs = 1 here\n",
        "                action, _ = self.model.predict(obs, deterministic=True)\n",
        "                # DummyVecEnv.step() -> obs, rewards, dones, infos\n",
        "                obs, rewards, dones, infos = self.eval_env.step(action)\n",
        "\n",
        "                # unwrap vec env outputs for single env\n",
        "                if isinstance(rewards, (np.ndarray, list, tuple)):\n",
        "                    r = float(rewards[0])\n",
        "                else:\n",
        "                    r = float(rewards)\n",
        "\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    d = bool(dones[0])\n",
        "                else:\n",
        "                    d = bool(dones)\n",
        "\n",
        "                if isinstance(infos, (list, tuple)) and len(infos) > 0:\n",
        "                    info = infos[0]\n",
        "                else:\n",
        "                    info = infos\n",
        "\n",
        "                ep_reward += r\n",
        "                ep_steps += 1\n",
        "                done = d\n",
        "\n",
        "                # if isinstance(info, dict):\n",
        "                #     if info.get(\"success\", False):\n",
        "                #         ep_success = True\n",
        "                #     if info.get(\"collision\", False):\n",
        "                #         ep_collision = True\n",
        "                #     if info.get(\"traffic_violation\", False):\n",
        "                #         ep_traffic_violation = True\n",
        "\n",
        "                #     if \"avg_speed\" in info:\n",
        "                #         ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "                #     elif \"speed\" in info:\n",
        "                #         ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "                # flags from MetaDrive info / our wrapper\n",
        "                if isinstance(info, dict):\n",
        "                    # success: either our wrapper's \"success\" OR MetaDrive's arrive_dest\n",
        "                    if info.get(\"success\", False) or info.get(\"arrive_dest\", False):\n",
        "                        ep_success = True\n",
        "\n",
        "                    # collision: either our wrapper's \"collision\" OR any crash/out-of-road\n",
        "                    if (\n",
        "                        info.get(\"collision\", False)\n",
        "                        or info.get(\"crash_vehicle\", False)\n",
        "                        or info.get(\"crash_object\", False)\n",
        "                        or info.get(\"crash_building\", False)\n",
        "                        or info.get(\"out_of_road\", False)\n",
        "                    ):\n",
        "                        ep_collision = True\n",
        "\n",
        "                    # traffic violation if we ever log it; otherwise this will stay 0\n",
        "                    if info.get(\"traffic_violation\", False):\n",
        "                        ep_traffic_violation = True\n",
        "\n",
        "                    # speed logging\n",
        "                    if \"avg_speed\" in info:\n",
        "                        ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "                    elif \"speed\" in info:\n",
        "                        ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "\n",
        "            episode_rewards.append(ep_reward)\n",
        "            episode_successes.append(1.0 if ep_success else 0.0)\n",
        "            episode_collisions.append(1.0 if ep_collision else 0.0)\n",
        "            episode_traffic_violations.append(1.0 if ep_traffic_violation else 0.0)\n",
        "            episode_lengths.append(ep_steps)\n",
        "            if ep_speeds:\n",
        "                episode_speeds.append(sum(ep_speeds) / len(ep_speeds))\n",
        "\n",
        "        mean_reward = float(sum(episode_rewards) / len(episode_rewards)) if episode_rewards else 0.0\n",
        "        success_rate = float(sum(episode_successes) / len(episode_successes)) if episode_successes else 0.0\n",
        "        collision_rate = float(sum(episode_collisions) / len(episode_collisions)) if episode_collisions else 0.0\n",
        "        traffic_violation_rate = float(sum(episode_traffic_violations) / len(episode_traffic_violations)) if episode_traffic_violations else 0.0\n",
        "        avg_speed = float(sum(episode_speeds) / len(episode_speeds)) if episode_speeds else 0.0\n",
        "        avg_episode_length = float(sum(episode_lengths) / len(episode_lengths)) if episode_lengths else 0.0\n",
        "\n",
        "        row = {\n",
        "            \"timesteps\": int(self.num_timesteps),\n",
        "            \"mean_reward\": mean_reward,\n",
        "            \"success_rate\": success_rate,\n",
        "            \"collision_rate\": collision_rate,\n",
        "            \"traffic_violation_rate\": traffic_violation_rate,\n",
        "            \"avg_speed\": avg_speed,\n",
        "            \"avg_episode_length\": avg_episode_length,\n",
        "        }\n",
        "\n",
        "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "            writer.writerow(row)\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print(f\"[Metrics] t={self.num_timesteps}  succ={success_rate:.2f}  \"\n",
        "                  f\"coll={collision_rate:.2f}  len={avg_episode_length:.1f}\")\n",
        "\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "zGjpX3W1FZsu"
      },
      "id": "zGjpX3W1FZsu",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class PrettyEvalCallback(EvalCallback):\n",
        "    \"\"\"\n",
        "    Clean pretty printing for eval:\n",
        "      - One separator before the first eval\n",
        "      - No spam between evals\n",
        "      - One separator at the end of training on this stage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._started = False    # whether we have printed the first separator\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Check if it's time to evaluate\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            # On first eval, print top separator\n",
        "            if not self._started:\n",
        "                print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "                self._started = True\n",
        "\n",
        "        return super()._on_step()\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        # After training for this stage: print final separator\n",
        "        if self._started:\n",
        "            print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "        return super()._on_training_end()\n"
      ],
      "metadata": {
        "id": "-eFLK2DKKDJR"
      },
      "id": "-eFLK2DKKDJR",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_run_config(out_dir, algo, stage, seed):\n",
        "    \"\"\"\n",
        "    Save basic run config (algo, stage, hyperparams, seed) to config.json\n",
        "    so you can reproduce / inspect later.\n",
        "    \"\"\"\n",
        "    cfg = {\n",
        "        \"algo\": algo,\n",
        "        \"seed\": seed,\n",
        "        \"stage\": {\n",
        "            \"id\": stage[\"id\"],\n",
        "            \"name\": stage[\"name\"],\n",
        "            \"map\": stage[\"map\"],\n",
        "            \"traffic\": stage[\"traffic\"],\n",
        "            \"budget\": stage[\"budget\"],\n",
        "            \"reward\": stage[\"reward\"],\n",
        "        },\n",
        "        \"hyperparams\": HYPERS[algo],\n",
        "        \"heldout_map\": {\"map\": HELDOUT_SCENARIO_STAGE['map'], \"traffic\": HELDOUT_SCENARIO_STAGE['traffic'], \"reward\": HELDOUT_SCENARIO_STAGE['reward']},\n",
        "    }\n",
        "    with open(out_dir / \"config.json\", \"w\") as f:\n",
        "        json.dump(cfg, f, indent=2)"
      ],
      "metadata": {
        "id": "rPOxRfNNHKbp"
      },
      "id": "rPOxRfNNHKbp",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(HELDOUT_SCENARIO_STAGE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soBC1415it33",
        "outputId": "79f26125-276a-4639-f9c8-ba5dd320dcb2"
      },
      "id": "soBC1415it33",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'HELDOUT_SCENARIO', 'name': 'HELDOUT_SCrRXO_MedTraffic', 'map': 'SCrRXO', 'traffic': 0.3, 'budget': 0, 'reward': {'base_w': 1.0, 'speed_w': 0.1, 'max_speed_kmh': 80.0, 'collision_penalty': -10.0, 'offroad_penalty': -8.0, 'traffic_violation_penalty': -5.0, 'success_bonus': 15.0, 'step_penalty': -0.05}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0303bc4f",
      "metadata": {
        "id": "0303bc4f"
      },
      "source": [
        "### 4.3 Training functions\n",
        "\n",
        "These functions create models, attach callbacks, and run training. Each saves checkpoint, best-model and CSV metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "9a2ecb4e",
      "metadata": {
        "id": "9a2ecb4e"
      },
      "outputs": [],
      "source": [
        "# def make_model(algo, env, hyperparams):\n",
        "#     if algo == 'PPO':\n",
        "#         model = PPO(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
        "#                     policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
        "#                     n_steps=hyperparams['n_steps'], batch_size=hyperparams['batch_size'], n_epochs=hyperparams['n_epochs'], gamma=hyperparams['gamma'])\n",
        "#         return model\n",
        "#     if algo == 'SAC':\n",
        "#         model = SAC(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
        "#                     policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
        "#                     batch_size=hyperparams.get('batch_size',256), buffer_size=hyperparams.get('buffer_size',100000), gamma=hyperparams.get('gamma',0.99))\n",
        "#         return model\n",
        "#     if algo == 'DQN':\n",
        "#         model = DQN(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
        "#                     policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
        "#                     buffer_size=hyperparams.get('buffer_size',50000), batch_size=hyperparams.get('batch_size',32), train_freq=hyperparams.get('train_freq',4))\n",
        "#         return model\n",
        "#     raise ValueError('Unknown algo')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def make_model(algo, env, hyperparams):\n",
        "    common_kwargs = dict(\n",
        "        verbose=0,  # make SB3 quiet in console\n",
        "        tensorboard_log=str(EXPERIMENT_ROOT / 'tensorboard'),\n",
        "        policy_kwargs=hyperparams['policy_kwargs'],\n",
        "        learning_rate=hyperparams['learning_rate'],\n",
        "    )\n",
        "\n",
        "    if algo == 'PPO':\n",
        "        model = PPO(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            n_steps=hyperparams['n_steps'],\n",
        "            batch_size=hyperparams['batch_size'],\n",
        "            n_epochs=hyperparams['n_epochs'],\n",
        "            gamma=hyperparams['gamma'],\n",
        "            device=\"cpu\",\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if algo == 'SAC':\n",
        "        model = SAC(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            batch_size=hyperparams.get('batch_size', 256),\n",
        "            buffer_size=hyperparams.get('buffer_size', 100_000),\n",
        "            gamma=hyperparams.get('gamma', 0.99),\n",
        "            device=DEVICE,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if algo == 'DQN':\n",
        "        model = DQN(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            buffer_size=hyperparams.get('buffer_size', 50_000),\n",
        "            batch_size=hyperparams.get('batch_size', 32),\n",
        "            train_freq=hyperparams.get('train_freq', 4),\n",
        "            device=DEVICE,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    raise ValueError('Unknown algo')\n"
      ],
      "metadata": {
        "id": "yLzoiNexEQ6n"
      },
      "id": "yLzoiNexEQ6n",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=N_ENVS):\n",
        "#     \"\"\"\n",
        "#     Non-curriculum baseline:\n",
        "#     - Train *from scratch* on a single stage for TOTAL_CURRICULUM_BUDGET steps.\n",
        "#     - Stage carries map, traffic and reward config.\n",
        "#     \"\"\"\n",
        "#     map_name = stage[\"map\"]\n",
        "#     out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n",
        "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
        "#     print(\"Training non-curriculum:\", algo, stage[\"name\"], \"seed\", seed)\n",
        "\n",
        "#     use_discrete = (algo == \"DQN\")\n",
        "\n",
        "#     # training envs\n",
        "#     env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
        "\n",
        "#     # eval env (single copy)\n",
        "#     eval_env_vec = make_vec_env(stage, n_envs=1, use_discrete=use_discrete, seed=seed + 100)\n",
        "#     eval_env = eval_env_vec.env_fns[0]() if hasattr(eval_env_vec, \"env_fns\") else eval_env_vec\n",
        "\n",
        "#     model = make_model(algo, env, HYPERS[algo])\n",
        "\n",
        "#     # callbacks\n",
        "#     eval_cb = EvalCallback(\n",
        "#         eval_env,\n",
        "#         best_model_save_path=str(out_dir / \"best_model\"),\n",
        "#         log_path=str(out_dir / \"eval\"),\n",
        "#         eval_freq=EVAL_FREQ,\n",
        "#         n_eval_episodes=EVAL_EPISODES,\n",
        "#         deterministic=True,\n",
        "#     )\n",
        "#     ckpt_cb = CheckpointCallback(\n",
        "#         save_freq=EVAL_FREQ,\n",
        "#         save_path=str(out_dir / \"checkpoints\"),\n",
        "#         name_prefix=\"ckpt\",\n",
        "#     )\n",
        "#     metrics_csv = out_dir / \"metrics.csv\"\n",
        "#     metrics_cb = MetricsCallback(eval_env, str(metrics_csv), eval_episodes=EVAL_EPISODES)\n",
        "\n",
        "#     # train\n",
        "#     model.learn(total_timesteps=total_timesteps, callback=[eval_cb, ckpt_cb])\n",
        "#     model.save(str(out_dir / \"model.zip\"))\n",
        "\n",
        "#     # held-out evaluation (reuse same reward shaping config for fairness)\n",
        "#     held_stage = {\n",
        "#         \"id\": \"HELDOUT\",\n",
        "#         \"name\": f\"HELDOUT_{HELDOUT_MAP[0]}\",\n",
        "#         \"map\": HELDOUT_MAP[0],\n",
        "#         \"traffic\": HELDOUT_MAP[1],\n",
        "#         \"budget\": 0,\n",
        "#         \"reward\": stage[\"reward\"],  # same shaping as training stage\n",
        "#     }\n",
        "#     held_env_vec = make_vec_env(held_stage, n_envs=1, use_discrete=use_discrete, seed=seed + 500)\n",
        "#     held_env = held_env_vec.env_fns[0]() if hasattr(held_env_vec, \"env_fns\") else held_env_vec\n",
        "#     mean_reward, std_reward = evaluate_policy(model, held_env, n_eval_episodes=100)\n",
        "#     pd.DataFrame([{\"mean_reward\": mean_reward, \"std_reward\": std_reward}]).to_csv(\n",
        "#         out_dir / \"heldout_metrics.csv\", index=False\n",
        "#     )\n",
        "\n",
        "#     print(\"Non-curriculum training complete and saved to\", out_dir)\n",
        "#     return out_dir\n"
      ],
      "metadata": {
        "id": "rMVuVpGBDuw3"
      },
      "id": "rMVuVpGBDuw3",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=N_ENVS):\n",
        "#     \"\"\"\n",
        "#     Non-curriculum baseline:\n",
        "#     - Train from scratch on a SINGLE (hard) stage for 'total_timesteps'.\n",
        "#     - Evaluate on held-out map at the end.\n",
        "#     \"\"\"\n",
        "\n",
        "#     out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n",
        "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
        "#     print(\"Training NON-CURRICULUM:\", algo, stage[\"name\"], \"seed\", seed)\n",
        "\n",
        "#     use_discrete = (algo == \"DQN\")\n",
        "\n",
        "#     # training envs\n",
        "#     env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
        "\n",
        "#     # eval env (for callbacks)\n",
        "#     eval_env = env\n",
        "#     # eval_env = make_eval_vec_env(stage, use_discrete=use_discrete, seed=seed + 100)\n",
        "#     # eval_env = eval_env_vec.env_fns[0]() if hasattr(eval_env_vec, \"env_fns\") else eval_env_vec\n",
        "\n",
        "#     model = make_model(algo, env, HYPERS[algo])\n",
        "\n",
        "#     # callbacks\n",
        "#     eval_cb = PrettyEvalCallback(\n",
        "#         eval_env,\n",
        "#         best_model_save_path=str(out_dir / \"best_model\"),\n",
        "#         log_path=str(out_dir / \"eval\"),\n",
        "#         eval_freq=EVAL_FREQ,\n",
        "#         n_eval_episodes=EVAL_EPISODES,\n",
        "#         deterministic=True,\n",
        "#         verbose=1\n",
        "#     )\n",
        "#     ckpt_cb = CheckpointCallback(\n",
        "#         save_freq=EVAL_FREQ,\n",
        "#         save_path=str(out_dir / \"checkpoints\"),\n",
        "#         name_prefix=\"ckpt\",\n",
        "#     )\n",
        "#     metrics_csv = out_dir / \"metrics.csv\"\n",
        "#     metrics_cb = MetricsCallback(\n",
        "#         eval_env,\n",
        "#         csv_path=str(metrics_csv),\n",
        "#         eval_freq=EVAL_FREQ,\n",
        "#         eval_episodes=EVAL_EPISODES,\n",
        "#         verbose=0,\n",
        "#     )\n",
        "\n",
        "#     # save run config\n",
        "#     save_run_config(out_dir, algo, stage, seed)\n",
        "\n",
        "#     # train\n",
        "#     model.learn(\n",
        "#         total_timesteps=total_timesteps,\n",
        "#         callback=[eval_cb, ckpt_cb, metrics_cb],\n",
        "#     )\n",
        "#     model.save(str(out_dir / \"model.zip\"))\n",
        "\n",
        "#     # held-out evaluation\n",
        "#     # held_stage = {\n",
        "#     #     \"id\": \"HELDOUT\",\n",
        "#     #     \"name\": f\"HELDOUT_{HELDOUT_MAP[0]}\",\n",
        "#     #     \"map\": HELDOUT_MAP[0],\n",
        "#     #     \"traffic\": HELDOUT_MAP[1],\n",
        "#     #     \"budget\": 0,\n",
        "#     #     \"reward\": stage[\"reward\"],  # use same shaping as training stage\n",
        "#     # }\n",
        "#     # Held-out evaluation map: 20-block PG map with medium traffic\n",
        "#     HELDOUT_STAGE = {\n",
        "#         \"id\": \"HELDOUT\",\n",
        "#         \"name\": \"HELDOUT_20Block_MedTraffic\",\n",
        "#         \"map\": 20,                   # ✅ 20-block PG map (int, not \"20-block\")\n",
        "#         \"traffic\": 0.15,             # medium traffic\n",
        "#         \"budget\": 0,                 # no training budget, eval only\n",
        "#         # reuse the hardest-stage reward shaping so it's comparable\n",
        "#         \"reward\": STAGES[-1][\"reward\"],\n",
        "#     }\n",
        "\n",
        "#     # held_env = make_eval_vec_env(held_stage, use_discrete=use_discrete, seed=seed + 500)\n",
        "#     # held_env = held_env_vec.env_fns[0]() if hasattr(held_env_vec, \"env_fns\") else held_env_vec\n",
        "\n",
        "#     # mean_reward, std_reward = evaluate_policy(model, held_env, n_eval_episodes=100)\n",
        "#     # pd.DataFrame([{\"mean_reward\": mean_reward, \"std_reward\": std_reward}]).to_csv(\n",
        "#         # out_dir / \"heldout_metrics.csv\", index=False\n",
        "#     # )\n",
        "\n",
        "#     env.close()\n",
        "#     eval_env.close()\n",
        "#     # held_env.close()\n",
        "\n",
        "#     print(\"Non-curriculum training complete and saved to\", out_dir)\n",
        "#     return out_dir\n"
      ],
      "metadata": {
        "id": "QDgBvXRGHPwz"
      },
      "id": "QDgBvXRGHPwz",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_env_episodes(model, env, n_episodes=20, deterministic=True):\n",
        "    \"\"\"\n",
        "    Run n_episodes on a *single* (non-vec) env.\n",
        "    Returns (mean_reward, std_reward, mean_ep_len).\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        truncated = False\n",
        "        ep_r = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        while not (done or truncated):\n",
        "            action, _ = model.predict(obs, deterministic=deterministic)\n",
        "            obs, r, done, truncated, info = env.step(action)\n",
        "            ep_r += float(r)\n",
        "            steps += 1\n",
        "\n",
        "        rewards.append(ep_r)\n",
        "        lengths.append(steps)\n",
        "\n",
        "    rewards = np.array(rewards, dtype=np.float32)\n",
        "    lengths = np.array(lengths, dtype=np.float32)\n",
        "    return float(rewards.mean()), float(rewards.std()), float(lengths.mean())\n"
      ],
      "metadata": {
        "id": "z5yA8SnOtpCg"
      },
      "id": "z5yA8SnOtpCg",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=1):\n",
        "    \"\"\"\n",
        "    Non-curriculum baseline:\n",
        "    - Train from scratch on a SINGLE stage for 'total_timesteps'.\n",
        "    - Eval during training on SAME env via PrettyEvalCallback (already set up).\n",
        "    - After training, eval on two held-out envs:\n",
        "        1) SCrRXO + medium traffic (MetaDriveEnv)\n",
        "        2) VaryingDynamicsEnv (randomized dynamics)\n",
        "    \"\"\"\n",
        "\n",
        "    out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Training NON-CURRICULUM: {algo} {stage['name']} seed {seed}\\n\")\n",
        "\n",
        "    use_discrete = (algo == \"DQN\")\n",
        "\n",
        "    # ---- training env (single vec env) ----\n",
        "    env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
        "\n",
        "    model = make_model(algo, env, HYPERS[algo])\n",
        "\n",
        "    # in-training eval (same env)\n",
        "    eval_cb = PrettyEvalCallback(\n",
        "        env,\n",
        "        best_model_save_path=str(out_dir / \"best_model\"),\n",
        "        log_path=str(out_dir / \"eval\"),\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        n_eval_episodes=EVAL_EPISODES,\n",
        "        deterministic=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    ckpt_cb = CheckpointCallback(\n",
        "        save_freq=EVAL_FREQ,\n",
        "        save_path=str(out_dir / \"checkpoints\"),\n",
        "        name_prefix=\"ckpt\",\n",
        "    )\n",
        "\n",
        "    metrics_csv = out_dir / \"metrics.csv\"\n",
        "    metrics_cb = MetricsCallback(\n",
        "        eval_env=env.envs[0],       # since you’re using n_envs=1 (DummyVecEnv)\n",
        "        csv_path=str(metrics_csv),\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        eval_episodes=EVAL_EPISODES,\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    save_run_config(out_dir, algo, stage, seed)\n",
        "\n",
        "    # ---- train ----\n",
        "    model.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=[eval_cb, ckpt_cb, metrics_cb],\n",
        "    )\n",
        "    model.save(str(out_dir / \"model.zip\"))\n",
        "\n",
        "    print(f\"\\nNon-curriculum training complete and saved to {out_dir}\")\n",
        "\n",
        "    # Close training vec env to avoid engine conflicts before new envs\n",
        "    env.close()\n",
        "\n",
        "    # =====================================================================\n",
        "    # HELD-OUT 1: SCrRXO + medium traffic\n",
        "    # =====================================================================\n",
        "    print(\"\\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\")\n",
        "\n",
        "    # build a *single* env instance using same wrappers\n",
        "    held1_make = make_metadrive_env(\n",
        "        HELDOUT_SCENARIO_STAGE,\n",
        "        use_discrete=use_discrete,\n",
        "        seed=seed + 1000,\n",
        "        render=False,\n",
        "    )\n",
        "    held1_env = held1_make()\n",
        "\n",
        "    h1_mean, h1_std = evaluate_policy( #, h1_len if manual function\n",
        "        model,\n",
        "        held1_env,\n",
        "        n_eval_episodes=20,\n",
        "        deterministic=True,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"HELD-OUT 1 (SCrRXO): mean_reward={h1_mean:.2f} ± {h1_std:.2f}, \"\n",
        "        # f\"mean_ep_len={h1_len:.1f}\"\n",
        "    )\n",
        "\n",
        "    pd.DataFrame(\n",
        "        [{\n",
        "            \"algo\": algo,\n",
        "            \"train_stage\": stage[\"name\"],\n",
        "            \"heldout_name\": HELDOUT_SCENARIO_STAGE[\"name\"],\n",
        "            \"mean_reward\": h1_mean,\n",
        "            \"std_reward\": h1_std,\n",
        "            # \"mean_ep_len\": h1_len,\n",
        "        }]\n",
        "    ).to_csv(out_dir / \"heldout_scrrxo_metrics.csv\", index=False)\n",
        "\n",
        "    held1_env.close()\n",
        "\n",
        "    # =====================================================================\n",
        "    # HELD-OUT 2: VaryingDynamicsEnv\n",
        "    # =====================================================================\n",
        "    print(\"\\n[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\")\n",
        "\n",
        "    # build varying dynamics env\n",
        "    vd_env = VaryingDynamicsEnv(VARYING_DYNAMICS_CONFIG)\n",
        "    vd_env = MetaDriveGymCompatibilityWrapper(vd_env)\n",
        "\n",
        "    if use_discrete:\n",
        "        # same discrete mapping you used for training DQN\n",
        "        discrete_mapping = [(-1.0, 0.0), (-1.0, 0.3), (0.0, 0.5), (1.0, 0.3), (1.0, 0.0)]\n",
        "        vd_env = DiscreteActionWrapper(vd_env, discrete_mapping)\n",
        "\n",
        "    h2_mean, h2_std, h2_len = evaluate_env_episodes(\n",
        "        model,\n",
        "        vd_env,\n",
        "        n_episodes=20,\n",
        "        deterministic=True,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"HELD-OUT 2 (VaryingDynamics): mean_reward={h2_mean:.2f} ± {h2_std:.2f}, \"\n",
        "        f\"mean_ep_len={h2_len:.1f}\"\n",
        "    )\n",
        "\n",
        "    pd.DataFrame(\n",
        "        [{\n",
        "            \"algo\": algo,\n",
        "            \"train_stage\": stage[\"name\"],\n",
        "            \"heldout_name\": \"VaryingDynamicsEnv\",\n",
        "            \"mean_reward\": h2_mean,\n",
        "            \"std_reward\": h2_std,\n",
        "            \"mean_ep_len\": h2_len,\n",
        "        }]\n",
        "    ).to_csv(out_dir / \"heldout_varying_metrics.csv\", index=False)\n",
        "\n",
        "    vd_env.close()\n",
        "\n",
        "    return out_dir\n"
      ],
      "metadata": {
        "id": "eCc8tf5CtVuA"
      },
      "id": "eCc8tf5CtVuA",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83889cd2",
      "metadata": {
        "id": "83889cd2"
      },
      "source": [
        "### 4.4. Visualization (plot metrics, learning curves, and display videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "57e24184",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57e24184",
        "outputId": "7a7e8de7-ff8e-4f5a-d195-2a27a2f86a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot helper ready\n"
          ]
        }
      ],
      "source": [
        "def plot_metrics(csv_path, title=None):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print('CSV not found:', csv_path); return\n",
        "    df = pd.read_csv(csv_path)\n",
        "    fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
        "    axs = axs.flatten()\n",
        "    axs[0].plot(df['total_timesteps'], df['mean_reward'], marker='o'); axs[0].set_title('Mean reward')\n",
        "    axs[1].plot(df['total_timesteps'], df['success_rate'], marker='o'); axs[1].set_title('Success rate')\n",
        "    axs[2].plot(df['total_timesteps'], df['collision_rate'], marker='o'); axs[2].set_title('Collision rate')\n",
        "    axs[3].plot(df['total_timesteps'], df['avg_speed'], marker='o'); axs[3].set_title('Avg speed')\n",
        "    if title: fig.suptitle(title)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "print('Plot helper ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644f2875",
      "metadata": {
        "id": "644f2875"
      },
      "source": [
        "## 5. Toy pilot run\n",
        "\n",
        "Small pilot run to validate pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "aef7a3ca",
      "metadata": {
        "id": "aef7a3ca"
      },
      "outputs": [],
      "source": [
        "# # run one tiny non-curriculum PPO for 2000 steps on Straight map\n",
        "\n",
        "# out = train_noncurriculum('SAC', STAGES[2], total_timesteps=2000, seed=0, n_envs=1)\n",
        "# print('Pilot saved at:', out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c6e5f6",
      "metadata": {
        "id": "95c6e5f6"
      },
      "source": [
        "## 6. Full experiment\n",
        "\n",
        "For each algorithm, for each seed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = [\"PPO\"] # \"SAC\", \"DQN\",\n",
        "\n",
        "for algo in algos:\n",
        "    for seed in SEEDS:\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"ALGO={algo}  SEED={seed}\")\n",
        "\n",
        "        for stage in STAGES:\n",
        "            train_noncurriculum(\n",
        "                algo=algo,\n",
        "                stage=stage,\n",
        "                # total_timesteps=5000,\n",
        "                total_timesteps=stage[\"budget\"],  # use this stage's budget\n",
        "                seed=seed,\n",
        "                n_envs=N_ENVS,\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPZ5O3NLHg94",
        "outputId": "ce4cb02d-e87b-492f-95dd-2aab4bc358a5"
      },
      "id": "xPZ5O3NLHg94",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ALGO=PPO  SEED=0\n",
            "Training NON-CURRICULUM: PPO C0_Straight seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=0.87 +/- 0.00\n",
            "Episode length: 37.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=4.94 +/- 0.00\n",
            "Episode length: 35.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30000, episode_reward=16.71 +/- 0.00\n",
            "Episode length: 45.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=-1.86 +/- 0.00\n",
            "Episode length: 25.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=135.65 +/- 0.00\n",
            "Episode length: 92.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=60000, episode_reward=136.96 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=70000, episode_reward=135.86 +/- 0.00\n",
            "Episode length: 92.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=136.51 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=135.48 +/- 0.00\n",
            "Episode length: 92.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=136.09 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/PPO/noncurriculum/seed_0/C0_Straight\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=42.84 ± 0.01, \n",
            "\n",
            "[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\n",
            "HELD-OUT 2 (VaryingDynamics): mean_reward=117.35 ± 57.31, mean_ep_len=999.2\n",
            "Training NON-CURRICULUM: PPO C1_Roundabout seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=-2.51 +/- 0.00\n",
            "Episode length: 38.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-5.34 +/- 0.00\n",
            "Episode length: 33.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=25.84 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=4.38 +/- 0.00\n",
            "Episode length: 44.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=2.81 +/- 0.00\n",
            "Episode length: 41.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=46.77 +/- 0.00\n",
            "Episode length: 67.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=70000, episode_reward=11.98 +/- 0.00\n",
            "Episode length: 43.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=-6.68 +/- 0.00\n",
            "Episode length: 26.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=49.25 +/- 0.00\n",
            "Episode length: 71.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=100000, episode_reward=48.68 +/- 0.00\n",
            "Episode length: 70.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=49.04 +/- 0.00\n",
            "Episode length: 69.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=48.80 +/- 0.00\n",
            "Episode length: 67.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=0.99 +/- 0.00\n",
            "Episode length: 34.00 +/- 0.00\n",
            "Eval num_timesteps=140000, episode_reward=48.56 +/- 0.00\n",
            "Episode length: 68.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=49.63 +/- 0.00\n",
            "Episode length: 68.00 +/- 0.00\n",
            "New best mean reward!\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/PPO/noncurriculum/seed_0/C1_Roundabout\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=41.53 ± 0.00, \n",
            "\n",
            "[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\n",
            "HELD-OUT 2 (VaryingDynamics): mean_reward=134.03 ± 65.04, mean_ep_len=114.9\n",
            "Training NON-CURRICULUM: PPO C2_LightTraffic seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=50.29 +/- 0.00\n",
            "Episode length: 78.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=19.64 +/- 0.00\n",
            "Episode length: 51.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=7.59 +/- 0.00\n",
            "Episode length: 41.00 +/- 0.00\n",
            "Eval num_timesteps=40000, episode_reward=-4.89 +/- 0.00\n",
            "Episode length: 28.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=11.07 +/- 0.00\n",
            "Episode length: 45.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=10.07 +/- 0.00\n",
            "Episode length: 44.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=279.97 +/- 0.00\n",
            "Episode length: 170.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=80000, episode_reward=257.13 +/- 0.00\n",
            "Episode length: 160.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=148.47 +/- 0.00\n",
            "Episode length: 112.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=254.54 +/- 0.00\n",
            "Episode length: 163.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=272.20 +/- 0.00\n",
            "Episode length: 172.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=244.24 +/- 0.00\n",
            "Episode length: 159.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=255.54 +/- 0.74\n",
            "Episode length: 159.20 +/- 0.40\n",
            "Eval num_timesteps=140000, episode_reward=-13.09 +/- 0.00\n",
            "Episode length: 28.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=62.09 +/- 0.00\n",
            "Episode length: 76.00 +/- 0.00\n",
            "Eval num_timesteps=160000, episode_reward=268.74 +/- 11.58\n",
            "Episode length: 166.60 +/- 5.39\n",
            "Eval num_timesteps=170000, episode_reward=255.14 +/- 0.00\n",
            "Episode length: 159.00 +/- 0.00\n",
            "Eval num_timesteps=180000, episode_reward=251.03 +/- 0.00\n",
            "Episode length: 162.00 +/- 0.00\n",
            "Eval num_timesteps=190000, episode_reward=-11.50 +/- 0.00\n",
            "Episode length: 26.00 +/- 0.00\n",
            "Eval num_timesteps=200000, episode_reward=255.51 +/- 0.00\n",
            "Episode length: 162.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/PPO/noncurriculum/seed_0/C2_LightTraffic\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=43.49 ± 0.00, \n",
            "\n",
            "[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\n",
            "HELD-OUT 2 (VaryingDynamics): mean_reward=96.46 ± 52.44, mean_ep_len=143.8\n",
            "Training NON-CURRICULUM: PPO C3_DenseTraffic seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=-6.27 +/- 0.00\n",
            "Episode length: 38.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-0.42 +/- 0.00\n",
            "Episode length: 39.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30000, episode_reward=41.03 +/- 0.00\n",
            "Episode length: 65.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=-2.66 +/- 0.00\n",
            "Episode length: 37.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=58.21 +/- 0.01\n",
            "Episode length: 77.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=60000, episode_reward=56.91 +/- 0.07\n",
            "Episode length: 76.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=-3.31 +/- 0.00\n",
            "Episode length: 35.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=-10.34 +/- 0.00\n",
            "Episode length: 30.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=46.06 +/- 0.01\n",
            "Episode length: 70.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=55.00 +/- 0.89\n",
            "Episode length: 75.60 +/- 0.49\n",
            "Eval num_timesteps=110000, episode_reward=58.08 +/- 0.00\n",
            "Episode length: 76.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=-10.98 +/- 0.00\n",
            "Episode length: 29.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=73.02 +/- 0.16\n",
            "Episode length: 83.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=140000, episode_reward=56.13 +/- 0.00\n",
            "Episode length: 76.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=55.15 +/- 0.03\n",
            "Episode length: 73.00 +/- 0.00\n",
            "Eval num_timesteps=160000, episode_reward=55.72 +/- 0.00\n",
            "Episode length: 74.00 +/- 0.00\n",
            "Eval num_timesteps=170000, episode_reward=55.38 +/- 0.77\n",
            "Episode length: 74.20 +/- 0.40\n",
            "Eval num_timesteps=180000, episode_reward=56.18 +/- 0.00\n",
            "Episode length: 75.00 +/- 0.00\n",
            "Eval num_timesteps=190000, episode_reward=55.85 +/- 0.00\n",
            "Episode length: 75.00 +/- 0.00\n",
            "Eval num_timesteps=200000, episode_reward=55.58 +/- 0.05\n",
            "Episode length: 74.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/PPO/noncurriculum/seed_0/C3_DenseTraffic\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=43.51 ± 0.00, \n",
            "\n",
            "[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\n",
            "HELD-OUT 2 (VaryingDynamics): mean_reward=113.90 ± 48.70, mean_ep_len=117.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "bde3ba34",
      "metadata": {
        "id": "bde3ba34"
      },
      "outputs": [],
      "source": [
        "# algos = ['PPO','SAC','DQN']\n",
        "# for algo in algos:\n",
        "#     # non-curriculum: for each target map train for the total curriculum budget (to match sample budget)\n",
        "#     for seed in SEEDS:\n",
        "#         for (_, map_name, traffic, _) in STAGES:\n",
        "#             train_noncurriculum(algo, map_name, traffic, total_timesteps=TOTAL_CURRICULUM_BUDGET, seed=seed, n_envs=N_ENVS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualization"
      ],
      "metadata": {
        "id": "qpsW4W5NI9LG"
      },
      "id": "qpsW4W5NI9LG"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Adjust if your root is named differently\n",
        "EXPERIMENT_ROOT = Path(\"experiments\")\n",
        "\n",
        "CURVES_DIR = EXPERIMENT_ROOT / \"curves\"\n",
        "CURVES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_eval_npz(eval_npz_path: Path):\n",
        "    \"\"\"\n",
        "    Load SB3 EvalCallback npz.\n",
        "    Returns: timesteps, mean_rewards, std_rewards, mean_ep_len\n",
        "    \"\"\"\n",
        "    data = np.load(eval_npz_path)\n",
        "    timesteps = data[\"timesteps\"].flatten()          # [n_eval]\n",
        "    results = data[\"results\"]                        # [n_eval, n_episodes]\n",
        "    ep_lengths = data[\"ep_lengths\"]                  # [n_eval, n_episodes]\n",
        "\n",
        "    mean_rewards = results.mean(axis=1)\n",
        "    std_rewards = results.std(axis=1)\n",
        "    mean_ep_len = ep_lengths.mean(axis=1)\n",
        "\n",
        "    return timesteps, mean_rewards, std_rewards, mean_ep_len\n",
        "\n",
        "\n",
        "def plot_stage_curves(algo, seed_name, stage_name, eval_npz_path):\n",
        "    \"\"\"\n",
        "    Make per-stage plots:\n",
        "      - reward vs timesteps\n",
        "      - episode length vs timesteps\n",
        "    Save to experiments/curves\n",
        "    \"\"\"\n",
        "    t, mean_r, std_r, mean_len = load_eval_npz(eval_npz_path)\n",
        "\n",
        "    # 1) Reward curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(t, mean_r, marker=\"o\")\n",
        "    plt.fill_between(t, mean_r - std_r, mean_r + std_r, alpha=0.2)\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Eval mean return\")\n",
        "    plt.title(f\"{algo} {seed_name} – {stage_name} (eval reward)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_reward.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # 2) Episode length curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(t, mean_len, marker=\"o\")\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Mean episode length\")\n",
        "    plt.title(f\"{algo} {seed_name} – {stage_name} (episode length)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_ep_len.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_combined_curves(algo, seed_name, stage_to_npz):\n",
        "    \"\"\"\n",
        "    Combined plots across stages for one algo+seed:\n",
        "      - all reward curves\n",
        "      - all episode length curves\n",
        "    stage_to_npz: dict {stage_name: eval_npz_path}\n",
        "    \"\"\"\n",
        "    # Reward\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    for stage_name, npz_path in stage_to_npz.items():\n",
        "        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n",
        "        plt.plot(t, mean_r, marker=\"o\", label=stage_name)\n",
        "\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Eval mean return\")\n",
        "    plt.title(f\"{algo} {seed_name} – eval reward (all stages)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_reward.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # Episode length\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    for stage_name, npz_path in stage_to_npz.items():\n",
        "        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n",
        "        plt.plot(t, mean_len, marker=\"o\", label=stage_name)\n",
        "\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Mean episode length\")\n",
        "    plt.title(f\"{algo} {seed_name} – episode length (all stages)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_ep_len.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_heldout_bars(algo, seed_name, seed_dir: Path):\n",
        "    \"\"\"\n",
        "    If held-out CSVs exist for this algo+seed, make bar plots:\n",
        "      - heldout_scrrxo_metrics.csv across stages\n",
        "      - heldout_varying_metrics.csv across stages\n",
        "    \"\"\"\n",
        "    # Find all stages under this seed dir\n",
        "    stage_dirs = [\n",
        "        d for d in seed_dir.iterdir()\n",
        "        if d.is_dir() and not d.name.startswith(\".\")\n",
        "    ]\n",
        "\n",
        "    # ----- Held-out 1: SCrRXO -----\n",
        "    rows = []\n",
        "    for sd in stage_dirs:\n",
        "        csv_path = sd / \"heldout_scrrxo_metrics.csv\"\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            if not df.empty:\n",
        "                r = df.iloc[0].to_dict()\n",
        "                r[\"stage\"] = sd.name\n",
        "                rows.append(r)\n",
        "\n",
        "    if rows:\n",
        "        df_scr = pd.DataFrame(rows)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(df_scr[\"stage\"], df_scr[\"mean_reward\"])\n",
        "        plt.xlabel(\"Training stage\")\n",
        "        plt.ylabel(\"Held-out mean reward\")\n",
        "        plt.title(f\"{algo} {seed_name} – Held-out SCrRXO performance\")\n",
        "        plt.grid(axis=\"y\")\n",
        "        plt.tight_layout()\n",
        "        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_SCrRXO.png\"\n",
        "        plt.savefig(out_path, dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # ----- Held-out 2: VaryingDynamics -----\n",
        "    rows = []\n",
        "    for sd in stage_dirs:\n",
        "        csv_path = sd / \"heldout_varying_metrics.csv\"\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            if not df.empty:\n",
        "                r = df.iloc[0].to_dict()\n",
        "                r[\"stage\"] = sd.name\n",
        "                rows.append(r)\n",
        "\n",
        "    if rows:\n",
        "        df_vd = pd.DataFrame(rows)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(df_vd[\"stage\"], df_vd[\"mean_reward\"])\n",
        "        plt.xlabel(\"Training stage\")\n",
        "        plt.ylabel(\"Held-out mean reward\")\n",
        "        plt.title(f\"{algo} {seed_name} – Held-out VaryingDynamics performance\")\n",
        "        plt.grid(axis=\"y\")\n",
        "        plt.tight_layout()\n",
        "        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_VaryingDynamics.png\"\n",
        "        plt.savefig(out_path, dpi=150)\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "-j-jWQJJgf8k"
      },
      "id": "-j-jWQJJgf8k",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# MAIN: walk experiments/ and generate all possible plots\n",
        "# ======================================================================\n",
        "\n",
        "for algo_dir in EXPERIMENT_ROOT.iterdir():\n",
        "    if not algo_dir.is_dir():\n",
        "        continue\n",
        "    algo = algo_dir.name  # e.g. \"PPO\", \"SAC\", \"DQN\"\n",
        "\n",
        "    noncurr_dir = algo_dir / \"noncurriculum\"\n",
        "    if not noncurr_dir.exists():\n",
        "        continue\n",
        "\n",
        "    for seed_dir in noncurr_dir.iterdir():\n",
        "        if not seed_dir.is_dir() or not seed_dir.name.startswith(\"seed_\"):\n",
        "            continue\n",
        "        seed_name = seed_dir.name  # e.g. \"seed_0\"\n",
        "\n",
        "        print(f\"Processing {algo} / {seed_name} ...\")\n",
        "\n",
        "        # collect per-stage npz paths for combined plots\n",
        "        stage_to_npz = {}\n",
        "\n",
        "        # per-stage plots\n",
        "        for stage_dir in seed_dir.iterdir():\n",
        "            if not stage_dir.is_dir():\n",
        "                continue\n",
        "            stage_name = stage_dir.name  # e.g. \"C0_Straight\"\n",
        "\n",
        "            eval_npz_path = stage_dir / \"eval\" / \"evaluations.npz\"\n",
        "            if not eval_npz_path.exists():\n",
        "                continue\n",
        "\n",
        "            # individual plots\n",
        "            plot_stage_curves(algo, seed_name, stage_name, eval_npz_path)\n",
        "            stage_to_npz[stage_name] = eval_npz_path\n",
        "\n",
        "        # combined curves across stages\n",
        "        if stage_to_npz:\n",
        "            plot_combined_curves(algo, seed_name, stage_to_npz)\n",
        "\n",
        "        # held-out bar plots\n",
        "        plot_heldout_bars(algo, seed_name, seed_dir)\n",
        "\n",
        "print(\"All curves saved under:\", CURVES_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHiU4aOEj-7z",
        "outputId": "51b68d0c-00cd-4349-f43d-bcea1c894c9a"
      },
      "id": "vHiU4aOEj-7z",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing PPO / seed_0 ...\n",
            "All curves saved under: experiments/curves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"experiments_PPO\", \"zip\", \"experiments\")\n",
        "\n",
        "print(\"Zipped to experiments_PPO.zip\")"
      ],
      "metadata": {
        "id": "i0oLMRwtFbhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c28bf3dd-aeb8-43cd-f07d-070fa11883bb"
      },
      "id": "i0oLMRwtFbhQ",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipped to experiments_PPO.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZWIeaOMj_mO"
      },
      "id": "sZWIeaOMj_mO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}