{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15c876cf",
      "metadata": {
        "id": "15c876cf"
      },
      "source": [
        "# NonCurriculum_MetaDrive_SB3_Experiments\n",
        "\n",
        "This notebook contains a full, reproducible experiment pipeline for **non-curriculum** based reinforcement learning for autonomous driving using **MetaDrive** and **Stable Baselines3 (SB3)**. It includes:\n",
        "\n",
        "- Full environment factory and wrappers (including a discrete-action wrapper for DQN).\n",
        "- Exact stage definitions (C0..C3) and matching budgets.\n",
        "- Non-curriculum runner (train each target map separately for the same total sample budget).\n",
        "- Evaluation harness (metrics logging, CSV saving, TensorBoard integration, video recording).\n",
        "- Hyperparameters and experiment folder conventions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8b7750",
      "metadata": {
        "id": "8f8b7750"
      },
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y metadrive metadrive-simulator metadrive-simulator-py3-12 || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KG6c_nUg1EH",
        "outputId": "72a06556-fee3-4e0c-c72a-3c97e114823f"
      },
      "id": "9KG6c_nUg1EH",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping metadrive as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator-py3-12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bbe3fc75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe3fc75",
        "outputId": "04f5560c-54cf-48a1-82a7-d2b34ab2c93d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# !pip install stable-baselines3[extra] gymnasium metadrive numpy pandas matplotlib tensorboard opencv-python\n",
        "# !pip install stable-baselines3[extra] tensorboard opencv-python\n",
        "!pip install -q \"stable-baselines3[extra]\" \"metadrive-simulator-py3-12\" tensorboard opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import metadrive\n",
        "\n",
        "print(\"metadrive.__file__ :\", getattr(metadrive, \"__file__\", None))\n",
        "print(\"metadrive.__path__ :\", getattr(metadrive, \"__path__\", None))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDpNIf_ug8-1",
        "outputId": "b41dc633-0cc7-49ea-88c2-cf5f224dacc8"
      },
      "id": "MDpNIf_ug8-1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadrive.__file__ : /usr/local/lib/python3.12/dist-packages/metadrive/__init__.py\n",
            "metadrive.__path__ : ['/usr/local/lib/python3.12/dist-packages/metadrive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/metadriverse/metadrive.git\n",
        "# %cd metadrive\n",
        "# !pip install -e .\n"
      ],
      "metadata": {
        "id": "vAvVvccaKQJQ"
      },
      "id": "vAvVvccaKQJQ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m metadrive.pull_asset\n"
      ],
      "metadata": {
        "id": "yD7V6MWtKbUn"
      },
      "id": "yD7V6MWtKbUn",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m metadrive.examples.profile_metadrive\n"
      ],
      "metadata": {
        "id": "-zr38W64KdZd"
      },
      "id": "-zr38W64KdZd",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content\n"
      ],
      "metadata": {
        "id": "Kf1vBH1BKfpt"
      },
      "id": "Kf1vBH1BKfpt",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import metadrive, inspect, os\n",
        "print(\"Using metadrive from:\", metadrive.__file__)\n",
        "print(\"Contents:\", os.listdir(os.path.dirname(metadrive.__file__)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXDForLMfJsl",
        "outputId": "91bdc87c-8169-444d-8322-0586de5e08b9"
      },
      "id": "ZXDForLMfJsl",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using metadrive from: /usr/local/lib/python3.12/dist-packages/metadrive/__init__.py\n",
            "Contents: ['type.py', '__init__.py', '__pycache__', 'constants.py', 'envs', 'base_class', 'policy', 'obs', 'engine', 'examples', 'utils', 'component', 'tests', 'shaders', 'version.py', 'pull_asset.py', 'scenario', 'manager', 'render_pipeline', 'third_party']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Imports"
      ],
      "metadata": {
        "id": "kKG8RN5zKYtb"
      },
      "id": "kKG8RN5zKYtb"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e81fb189",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e81fb189",
        "outputId": "c85f1944-2989-4ce5-a684-71dafff41beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import logging\n",
        "\n",
        "# SB3 imports\n",
        "from metadrive.envs.metadrive_env import MetaDriveEnv\n",
        "from metadrive.envs.varying_dynamics_env import VaryingDynamicsEnv\n",
        "from stable_baselines3 import PPO, SAC, DQN\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# MetaDrive import guard\n",
        "# try:\n",
        "# from metadrive.envs.metadrive_env import MetaDriveEnv\n",
        "# except Exception as e:\n",
        "    # MetaDriveEnv = None\n",
        "    # print('MetaDrive import failed.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")                       # ignore everything\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", module=\"stable_baselines3\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n"
      ],
      "metadata": {
        "id": "PpACB8RcK8m8"
      },
      "id": "PpACB8RcK8m8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cc3da0a2",
      "metadata": {
        "id": "cc3da0a2"
      },
      "source": [
        "## 3. Experiment configuration\n",
        "setup: maps, stages, budgets, hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e386723b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e386723b",
        "outputId": "1bfdf063-f4fb-4814-caa4-540bd03f5926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total curriculum budget (per algorithm) = 650000\n"
          ]
        }
      ],
      "source": [
        "### Maps, stages and budgets\n",
        "# STAGES = [\n",
        "#     (\"C0_Straight\", \"Straight\", 0.0, 200_000),\n",
        "#     (\"C1_Curve\", \"Curve\", 0.0, 300_000),\n",
        "#     (\"C2_Roundabout\",\"Roundabout\",0.0,400_000),\n",
        "#     (\"C3_Dynamic\", \"20-block\", 0.3, 400_000),\n",
        "# ]\n",
        "\n",
        "### Maps, curriculum stages and budgets (with reward configs)\n",
        "\n",
        "STAGES = [\n",
        "    # C0: straight road, no traffic – just learn to go forward safely.\n",
        "    {\n",
        "        \"id\": \"C0\",\n",
        "        \"name\": \"C0_Straight\",\n",
        "        \"env_type\": \"general\",\n",
        "        \"map\": \"S\", # Straight\n",
        "        \"traffic\": 0.0,\n",
        "        \"budget\": 100_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,              # scale MetaDrive base reward\n",
        "            \"speed_w\": 0.05,            # weak speed shaping\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -5.0,\n",
        "            \"offroad_penalty\": -3.0,\n",
        "            \"traffic_violation_penalty\": -2.0,\n",
        "            \"success_bonus\": 10.0,\n",
        "            \"step_penalty\": -0.001,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C1: roundabout, no traffic – topology harder, still single-ego.\n",
        "    {\n",
        "        \"id\": \"C1\",\n",
        "        \"name\": \"C1_Roundabout\",\n",
        "        \"env_type\": \"general\",\n",
        "        \"map\": \"O\", # Roundabout\n",
        "        \"traffic\": 0.0,\n",
        "        \"budget\": 150_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.05,\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -6.0,  # slightly harsher for bad manoeuvres\n",
        "            \"offroad_penalty\": -4.0,\n",
        "            \"traffic_violation_penalty\": -3.0,\n",
        "            \"success_bonus\": 10.0,\n",
        "            \"step_penalty\": -0.005,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C2: 20-block PG map with **light traffic** – first exposure to traffic.\n",
        "    {\n",
        "        \"id\": \"C2\",\n",
        "        \"name\": \"C2_LightTraffic\",\n",
        "        \"map\": 10, # 20-block\n",
        "        \"traffic\": 0.05,               # light traffic\n",
        "        \"budget\": 200_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.08,            # encourage moving at speed in traffic\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -8.0,\n",
        "            \"offroad_penalty\": -6.0,\n",
        "            \"traffic_violation_penalty\": -4.0,\n",
        "            \"success_bonus\": 12.0,\n",
        "            \"step_penalty\": -0.01,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C3: same PG map with **dense traffic** – “multi-agent-ish” final stage.\n",
        "    # Still single learning ego, but many interacting vehicles (like CuRLA's\n",
        "    # higher-traffic final curriculum stage).\n",
        "    {\n",
        "        \"id\": \"C3\",\n",
        "        \"name\": \"C3_DenseTraffic\",\n",
        "        \"map\": 20,\n",
        "        \"traffic\": 0.30,               # dense traffic ≈ mild multi-agent\n",
        "        \"budget\": 200_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.10,\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -10.0, # strong safety pressure\n",
        "            \"offroad_penalty\": -8.0,\n",
        "            \"traffic_violation_penalty\": -5.0,\n",
        "            \"success_bonus\": 15.0,\n",
        "            \"step_penalty\": -0.05,\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "TOTAL_CURRICULUM_BUDGET = sum(s[\"budget\"] for s in STAGES)\n",
        "print(\"Total curriculum budget (per algorithm) =\", TOTAL_CURRICULUM_BUDGET)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Held-out 1: fixed 6-block map SCrRXO with medium traffic\n",
        "HELDOUT_SCENARIO_STAGE = {\n",
        "    \"id\": \"HELDOUT_SCENARIO\",\n",
        "    \"name\": \"HELDOUT_SCrRXO_MedTraffic\",\n",
        "    \"map\": \"SCrRXO\", # Straight -> Circular -> in-ramp -> out-ramp -> intersection -> roundabout\n",
        "    \"traffic\": 0.30, # medium-ish traffic\n",
        "    \"budget\": 0, # no training budget, eval only\n",
        "    \"reward\": STAGES[-1][\"reward\"],  # reuse hardest-stage shaping for comparability\n",
        "}\n",
        "\n",
        "# Held-out 2: VaryingDynamics environment (dynamics robustness test)\n",
        "VARYING_DYNAMICS_CONFIG = dict(\n",
        "    num_scenarios=100,\n",
        "    map=5,                  # small PG map\n",
        "    log_level=logging.ERROR,\n",
        "    # random_dynamics uses default ranges from docs; that's enough to show robustness\n",
        ")"
      ],
      "metadata": {
        "id": "B-HaQNeltf-R"
      },
      "id": "B-HaQNeltf-R",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder convention\n",
        "EXPERIMENT_ROOT = Path('experiments')\n",
        "EXPERIMENT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "# Seeds and workers\n",
        "SEEDS = [0]\n",
        "N_ENVS = 1 # should be 8 but metadrive issues\n",
        "EVAL_FREQ = 10_000\n",
        "EVAL_EPISODES = 5\n",
        "\n",
        "# Held-out test map\n",
        "# HELDOUT_MAP = (\"Fork\", 0.2)"
      ],
      "metadata": {
        "id": "w_ndV2-1wbQZ"
      },
      "id": "w_ndV2-1wbQZ",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "HYPERS = {\n",
        "    'PPO': {\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[64,64]}, # can do 64, 64 as well\n",
        "        'learning_rate':3e-4,\n",
        "        'n_steps':1024, # compute issue, can do 2048 if faster\n",
        "        'batch_size':64,\n",
        "        'n_epochs':10,\n",
        "        'gamma':0.99,\n",
        "        'clip_range':0.2\n",
        "    },\n",
        "    'SAC': {\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[256,256]},\n",
        "        'learning_rate':3e-4,\n",
        "        'batch_size':256,\n",
        "        'buffer_size':100_000,\n",
        "        'gamma':0.99\n",
        "    },\n",
        "    'DQN': { # Atari setup\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[64,64]},\n",
        "        'learning_rate':1e-4,\n",
        "        'buffer_size':100_000, # can do 50,000 if needed\n",
        "        'batch_size':32,\n",
        "        'train_freq':4\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "HjNj-DhhwaJR"
      },
      "id": "HjNj-DhhwaJR",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e299413",
      "metadata": {
        "id": "0e299413"
      },
      "source": [
        "Includes a discrete-action wrapper for DQN (maps discrete indices -> continuous steer/throttle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3e0cb97e",
      "metadata": {
        "id": "3e0cb97e"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class DiscreteActionWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env, mapping):\n",
        "        super().__init__(env)\n",
        "        self.mapping = mapping\n",
        "        self.action_space = spaces.Discrete(len(mapping))\n",
        "\n",
        "    def action(self, action):\n",
        "        return np.array(self.mapping[action], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CurriculumRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Stage-dependent reward shaping:\n",
        "    - Start from MetaDrive's base reward.\n",
        "    - Add speed term.\n",
        "    - Add collision / off-road / traffic-violation penalties.\n",
        "    - Add success bonus.\n",
        "\n",
        "    Uses the per-stage reward config from STAGES.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, reward_cfg):\n",
        "        super().__init__(env)\n",
        "        self.cfg = reward_cfg\n",
        "        self.max_speed = self.cfg.get(\"max_speed_kmh\", 80.0)\n",
        "\n",
        "    def step(self, action):\n",
        "        # MetaDrive uses Gymnasium API: obs, reward, terminated, truncated, info\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # --- speed term ---\n",
        "        # MetaDrive usually exposes speed either as 'speed' or 'velocity'\n",
        "        raw_speed = float(info.get(\"speed\", info.get(\"velocity\", 0.0)))\n",
        "        speed = max(0.0, min(raw_speed, self.max_speed))\n",
        "        speed_term = self.cfg.get(\"speed_w\", 0.0) * (speed / self.max_speed)\n",
        "\n",
        "        # --- start from scaled base reward + speed shaping ---\n",
        "        r = self.cfg.get(\"base_w\", 1.0) * base_r + speed_term\n",
        "\n",
        "        # --- per-step cost (encourage finishing sooner) ---\n",
        "        r += self.cfg.get(\"step_penalty\", 0.0)\n",
        "\n",
        "        # --- collision penalties ---\n",
        "        crashed = (\n",
        "            info.get(\"crash_vehicle\", False)\n",
        "            or info.get(\"crash_object\", False)\n",
        "            or info.get(\"crash_building\", False)\n",
        "        )\n",
        "        if crashed:\n",
        "            r += self.cfg.get(\"collision_penalty\", 0.0)\n",
        "        info[\"collision\"] = bool(crashed)\n",
        "\n",
        "        # --- off-road / traffic-violation penalties ---\n",
        "        offroad = info.get(\"out_of_road\", False)\n",
        "        if offroad:\n",
        "            r += self.cfg.get(\"offroad_penalty\", 0.0)\n",
        "\n",
        "        # generic \"traffic violation\" flag for your metrics callback\n",
        "        traffic_violation = bool(offroad or info.get(\"traffic_light_violation\", False))\n",
        "        if traffic_violation:\n",
        "            r += self.cfg.get(\"traffic_violation_penalty\", 0.0)\n",
        "        info[\"traffic_violation\"] = traffic_violation\n",
        "\n",
        "        # --- success bonus at terminal step ---\n",
        "        success = bool(info.get(\"arrive_dest\", False) or info.get(\"success\", False))\n",
        "        if terminated and success:\n",
        "            r += self.cfg.get(\"success_bonus\", 0.0)\n",
        "        info[\"success\"] = success\n",
        "\n",
        "        # For logging\n",
        "        info[\"avg_speed\"] = speed\n",
        "        info[\"shaped_reward\"] = r\n",
        "\n",
        "        return obs, r, terminated, truncated, info"
      ],
      "metadata": {
        "id": "wLTVGUAADX1r"
      },
      "id": "wLTVGUAADX1r",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "75c2b81f",
      "metadata": {
        "id": "75c2b81f"
      },
      "source": [
        "## 4. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Environment factory"
      ],
      "metadata": {
        "id": "nGXJIcxPxSfh"
      },
      "id": "nGXJIcxPxSfh"
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "class MetaDriveGymCompatibilityWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Makes MetaDriveEnv follow the Gymnasium reset() and step() signature.\n",
        "    Removes unsupported arguments like options.\n",
        "    \"\"\"\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            obs, info = self.env.reset(seed=seed)\n",
        "        else:\n",
        "            obs, info = self.env.reset()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        # MetaDrive uses done only; Gymnasium expects (terminated, truncated)\n",
        "        terminated = done\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "vKLjHVxvkKCh"
      },
      "id": "vKLjHVxvkKCh",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import logging\n",
        "\n",
        "def make_metadrive_env(stage, use_discrete=False, seed=0, render=False):\n",
        "    \"\"\"\n",
        "    stage: one of the dicts from STAGES. Uses:\n",
        "        stage[\"map\"], stage[\"traffic\"], stage[\"reward\"]\n",
        "    \"\"\"\n",
        "    map_name = stage[\"map\"]\n",
        "    traffic_density = stage[\"traffic\"]\n",
        "    reward_cfg = stage[\"reward\"]\n",
        "\n",
        "    def _init():\n",
        "        cfg = {\n",
        "            \"map\": map_name,\n",
        "            \"traffic_density\": traffic_density,\n",
        "            \"use_render\": render,\n",
        "            \"start_seed\": seed,\n",
        "            # \"random_spawn\": True,\n",
        "            \"debug\": False,\n",
        "            \"log_level\": logging.ERROR,\n",
        "            # cap episode length so eval can't run forever\n",
        "            \"horizon\": 1000, # max steps per episode\n",
        "            \"truncate_as_terminate\": True,   # treat horizon as done\n",
        "        }\n",
        "        env = MetaDriveEnv(cfg)\n",
        "\n",
        "        # Fix reset() signature mismatch\n",
        "        env = MetaDriveGymCompatibilityWrapper(env)\n",
        "\n",
        "        # CuRLA-style stage-dependent reward shaping\n",
        "        env = CurriculumRewardWrapper(env, reward_cfg)\n",
        "\n",
        "        # discrete-action wrapper for DQN: [steering, throttle]\n",
        "        if use_discrete:\n",
        "            mapping = [\n",
        "              (-1.0, 0.0), # hard left, no throttle\n",
        "              (-1.0, 0.3), # left + some accel\n",
        "              (0.0, 0.5), # go straight, accel\n",
        "              (1.0, 0.3), # right + some accel\n",
        "              (1.0, 0.0), # hard right, no throttle\n",
        "            ]\n",
        "            env = DiscreteActionWrapper(env, mapping)\n",
        "\n",
        "        return Monitor(env)\n",
        "\n",
        "    return _init\n"
      ],
      "metadata": {
        "id": "sv070jNaDnyR"
      },
      "id": "sv070jNaDnyR",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_vec_env(stage, n_envs=8, use_discrete=False, seed=0, parallel=False):\n",
        "#     factories = [\n",
        "#         make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "#         for i in range(n_envs)\n",
        "#     ]\n",
        "#     if parallel:\n",
        "#         return SubprocVecEnv(factories)\n",
        "#     else:\n",
        "#         return DummyVecEnv(factories)\n",
        "\n",
        "# from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "\n",
        "# def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n",
        "#     \"\"\"\n",
        "#     Create vectorized MetaDrive envs.\n",
        "#     IMPORTANT: MetaDrive can only have one engine per process.\n",
        "#     So:\n",
        "#       - n_envs == 1  -> use DummyVecEnv (single process, single env)\n",
        "#       - n_envs > 1   -> use SubprocVecEnv (one env per process)\n",
        "#     \"\"\"\n",
        "#     if n_envs == 1:\n",
        "#         return DummyVecEnv([\n",
        "#             make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n",
        "#         ])\n",
        "#     else:\n",
        "#         env_fns = [\n",
        "#             make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "#             for i in range(n_envs)\n",
        "#         ]\n",
        "#         return SubprocVecEnv(env_fns)\n",
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n",
        "    \"\"\"\n",
        "    Create vectorized MetaDrive envs.\n",
        "    FIX: We must ALWAYS use SubprocVecEnv for MetaDrive.\n",
        "    Using DummyVecEnv (single process) prevents creating a second env\n",
        "    (like eval_env) because MetaDrive allows only one engine per process.\n",
        "    \"\"\"\n",
        "    # env_fns = [\n",
        "    #     make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "    #     for i in range(n_envs)\n",
        "    # ]\n",
        "\n",
        "    # # FORCE SubprocVecEnv even if n_envs=1\n",
        "    # return SubprocVecEnv(env_fns)\n",
        "\n",
        "    return DummyVecEnv([\n",
        "        make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n",
        "    ])"
      ],
      "metadata": {
        "id": "v9TCh0Q_DpL4"
      },
      "id": "v9TCh0Q_DpL4",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO/SAC use continous control (`Box`) from MetaDrive while DQN uses a manually defined discrete control space."
      ],
      "metadata": {
        "id": "dTkPoInXfp4F"
      },
      "id": "dTkPoInXfp4F"
    },
    {
      "cell_type": "code",
      "source": [
        "vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=False, seed=0)\n",
        "base_env = vec.envs[0]    # DummyVecEnv\n",
        "print(\"PPO/SAC action space:\", base_env.action_space)\n",
        "print(\"obs space:\", base_env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-OkesJEflDV",
        "outputId": "fd9ddf9a-66b9-4fc1-8f4f-cfd080b6f07e"
      },
      "id": "Q-OkesJEflDV",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPO/SAC action space: Box(-1.0, 1.0, (2,), float32)\n",
            "obs space: Box(-0.0, 1.0, (259,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=True, seed=0)\n",
        "base_env = vec.envs[0]    # DummyVecEnv\n",
        "print(\"DQN action space:\", base_env.action_space)\n",
        "print(\"obs space:\", base_env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA49LfETfmvF",
        "outputId": "4595fcb3-d858-494e-b049-c121d5f0cb62"
      },
      "id": "TA49LfETfmvF",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN action space: Discrete(5)\n",
            "obs space: Box(-0.0, 1.0, (259,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_eval_vec_env(stage, use_discrete=False, seed=0):\n",
        "    \"\"\"\n",
        "    Eval env:\n",
        "    - Always uses SubprocVecEnv (even with 1 worker) so that MetaDriveEnv\n",
        "      is only ever created in subprocesses, not in the main process.\n",
        "    \"\"\"\n",
        "    env_fns = [make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)]\n",
        "    return SubprocVecEnv(env_fns)\n"
      ],
      "metadata": {
        "id": "hJa0pGDE4Z1J"
      },
      "id": "hJa0pGDE4Z1J",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34d56696",
      "metadata": {
        "id": "34d56696"
      },
      "source": [
        "### 4.2 log per-eval metrics (success rate, collisions, speed etc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Run a short evaluation every eval_freq steps and log:\n",
        "      - mean_reward\n",
        "      - success_rate\n",
        "      - collision_rate\n",
        "      - traffic_violation_rate\n",
        "      - avg_speed\n",
        "      - avg_episode_length\n",
        "\n",
        "    Saves to a CSV at csv_path.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eval_env, csv_path, eval_freq=50_000, eval_episodes=10, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.eval_env = eval_env\n",
        "        self.csv_path = csv_path\n",
        "        self.eval_freq = eval_freq\n",
        "        self.eval_episodes = eval_episodes\n",
        "\n",
        "        # Create dir if needed\n",
        "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "        # Write header if file doesn't exist\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "                writer = csv.DictWriter(\n",
        "                    f,\n",
        "                    fieldnames=[\n",
        "                        \"timesteps\",\n",
        "                        \"mean_reward\",\n",
        "                        \"success_rate\",\n",
        "                        \"collision_rate\",\n",
        "                        \"traffic_violation_rate\",\n",
        "                        \"avg_speed\",\n",
        "                        \"avg_episode_length\",\n",
        "                    ],\n",
        "                )\n",
        "                writer.writeheader()\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Only evaluate every eval_freq calls\n",
        "        if self.n_calls % self.eval_freq != 0:\n",
        "            return True\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_successes = []\n",
        "        episode_collisions = []\n",
        "        episode_traffic_violations = []\n",
        "        episode_speeds = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for _ in range(self.eval_episodes):\n",
        "            # DummyVecEnv.reset() -> obs (no info, vec-batched)\n",
        "            obs = self.eval_env.reset()\n",
        "            info = {}\n",
        "            done = False\n",
        "\n",
        "            ep_reward = 0.0\n",
        "            ep_success = False\n",
        "            ep_collision = False\n",
        "            ep_traffic_violation = False\n",
        "            ep_steps = 0\n",
        "            ep_speeds = []\n",
        "\n",
        "            while not done:\n",
        "                # obs shape: (n_envs, obs_dim); n_envs = 1 here\n",
        "                action, _ = self.model.predict(obs, deterministic=True)\n",
        "                # DummyVecEnv.step() -> obs, rewards, dones, infos\n",
        "                obs, rewards, dones, infos = self.eval_env.step(action)\n",
        "\n",
        "                # unwrap vec env outputs for single env\n",
        "                if isinstance(rewards, (np.ndarray, list, tuple)):\n",
        "                    r = float(rewards[0])\n",
        "                else:\n",
        "                    r = float(rewards)\n",
        "\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    d = bool(dones[0])\n",
        "                else:\n",
        "                    d = bool(dones)\n",
        "\n",
        "                if isinstance(infos, (list, tuple)) and len(infos) > 0:\n",
        "                    info = infos[0]\n",
        "                else:\n",
        "                    info = infos\n",
        "\n",
        "                ep_reward += r\n",
        "                ep_steps += 1\n",
        "                done = d\n",
        "\n",
        "                # if isinstance(info, dict):\n",
        "                #     if info.get(\"success\", False):\n",
        "                #         ep_success = True\n",
        "                #     if info.get(\"collision\", False):\n",
        "                #         ep_collision = True\n",
        "                #     if info.get(\"traffic_violation\", False):\n",
        "                #         ep_traffic_violation = True\n",
        "\n",
        "                #     if \"avg_speed\" in info:\n",
        "                #         ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "                #     elif \"speed\" in info:\n",
        "                #         ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "                # flags from MetaDrive info / our wrapper\n",
        "                if isinstance(info, dict):\n",
        "                    # success: either our wrapper's \"success\" OR MetaDrive's arrive_dest\n",
        "                    if info.get(\"success\", False) or info.get(\"arrive_dest\", False):\n",
        "                        ep_success = True\n",
        "\n",
        "                    # collision: either our wrapper's \"collision\" OR any crash/out-of-road\n",
        "                    if (\n",
        "                        info.get(\"collision\", False)\n",
        "                        or info.get(\"crash_vehicle\", False)\n",
        "                        or info.get(\"crash_object\", False)\n",
        "                        or info.get(\"crash_building\", False)\n",
        "                        or info.get(\"out_of_road\", False)\n",
        "                    ):\n",
        "                        ep_collision = True\n",
        "\n",
        "                    # traffic violation if we ever log it; otherwise this will stay 0\n",
        "                    if info.get(\"traffic_violation\", False):\n",
        "                        ep_traffic_violation = True\n",
        "\n",
        "                    # speed logging\n",
        "                    if \"avg_speed\" in info:\n",
        "                        ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "                    elif \"speed\" in info:\n",
        "                        ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "\n",
        "            episode_rewards.append(ep_reward)\n",
        "            episode_successes.append(1.0 if ep_success else 0.0)\n",
        "            episode_collisions.append(1.0 if ep_collision else 0.0)\n",
        "            episode_traffic_violations.append(1.0 if ep_traffic_violation else 0.0)\n",
        "            episode_lengths.append(ep_steps)\n",
        "            if ep_speeds:\n",
        "                episode_speeds.append(sum(ep_speeds) / len(ep_speeds))\n",
        "\n",
        "        mean_reward = float(sum(episode_rewards) / len(episode_rewards)) if episode_rewards else 0.0\n",
        "        success_rate = float(sum(episode_successes) / len(episode_successes)) if episode_successes else 0.0\n",
        "        collision_rate = float(sum(episode_collisions) / len(episode_collisions)) if episode_collisions else 0.0\n",
        "        traffic_violation_rate = float(sum(episode_traffic_violations) / len(episode_traffic_violations)) if episode_traffic_violations else 0.0\n",
        "        avg_speed = float(sum(episode_speeds) / len(episode_speeds)) if episode_speeds else 0.0\n",
        "        avg_episode_length = float(sum(episode_lengths) / len(episode_lengths)) if episode_lengths else 0.0\n",
        "\n",
        "        row = {\n",
        "            \"timesteps\": int(self.num_timesteps),\n",
        "            \"mean_reward\": mean_reward,\n",
        "            \"success_rate\": success_rate,\n",
        "            \"collision_rate\": collision_rate,\n",
        "            \"traffic_violation_rate\": traffic_violation_rate,\n",
        "            \"avg_speed\": avg_speed,\n",
        "            \"avg_episode_length\": avg_episode_length,\n",
        "        }\n",
        "\n",
        "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "            writer.writerow(row)\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print(f\"[Metrics] t={self.num_timesteps}  succ={success_rate:.2f}  \"\n",
        "                  f\"coll={collision_rate:.2f}  len={avg_episode_length:.1f}\")\n",
        "\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "zGjpX3W1FZsu"
      },
      "id": "zGjpX3W1FZsu",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class PrettyEvalCallback(EvalCallback):\n",
        "    \"\"\"\n",
        "    Clean pretty printing for eval:\n",
        "      - One separator before the first eval\n",
        "      - No spam between evals\n",
        "      - One separator at the end of training on this stage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._started = False    # whether we have printed the first separator\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Check if it's time to evaluate\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            # On first eval, print top separator\n",
        "            if not self._started:\n",
        "                print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "                self._started = True\n",
        "\n",
        "        return super()._on_step()\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        # After training for this stage: print final separator\n",
        "        if self._started:\n",
        "            print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "        return super()._on_training_end()\n"
      ],
      "metadata": {
        "id": "-eFLK2DKKDJR"
      },
      "id": "-eFLK2DKKDJR",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_run_config(out_dir, algo, stage, seed):\n",
        "    \"\"\"\n",
        "    Save basic run config (algo, stage, hyperparams, seed) to config.json\n",
        "    so you can reproduce / inspect later.\n",
        "    \"\"\"\n",
        "    cfg = {\n",
        "        \"algo\": algo,\n",
        "        \"seed\": seed,\n",
        "        \"stage\": {\n",
        "            \"id\": stage[\"id\"],\n",
        "            \"name\": stage[\"name\"],\n",
        "            \"map\": stage[\"map\"],\n",
        "            \"traffic\": stage[\"traffic\"],\n",
        "            \"budget\": stage[\"budget\"],\n",
        "            \"reward\": stage[\"reward\"],\n",
        "        },\n",
        "        \"hyperparams\": HYPERS[algo],\n",
        "        \"heldout_map\": {\"map\": HELDOUT_SCENARIO_STAGE['map'], \"traffic\": HELDOUT_SCENARIO_STAGE['traffic'], \"reward\": HELDOUT_SCENARIO_STAGE['reward']},\n",
        "    }\n",
        "    with open(out_dir / \"config.json\", \"w\") as f:\n",
        "        json.dump(cfg, f, indent=2)"
      ],
      "metadata": {
        "id": "rPOxRfNNHKbp"
      },
      "id": "rPOxRfNNHKbp",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(HELDOUT_SCENARIO_STAGE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soBC1415it33",
        "outputId": "c7de241a-2e41-40b9-a66f-ccc756c6c5e8"
      },
      "id": "soBC1415it33",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'HELDOUT_SCENARIO', 'name': 'HELDOUT_SCrRXO_MedTraffic', 'map': 'SCrRXO', 'traffic': 0.3, 'budget': 0, 'reward': {'base_w': 1.0, 'speed_w': 0.1, 'max_speed_kmh': 80.0, 'collision_penalty': -10.0, 'offroad_penalty': -8.0, 'traffic_violation_penalty': -5.0, 'success_bonus': 15.0, 'step_penalty': -0.05}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0303bc4f",
      "metadata": {
        "id": "0303bc4f"
      },
      "source": [
        "### 4.3 Training functions\n",
        "\n",
        "These functions create models, attach callbacks, and run training. Each saves checkpoint, best-model and CSV metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def make_model(algo, env, hyperparams):\n",
        "    common_kwargs = dict(\n",
        "        verbose=0,  # make SB3 quiet in console\n",
        "        tensorboard_log=str(EXPERIMENT_ROOT / 'tensorboard'),\n",
        "        policy_kwargs=hyperparams['policy_kwargs'],\n",
        "        learning_rate=hyperparams['learning_rate'],\n",
        "    )\n",
        "\n",
        "    if algo == 'PPO':\n",
        "        model = PPO(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            n_steps=hyperparams['n_steps'],\n",
        "            batch_size=hyperparams['batch_size'],\n",
        "            n_epochs=hyperparams['n_epochs'],\n",
        "            gamma=hyperparams['gamma'],\n",
        "            device=\"cpu\",\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if algo == 'SAC':\n",
        "        model = SAC(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            batch_size=hyperparams.get('batch_size', 256),\n",
        "            buffer_size=hyperparams.get('buffer_size', 100_000),\n",
        "            gamma=hyperparams.get('gamma', 0.99),\n",
        "            device=DEVICE,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if algo == 'DQN':\n",
        "        model = DQN(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            buffer_size=hyperparams.get('buffer_size', 50_000),\n",
        "            batch_size=hyperparams.get('batch_size', 32),\n",
        "            train_freq=hyperparams.get('train_freq', 4),\n",
        "            device=DEVICE,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    raise ValueError('Unknown algo')\n"
      ],
      "metadata": {
        "id": "yLzoiNexEQ6n"
      },
      "id": "yLzoiNexEQ6n",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_env_episodes(model, env, n_episodes=20, deterministic=True):\n",
        "    \"\"\"\n",
        "    Run n_episodes on a *single* (non-vec) env.\n",
        "    Returns (mean_reward, std_reward, mean_ep_len).\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        reset_out = env.reset()\n",
        "        # Handle reset() returning either obs or (obs, info)\n",
        "        if isinstance(reset_out, tuple):\n",
        "            obs, _info = reset_out\n",
        "        else:\n",
        "            obs = reset_out\n",
        "        info = {}\n",
        "        done = False\n",
        "        truncated = False\n",
        "        ep_r = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        while not (done or truncated):\n",
        "            action, _ = model.predict(obs, deterministic=deterministic)\n",
        "            step_out = env.step(action)\n",
        "            # step could be:\n",
        "            #  - (obs, reward, done, info)  [old Gym]\n",
        "            #  - (obs, reward, terminated, truncated, info) [Gymnasium]\n",
        "            if len(step_out) == 5:\n",
        "                obs, r, terminated, truncated, info = step_out\n",
        "                done = bool(terminated or truncated)\n",
        "            elif len(step_out) == 4:\n",
        "                obs, r, done, info = step_out\n",
        "                truncated = False\n",
        "            else:\n",
        "                raise RuntimeError(f\"Unexpected env.step() output length: {len(step_out)}\")\n",
        "\n",
        "            truncated = False\n",
        "            ep_r += float(r)\n",
        "            steps += 1\n",
        "\n",
        "        rewards.append(ep_r)\n",
        "        lengths.append(steps)\n",
        "\n",
        "    rewards = np.array(rewards, dtype=np.float32)\n",
        "    lengths = np.array(lengths, dtype=np.float32)\n",
        "    return float(rewards.mean()), float(rewards.std()), float(lengths.mean())\n"
      ],
      "metadata": {
        "id": "z5yA8SnOtpCg"
      },
      "id": "z5yA8SnOtpCg",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=1):\n",
        "    \"\"\"\n",
        "    Non-curriculum baseline:\n",
        "    - Train from scratch on a SINGLE stage for 'total_timesteps'.\n",
        "    - Eval during training on SAME env via PrettyEvalCallback (already set up).\n",
        "    - After training, eval on two held-out envs:\n",
        "        1) SCrRXO + medium traffic (MetaDriveEnv)\n",
        "        2) VaryingDynamicsEnv (randomized dynamics)\n",
        "    \"\"\"\n",
        "\n",
        "    out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Training NON-CURRICULUM: {algo} {stage['name']} seed {seed}\\n\")\n",
        "\n",
        "    use_discrete = (algo == \"DQN\")\n",
        "\n",
        "    # ---- training env (single vec env) ----\n",
        "    env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
        "\n",
        "    model = make_model(algo, env, HYPERS[algo])\n",
        "\n",
        "    # in-training eval (same env)\n",
        "    eval_cb = PrettyEvalCallback(\n",
        "        env,\n",
        "        best_model_save_path=str(out_dir / \"best_model\"),\n",
        "        log_path=str(out_dir / \"eval\"),\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        n_eval_episodes=EVAL_EPISODES,\n",
        "        deterministic=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    ckpt_cb = CheckpointCallback(\n",
        "        save_freq=EVAL_FREQ,\n",
        "        save_path=str(out_dir / \"checkpoints\"),\n",
        "        name_prefix=\"ckpt\",\n",
        "    )\n",
        "\n",
        "    metrics_csv = out_dir / \"metrics.csv\"\n",
        "    metrics_cb = MetricsCallback(\n",
        "        eval_env=env,       # since you’re using n_envs=1 (DummyVecEnv)\n",
        "        csv_path=str(metrics_csv),\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        eval_episodes=EVAL_EPISODES,\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    save_run_config(out_dir, algo, stage, seed)\n",
        "\n",
        "    # ---- train ----\n",
        "    model.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=[eval_cb, ckpt_cb, metrics_cb],\n",
        "    )\n",
        "    model.save(str(out_dir / \"model.zip\"))\n",
        "\n",
        "    print(f\"\\nNon-curriculum training complete and saved to {out_dir}\")\n",
        "\n",
        "    # Close training vec env to avoid engine conflicts before new envs\n",
        "    env.close()\n",
        "\n",
        "    # =====================================================================\n",
        "    # HELD-OUT 1: SCrRXO + medium traffic\n",
        "    # =====================================================================\n",
        "    print(\"\\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\")\n",
        "\n",
        "    # build a *single* env instance using same wrappers\n",
        "    held1_make = make_metadrive_env(\n",
        "        HELDOUT_SCENARIO_STAGE,\n",
        "        use_discrete=use_discrete,\n",
        "        seed=seed + 1000,\n",
        "        render=False,\n",
        "    )\n",
        "    held1_env = held1_make()\n",
        "    held1_env = MetaDriveGymCompatibilityWrapper(held1_env)\n",
        "\n",
        "    h1_mean, h1_std, h1_len = evaluate_env_episodes( #, h1_len if manual function\n",
        "        model,\n",
        "        held1_env,\n",
        "        n_episodes=20,\n",
        "        deterministic=True,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"HELD-OUT 1 (SCrRXO): mean_reward={h1_mean:.2f} ± {h1_std:.2f}, \"\n",
        "        f\"mean_ep_len={h1_len:.1f}\"\n",
        "    )\n",
        "\n",
        "    pd.DataFrame(\n",
        "        [{\n",
        "            \"algo\": algo,\n",
        "            \"train_stage\": stage[\"name\"],\n",
        "            \"heldout_name\": HELDOUT_SCENARIO_STAGE[\"name\"],\n",
        "            \"mean_reward\": h1_mean,\n",
        "            \"std_reward\": h1_std,\n",
        "            \"mean_ep_len\": h1_len,\n",
        "        }]\n",
        "    ).to_csv(out_dir / \"heldout_scrrxo_metrics.csv\", index=False)\n",
        "\n",
        "    held1_env.close()\n",
        "\n",
        "    # =====================================================================\n",
        "    # HELD-OUT 2: VaryingDynamicsEnv\n",
        "    # =====================================================================\n",
        "    # print(\"\\n[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\")\n",
        "\n",
        "    # # build varying dynamics env\n",
        "    # vd_env = VaryingDynamicsEnv(VARYING_DYNAMICS_CONFIG)\n",
        "    # vd_env = MetaDriveGymCompatibilityWrapper(vd_env)\n",
        "\n",
        "    # if use_discrete:\n",
        "    #     # same discrete mapping you used for training DQN\n",
        "    #     discrete_mapping = [(-1.0, 0.0), (-1.0, 0.3), (0.0, 0.5), (1.0, 0.3), (1.0, 0.0)]\n",
        "    #     vd_env = DiscreteActionWrapper(vd_env, discrete_mapping)\n",
        "\n",
        "    # h2_mean, h2_std, h2_len = evaluate_env_episodes(\n",
        "    #     model,\n",
        "    #     vd_env,\n",
        "    #     n_episodes=20,\n",
        "    #     deterministic=True,\n",
        "    # )\n",
        "\n",
        "    # print(\n",
        "    #     f\"HELD-OUT 2 (VaryingDynamics): mean_reward={h2_mean:.2f} ± {h2_std:.2f}, \"\n",
        "    #     f\"mean_ep_len={h2_len:.1f}\"\n",
        "    # )\n",
        "\n",
        "    # pd.DataFrame(\n",
        "    #     [{\n",
        "    #         \"algo\": algo,\n",
        "    #         \"train_stage\": stage[\"name\"],\n",
        "    #         \"heldout_name\": \"VaryingDynamicsEnv\",\n",
        "    #         \"mean_reward\": h2_mean,\n",
        "    #         \"std_reward\": h2_std,\n",
        "    #         \"mean_ep_len\": h2_len,\n",
        "    #     }]\n",
        "    # ).to_csv(out_dir / \"heldout_varying_metrics.csv\", index=False)\n",
        "\n",
        "    # vd_env.close()\n",
        "\n",
        "    return out_dir\n"
      ],
      "metadata": {
        "id": "eCc8tf5CtVuA"
      },
      "id": "eCc8tf5CtVuA",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83889cd2",
      "metadata": {
        "id": "83889cd2"
      },
      "source": [
        "### 4.4. Visualization (plot metrics, learning curves, and display videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "57e24184",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57e24184",
        "outputId": "1400ffa4-4aed-4dc2-8d7c-c22b56af3b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot helper ready\n"
          ]
        }
      ],
      "source": [
        "def plot_metrics(csv_path, title=None):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print('CSV not found:', csv_path); return\n",
        "    df = pd.read_csv(csv_path)\n",
        "    fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
        "    axs = axs.flatten()\n",
        "    axs[0].plot(df['total_timesteps'], df['mean_reward'], marker='o'); axs[0].set_title('Mean reward')\n",
        "    axs[1].plot(df['total_timesteps'], df['success_rate'], marker='o'); axs[1].set_title('Success rate')\n",
        "    axs[2].plot(df['total_timesteps'], df['collision_rate'], marker='o'); axs[2].set_title('Collision rate')\n",
        "    axs[3].plot(df['total_timesteps'], df['avg_speed'], marker='o'); axs[3].set_title('Avg speed')\n",
        "    if title: fig.suptitle(title)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "print('Plot helper ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644f2875",
      "metadata": {
        "id": "644f2875"
      },
      "source": [
        "## 5. Toy pilot run\n",
        "\n",
        "Small pilot run to validate pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "aef7a3ca",
      "metadata": {
        "id": "aef7a3ca"
      },
      "outputs": [],
      "source": [
        "# # run one tiny non-curriculum PPO for 2000 steps on Straight map\n",
        "\n",
        "# out = train_noncurriculum('SAC', STAGES[2], total_timesteps=2000, seed=0, n_envs=1)\n",
        "# print('Pilot saved at:', out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c6e5f6",
      "metadata": {
        "id": "95c6e5f6"
      },
      "source": [
        "## 6. Full experiment\n",
        "\n",
        "For each algorithm, for each seed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = [\"DQN\"] # \"SAC\", \"DQN\",\n",
        "\n",
        "for algo in algos:\n",
        "    for seed in SEEDS:\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"ALGO={algo}  SEED={seed}\")\n",
        "\n",
        "        for stage in STAGES:\n",
        "            train_noncurriculum(\n",
        "                algo=algo,\n",
        "                stage=stage,\n",
        "                # total_timesteps=5000,\n",
        "                total_timesteps=stage[\"budget\"],  # use this stage's budget\n",
        "                seed=seed,\n",
        "                n_envs=N_ENVS,\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPZ5O3NLHg94",
        "outputId": "c84bed6e-48d7-475d-9061-042ec74031b1"
      },
      "id": "xPZ5O3NLHg94",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ALGO=DQN  SEED=0\n",
            "Training NON-CURRICULUM: DQN C0_Straight seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=136.47 +/- 0.00\n",
            "Episode length: 128.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-7.64 +/- 0.00\n",
            "Episode length: 52.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=-8.69 +/- 0.00\n",
            "Episode length: 54.00 +/- 0.00\n",
            "Eval num_timesteps=40000, episode_reward=-9.18 +/- 0.00\n",
            "Episode length: 56.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=-9.19 +/- 0.00\n",
            "Episode length: 56.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=136.47 +/- 0.00\n",
            "Episode length: 128.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=136.47 +/- 0.00\n",
            "Episode length: 128.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=136.47 +/- 0.00\n",
            "Episode length: 128.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=136.47 +/- 0.00\n",
            "Episode length: 128.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=136.47 +/- 0.00\n",
            "Episode length: 128.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/DQN/noncurriculum/seed_0/C0_Straight\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=70.23 ± 0.00, mean_ep_len=111.0\n",
            "Training NON-CURRICULUM: DQN C1_Roundabout seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-4.98 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=-11.41 +/- 0.00\n",
            "Episode length: 56.00 +/- 0.00\n",
            "Eval num_timesteps=40000, episode_reward=-11.50 +/- 0.00\n",
            "Episode length: 15.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=140000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=47.12 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/DQN/noncurriculum/seed_0/C1_Roundabout\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=71.23 ± 0.00, mean_ep_len=112.0\n",
            "Training NON-CURRICULUM: DQN C2_LightTraffic seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=40000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=140000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=160000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=170000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=180000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=190000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=200000, episode_reward=38.91 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/DQN/noncurriculum/seed_0/C2_LightTraffic\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=70.23 ± 0.00, mean_ep_len=111.0\n",
            "Training NON-CURRICULUM: DQN C3_DenseTraffic seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-18.17 +/- 0.00\n",
            "Episode length: 15.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=-19.78 +/- 0.00\n",
            "Episode length: 56.00 +/- 0.00\n",
            "Eval num_timesteps=40000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=140000, episode_reward=33.53 +/- 0.00\n",
            "Episode length: 89.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=150000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=160000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=170000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=180000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=190000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "Eval num_timesteps=200000, episode_reward=32.53 +/- 0.00\n",
            "Episode length: 88.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/DQN/noncurriculum/seed_0/C3_DenseTraffic\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=65.31 ± 0.00, mean_ep_len=110.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "bde3ba34",
      "metadata": {
        "id": "bde3ba34"
      },
      "outputs": [],
      "source": [
        "# algos = ['PPO','SAC','DQN']\n",
        "# for algo in algos:\n",
        "#     # non-curriculum: for each target map train for the total curriculum budget (to match sample budget)\n",
        "#     for seed in SEEDS:\n",
        "#         for (_, map_name, traffic, _) in STAGES:\n",
        "#             train_noncurriculum(algo, map_name, traffic, total_timesteps=TOTAL_CURRICULUM_BUDGET, seed=seed, n_envs=N_ENVS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualization"
      ],
      "metadata": {
        "id": "qpsW4W5NI9LG"
      },
      "id": "qpsW4W5NI9LG"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Adjust if your root is named differently\n",
        "EXPERIMENT_ROOT = Path(\"experiments\")\n",
        "\n",
        "CURVES_DIR = EXPERIMENT_ROOT / \"curves\"\n",
        "CURVES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_eval_npz(eval_npz_path: Path):\n",
        "    \"\"\"\n",
        "    Load SB3 EvalCallback npz.\n",
        "    Returns: timesteps, mean_rewards, std_rewards, mean_ep_len\n",
        "    \"\"\"\n",
        "    data = np.load(eval_npz_path)\n",
        "    timesteps = data[\"timesteps\"].flatten()          # [n_eval]\n",
        "    results = data[\"results\"]                        # [n_eval, n_episodes]\n",
        "    ep_lengths = data[\"ep_lengths\"]                  # [n_eval, n_episodes]\n",
        "\n",
        "    mean_rewards = results.mean(axis=1)\n",
        "    std_rewards = results.std(axis=1)\n",
        "    mean_ep_len = ep_lengths.mean(axis=1)\n",
        "\n",
        "    return timesteps, mean_rewards, std_rewards, mean_ep_len\n",
        "\n",
        "\n",
        "def plot_stage_curves(algo, seed_name, stage_name, eval_npz_path):\n",
        "    \"\"\"\n",
        "    Make per-stage plots:\n",
        "      - reward vs timesteps\n",
        "      - episode length vs timesteps\n",
        "    Save to experiments/curves\n",
        "    \"\"\"\n",
        "    t, mean_r, std_r, mean_len = load_eval_npz(eval_npz_path)\n",
        "\n",
        "    # 1) Reward curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(t, mean_r, marker=\"o\")\n",
        "    plt.fill_between(t, mean_r - std_r, mean_r + std_r, alpha=0.2)\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Eval mean return\")\n",
        "    plt.title(f\"{algo} {seed_name} – {stage_name} (eval reward)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_reward.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # 2) Episode length curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(t, mean_len, marker=\"o\")\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Mean episode length\")\n",
        "    plt.title(f\"{algo} {seed_name} – {stage_name} (episode length)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_ep_len.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_combined_curves(algo, seed_name, stage_to_npz):\n",
        "    \"\"\"\n",
        "    Combined plots across stages for one algo+seed:\n",
        "      - all reward curves\n",
        "      - all episode length curves\n",
        "    stage_to_npz: dict {stage_name: eval_npz_path}\n",
        "    \"\"\"\n",
        "    # Reward\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    for stage_name, npz_path in stage_to_npz.items():\n",
        "        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n",
        "        plt.plot(t, mean_r, marker=\"o\", label=stage_name)\n",
        "\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Eval mean return\")\n",
        "    plt.title(f\"{algo} {seed_name} – eval reward (all stages)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_reward.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # Episode length\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    for stage_name, npz_path in stage_to_npz.items():\n",
        "        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n",
        "        plt.plot(t, mean_len, marker=\"o\", label=stage_name)\n",
        "\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Mean episode length\")\n",
        "    plt.title(f\"{algo} {seed_name} – episode length (all stages)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_ep_len.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_heldout_bars(algo, seed_name, seed_dir: Path):\n",
        "    \"\"\"\n",
        "    If held-out CSVs exist for this algo+seed, make bar plots:\n",
        "      - heldout_scrrxo_metrics.csv across stages\n",
        "      - heldout_varying_metrics.csv across stages\n",
        "    \"\"\"\n",
        "    # Find all stages under this seed dir\n",
        "    stage_dirs = [\n",
        "        d for d in seed_dir.iterdir()\n",
        "        if d.is_dir() and not d.name.startswith(\".\")\n",
        "    ]\n",
        "\n",
        "    # ----- Held-out 1: SCrRXO -----\n",
        "    rows = []\n",
        "    for sd in stage_dirs:\n",
        "        csv_path = sd / \"heldout_scrrxo_metrics.csv\"\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            if not df.empty:\n",
        "                r = df.iloc[0].to_dict()\n",
        "                r[\"stage\"] = sd.name\n",
        "                rows.append(r)\n",
        "\n",
        "    if rows:\n",
        "        df_scr = pd.DataFrame(rows)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(df_scr[\"stage\"], df_scr[\"mean_reward\"])\n",
        "        plt.xlabel(\"Training stage\")\n",
        "        plt.ylabel(\"Held-out mean reward\")\n",
        "        plt.title(f\"{algo} {seed_name} – Held-out SCrRXO performance\")\n",
        "        plt.grid(axis=\"y\")\n",
        "        plt.tight_layout()\n",
        "        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_SCrRXO.png\"\n",
        "        plt.savefig(out_path, dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # ----- Held-out 2: VaryingDynamics -----\n",
        "    rows = []\n",
        "    for sd in stage_dirs:\n",
        "        csv_path = sd / \"heldout_varying_metrics.csv\"\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            if not df.empty:\n",
        "                r = df.iloc[0].to_dict()\n",
        "                r[\"stage\"] = sd.name\n",
        "                rows.append(r)\n",
        "\n",
        "    if rows:\n",
        "        df_vd = pd.DataFrame(rows)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(df_vd[\"stage\"], df_vd[\"mean_reward\"])\n",
        "        plt.xlabel(\"Training stage\")\n",
        "        plt.ylabel(\"Held-out mean reward\")\n",
        "        plt.title(f\"{algo} {seed_name} – Held-out VaryingDynamics performance\")\n",
        "        plt.grid(axis=\"y\")\n",
        "        plt.tight_layout()\n",
        "        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_VaryingDynamics.png\"\n",
        "        plt.savefig(out_path, dpi=150)\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "-j-jWQJJgf8k"
      },
      "id": "-j-jWQJJgf8k",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# MAIN: walk experiments/ and generate all possible plots\n",
        "# ======================================================================\n",
        "\n",
        "for algo_dir in EXPERIMENT_ROOT.iterdir():\n",
        "    if not algo_dir.is_dir():\n",
        "        continue\n",
        "    algo = algo_dir.name  # e.g. \"PPO\", \"SAC\", \"DQN\"\n",
        "\n",
        "    noncurr_dir = algo_dir / \"noncurriculum\"\n",
        "    if not noncurr_dir.exists():\n",
        "        continue\n",
        "\n",
        "    for seed_dir in noncurr_dir.iterdir():\n",
        "        if not seed_dir.is_dir() or not seed_dir.name.startswith(\"seed_\"):\n",
        "            continue\n",
        "        seed_name = seed_dir.name  # e.g. \"seed_0\"\n",
        "\n",
        "        print(f\"Processing {algo} / {seed_name} ...\")\n",
        "\n",
        "        # collect per-stage npz paths for combined plots\n",
        "        stage_to_npz = {}\n",
        "\n",
        "        # per-stage plots\n",
        "        for stage_dir in seed_dir.iterdir():\n",
        "            if not stage_dir.is_dir():\n",
        "                continue\n",
        "            stage_name = stage_dir.name  # e.g. \"C0_Straight\"\n",
        "\n",
        "            eval_npz_path = stage_dir / \"eval\" / \"evaluations.npz\"\n",
        "            if not eval_npz_path.exists():\n",
        "                continue\n",
        "\n",
        "            # individual plots\n",
        "            plot_stage_curves(algo, seed_name, stage_name, eval_npz_path)\n",
        "            stage_to_npz[stage_name] = eval_npz_path\n",
        "\n",
        "        # combined curves across stages\n",
        "        if stage_to_npz:\n",
        "            plot_combined_curves(algo, seed_name, stage_to_npz)\n",
        "\n",
        "        # held-out bar plots\n",
        "        plot_heldout_bars(algo, seed_name, seed_dir)\n",
        "\n",
        "print(\"All curves saved under:\", CURVES_DIR)"
      ],
      "metadata": {
        "id": "vHiU4aOEj-7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59735c5-2e12-47de-e7ef-45ad80969c0d"
      },
      "id": "vHiU4aOEj-7z",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing DQN / seed_0 ...\n",
            "All curves saved under: experiments/curves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"experiments/tensorboard\")\n",
        "\n",
        "def load_tb_scalars(event_file):\n",
        "    event_file = str(event_file)   # <--- FIX HERE\n",
        "    ea = EventAccumulator(event_file)\n",
        "    ea.Reload()\n",
        "    scalars = {}\n",
        "    for tag in ea.Tags()[\"scalars\"]:\n",
        "        events = ea.Scalars(tag)\n",
        "        steps = [e.step for e in events]\n",
        "        values = [e.value for e in events]\n",
        "        scalars[tag] = (steps, values)\n",
        "    return scalars\n",
        "\n",
        "\n",
        "def plot_scalar(steps, values, title, out_path):\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(steps, values)\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(title)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_stage(stage_path):\n",
        "    print(f\"\\nProcessing stage: {stage_path.name}\")\n",
        "    curves_dir = stage_path / \"curves\"\n",
        "    curves_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Find tensorboard event file\n",
        "    event_files = list(stage_path.glob(\"**/events.out.tfevents.*\"))\n",
        "    if len(event_files)==0:\n",
        "        print(\"No TB file found for eval.\")\n",
        "        return\n",
        "    tb_file = event_files[0]\n",
        "    tb_scalars = load_tb_scalars(tb_file)\n",
        "\n",
        "    # Plot all general scalars\n",
        "    for tag, (steps, values) in tb_scalars.items():\n",
        "        clean_name = tag.replace(\"/\", \"_\")\n",
        "        out = curves_dir / f\"{clean_name}.png\"\n",
        "        plot_scalar(steps, values, f\"{stage_path.name}: {tag}\", out)\n",
        "\n",
        "    print(f\"Saved all curves for {stage_path.name} → {curves_dir}\")\n",
        "\n",
        "\n",
        "# =============================\n",
        "# PROCESS EACH STAGE\n",
        "# =============================\n",
        "all_stage_dirs = sorted([p for p in BASE.glob(\"*\") if p.is_dir()])\n",
        "\n",
        "for stage_dir in all_stage_dirs:\n",
        "    plot_stage(stage_dir)\n",
        "\n",
        "print(\"\\nFinished generating all plots.\")\n"
      ],
      "metadata": {
        "id": "sZWIeaOMj_mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013c833b-978d-411a-824b-01d58487a551"
      },
      "id": "sZWIeaOMj_mO",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing stage: DQN_1\n",
            "Saved all curves for DQN_1 → experiments/tensorboard/DQN_1/curves\n",
            "\n",
            "Processing stage: DQN_2\n",
            "Saved all curves for DQN_2 → experiments/tensorboard/DQN_2/curves\n",
            "\n",
            "Processing stage: DQN_3\n",
            "Saved all curves for DQN_3 → experiments/tensorboard/DQN_3/curves\n",
            "\n",
            "Processing stage: DQN_4\n",
            "Saved all curves for DQN_4 → experiments/tensorboard/DQN_4/curves\n",
            "\n",
            "Finished generating all plots.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### video viz"
      ],
      "metadata": {
        "id": "HG99vKdcTQH4"
      },
      "id": "HG99vKdcTQH4"
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import time\n",
        "# from pathlib import Path\n",
        "\n",
        "# import numpy as np\n",
        "# from stable_baselines3 import PPO, SAC, DQN\n",
        "# from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "\n",
        "\n",
        "# # --------------------------------------------------\n",
        "# # Helper: map algo name / class -> SB3 class\n",
        "# # --------------------------------------------------\n",
        "# def _resolve_algo_cls(algo):\n",
        "#     \"\"\"\n",
        "#     algo can be:\n",
        "#       - SB3 class (PPO, SAC, DQN)\n",
        "#       - string: \"ppo\", \"sac\", \"dqn\"\n",
        "#     Returns the SB3 class.\n",
        "#     \"\"\"\n",
        "#     if hasattr(algo, \"load\"):  # already a class like PPO/SAC/DQN\n",
        "#         return algo\n",
        "\n",
        "#     if isinstance(algo, str):\n",
        "#         key = algo.strip().upper()\n",
        "#         if key == \"PPO\":\n",
        "#             return PPO\n",
        "#         if key == \"SAC\":\n",
        "#             return SAC\n",
        "#         if key == \"DQN\":\n",
        "#             return DQN\n",
        "\n",
        "#     raise ValueError(f\"Unrecognized algo spec: {algo!r}\")\n",
        "\n",
        "\n",
        "# # --------------------------------------------------\n",
        "# # Main utility: visualize_policy\n",
        "# # --------------------------------------------------\n",
        "# def visualize_policy(\n",
        "#     model_path,\n",
        "#     stage,\n",
        "#     algo=\"PPO\",\n",
        "#     seed=0,\n",
        "#     n_episodes=2,\n",
        "#     max_steps=500,\n",
        "#     render=True,\n",
        "#     record_video=False,\n",
        "#     video_folder=\"videos\",\n",
        "#     video_length=500,\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Visualize a trained MetaDrive policy.\n",
        "\n",
        "#     Args:\n",
        "#         model_path (str | Path): path to SB3 model .zip\n",
        "#         stage (dict): one of your STAGES entries, e.g. STAGES[2]\n",
        "#         algo (str | class): \"PPO\", \"SAC\", \"DQN\" or the SB3 class\n",
        "#         seed (int): RNG seed for the env\n",
        "#         n_episodes (int): how many episodes to roll out\n",
        "#         max_steps (int): cap per episode (lightweight)\n",
        "#         render (bool): call env.render() (window / offscreen)\n",
        "#         record_video (bool): if True, records an MP4 via VecVideoRecorder\n",
        "#         video_folder (str): folder for videos if record_video=True\n",
        "#         video_length (int): max total timesteps for the recorded video\n",
        "\n",
        "#     Notes:\n",
        "#       - Uses your existing make_metadrive_env(stage, use_discrete, seed, render)\n",
        "#       - Handles DQN (discrete) vs PPO/SAC (continuous) automatically.\n",
        "#     \"\"\"\n",
        "#     model_path = str(model_path)\n",
        "#     algo_cls = _resolve_algo_cls(algo)\n",
        "\n",
        "#     # 1) Load model\n",
        "#     print(f\"\\n[visualize_policy] Loading model from: {model_path}\")\n",
        "#     model = algo_cls.load(model_path)\n",
        "#     print(f\"[visualize_policy] Loaded {algo_cls.__name__}\")\n",
        "\n",
        "#     # 2) Decide if this is a discrete agent\n",
        "#     use_discrete = isinstance(model, DQN)\n",
        "#     print(f\"[visualize_policy] use_discrete={use_discrete}\")\n",
        "\n",
        "#     # 3) Build vec env with the SAME wrappers as training\n",
        "#     #    Assumes you already defined make_metadrive_env exactly as in training.\n",
        "#     make_env_fn = make_metadrive_env(\n",
        "#         stage=stage,\n",
        "#         use_discrete=use_discrete,\n",
        "#         seed=seed,\n",
        "#         render=render,\n",
        "#     )\n",
        "\n",
        "#     vec_env = DummyVecEnv([make_env_fn])\n",
        "\n",
        "#     # 4) Optional: wrap with VecVideoRecorder\n",
        "#     if record_video:\n",
        "#         video_folder = Path(video_folder)\n",
        "#         video_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         video_env = VecVideoRecorder(\n",
        "#             vec_env,\n",
        "#             video_folder=str(video_folder),\n",
        "#             record_video_trigger=lambda step: step == 0,  # record from start\n",
        "#             video_length=video_length,\n",
        "#             name_prefix=f\"{stage['name']}_{algo_cls.__name__}\",\n",
        "#         )\n",
        "#         env = video_env\n",
        "#         print(f\"[visualize_policy] Recording video to: {video_folder}\")\n",
        "#     else:\n",
        "#         env = vec_env\n",
        "\n",
        "#     # 5) Rollout loop (SB3 VecEnv API)\n",
        "#     print(f\"[visualize_policy] Running {n_episodes} episode(s)...\")\n",
        "#     for ep in range(n_episodes):\n",
        "#         obs = env.reset()\n",
        "#         done = False\n",
        "#         ep_reward = 0.0\n",
        "#         step = 0\n",
        "\n",
        "#         while not done and step < max_steps:\n",
        "#             # model.predict works directly with VecEnv obs\n",
        "#             action, _ = model.predict(obs, deterministic=True)\n",
        "#             obs, rewards, dones, infos = env.step(action)\n",
        "\n",
        "#             # vec_env: rewards/dones/infos are arrays/lists; unwrap index 0\n",
        "#             r = float(rewards[0]) if isinstance(rewards, (np.ndarray, list, tuple)) else float(rewards)\n",
        "#             d = bool(dones[0]) if isinstance(dones, (np.ndarray, list, tuple)) else bool(dones)\n",
        "#             info = infos[0] if isinstance(infos, (list, tuple)) and len(infos) > 0 else infos\n",
        "\n",
        "#             ep_reward += r\n",
        "#             step += 1\n",
        "#             done = d\n",
        "\n",
        "#             if render and not record_video:\n",
        "#                 # VecEnv.render() forwards to underlying env[0].render()\n",
        "#                 env.render()\n",
        "#                 time.sleep(0.01)  # tiny pause so it’s watchable\n",
        "\n",
        "#         # Some MetaDrive wrappers put success / crash info in info\n",
        "#         success = False\n",
        "#         crash = False\n",
        "#         if isinstance(info, dict):\n",
        "#             success = bool(info.get(\"success\", False) or info.get(\"arrive_dest\", False))\n",
        "#             crash = bool(\n",
        "#                 info.get(\"collision\", False)\n",
        "#                 or info.get(\"crash_vehicle\", False)\n",
        "#                 or info.get(\"crash_object\", False)\n",
        "#                 or info.get(\"crash_building\", False)\n",
        "#                 or info.get(\"out_of_road\", False)\n",
        "#             )\n",
        "\n",
        "#         print(\n",
        "#             f\"[Episode {ep+1}/{n_episodes}] \"\n",
        "#             f\"steps={step}  return={ep_reward:.2f}  \"\n",
        "#             f\"success={success}  crash={crash}\"\n",
        "#         )\n",
        "\n",
        "#     # 6) Cleanup\n",
        "#     env.close()\n",
        "#     print(\"[visualize_policy] Done.\\n\")\n"
      ],
      "metadata": {
        "id": "H0BMJvwkTSuG"
      },
      "id": "H0BMJvwkTSuG",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize_policy(\n",
        "#     model_path=\"experiments/DQN/noncurriculum/seed_0/C2_LightTraffic/model.zip\",\n",
        "#     stage=STAGES[0],\n",
        "#     algo=\"DQN\",\n",
        "#     seed=0,\n",
        "#     n_episodes=2,\n",
        "#     max_steps=500,\n",
        "#     render=True,\n",
        "#     record_video=True,   # keep False if you're in Kaggle CPU and just want to check behavior\n",
        "# )\n"
      ],
      "metadata": {
        "id": "216kh4EzTLYt"
      },
      "id": "216kh4EzTLYt",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def record_metadrive_video(env_fn, model, out_path=\"demo.mp4\",\n",
        "                           n_steps=600, deterministic=True):\n",
        "    \"\"\"\n",
        "    Record a policy rollout in MetaDrive using offscreen rendering.\n",
        "    Works with all wrappers (Monitor, SB3, DiscreteActionWrapper, etc.)\n",
        "    \"\"\"\n",
        "    env = env_fn()\n",
        "\n",
        "    # unwrap to get the raw MetaDriveEnv\n",
        "    base_env = unwrap_metadrive(env)\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    # Reset (Gymnasium-safe)\n",
        "    try:\n",
        "        obs, info = env.reset()\n",
        "    except:\n",
        "        obs = env.reset()\n",
        "\n",
        "    for t in range(n_steps):\n",
        "        # Policy prediction\n",
        "        action, _ = model.predict(obs, deterministic=deterministic)\n",
        "\n",
        "        # Step\n",
        "        try:\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "        except:\n",
        "            obs, reward, terminated, info = env.step(action)\n",
        "            truncated = False\n",
        "\n",
        "        # SAFE frame capture\n",
        "        frame = base_env.engine.get_image()\n",
        "        frames.append(frame)\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Write MP4\n",
        "    imageio.mimwrite(out_path, frames, fps=30)\n",
        "    print(f\"[record_metadrive_video] Saved video → {out_path}\")\n",
        "\n",
        "def unwrap_metadrive(env):\n",
        "    \"\"\"\n",
        "    Unwraps SB3 and Gymnasium wrappers until reaching the underlying MetaDriveEnv.\n",
        "    \"\"\"\n",
        "    base = env\n",
        "    while hasattr(base, \"env\"):\n",
        "        base = base.env\n",
        "    return base\n"
      ],
      "metadata": {
        "id": "RvqeZsAwUXS7"
      },
      "id": "RvqeZsAwUXS7",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import PPO, SAC, DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "\n",
        "\n",
        "def load_sb3_model(algo: str, model_path: str, device: str = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Load a trained SB3 model given the algorithm name and path.\n",
        "    \"\"\"\n",
        "    algo = algo.upper()\n",
        "    if algo == \"PPO\":\n",
        "        return PPO.load(model_path, device=device)\n",
        "    elif algo == \"SAC\":\n",
        "        return SAC.load(model_path, device=device)\n",
        "    elif algo == \"DQN\":\n",
        "        return DQN.load(model_path, device=device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown algo '{algo}'. Expected one of ['PPO','SAC','DQN'].\")\n",
        "\n",
        "\n",
        "def visualize_policy(\n",
        "    algo: str,\n",
        "    stage: dict,\n",
        "    seed: int = 0,\n",
        "    model_path: str | None = None,\n",
        "    experiments_root: str = \"experiments\",\n",
        "    curriculum_mode: str = \"noncurriculum\",\n",
        "    max_steps: int = 600,\n",
        "    deterministic: bool = True,\n",
        "    record_video: bool = False,\n",
        "    video_folder: str | None = None,\n",
        "    video_length: int = 600,\n",
        "    device: str = \"cpu\",\n",
        "    plot_trajectory: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize a trained policy in MetaDrive.\n",
        "\n",
        "    - algo: \"PPO\", \"SAC\", or \"DQN\"\n",
        "    - stage: a dict from STAGES (must have at least \"name\")\n",
        "    - seed: which seed subfolder the model is in\n",
        "    - model_path: explicit path to model.zip (if None, auto-constructed)\n",
        "    - experiments_root: root folder containing algo/curriculum/seed_xx/...\n",
        "    - curriculum_mode: usually \"noncurriculum\" or \"curriculum\"\n",
        "    - max_steps: cap steps per rollout (keeps it light-weight)\n",
        "    - deterministic: SB3 predict() deterministic flag\n",
        "    - record_video: if True, record a short MP4 to video_folder\n",
        "    - video_folder: path to save videos (default under experiment dir)\n",
        "    - video_length: max steps recorded in video\n",
        "    - device: \"cpu\" or \"cuda\"\n",
        "    - plot_trajectory: if True, plot XY path at the end\n",
        "    \"\"\"\n",
        "\n",
        "    algo = algo.upper()\n",
        "    stage_name = stage[\"name\"]\n",
        "\n",
        "    # -------------------------------\n",
        "    # 1. Resolve model path\n",
        "    # -------------------------------\n",
        "    if model_path is None:\n",
        "        model_path = (\n",
        "            Path(experiments_root)\n",
        "            / algo\n",
        "            / curriculum_mode\n",
        "            / f\"seed_{seed}\"\n",
        "            / stage_name\n",
        "            / \"model.zip\"\n",
        "        )\n",
        "    model_path = str(model_path)\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model not found at: {model_path}\")\n",
        "\n",
        "    print(f\"[visualize_policy] Loading {algo} model from:\\n  {model_path}\")\n",
        "    model = load_sb3_model(algo, model_path, device=device)\n",
        "\n",
        "    # DQN uses discrete actions -> wrapper must be active\n",
        "    use_discrete = (algo == \"DQN\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # 2. Choose env build mode\n",
        "    #    - If record_video: VecEnv + VecVideoRecorder (no on-screen window)\n",
        "    #    - Else: single env with render=True for live viewing\n",
        "    # -------------------------------\n",
        "    # if record_video:\n",
        "    #     # -------- VIDEO MODE (no live render) ----------\n",
        "    #     if video_folder is None:\n",
        "    #         video_folder = (\n",
        "    #             Path(experiments_root)\n",
        "    #             / algo\n",
        "    #             / curriculum_mode\n",
        "    #             / f\"seed_{seed}\"\n",
        "    #             / stage_name\n",
        "    #             / \"videos\"\n",
        "    #         )\n",
        "    #     video_folder = Path(video_folder)\n",
        "    #     video_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    #     print(f\"[visualize_policy] Recording video to: {video_folder}\")\n",
        "\n",
        "    #     # build non-render env for video (window not needed)\n",
        "    #     def _env_fn():\n",
        "    #         return make_metadrive_env(\n",
        "    #             stage=stage,\n",
        "    #             use_discrete=use_discrete,\n",
        "    #             seed=seed + 999,  # different seed from training\n",
        "    #             render=False      # no on-screen window\n",
        "    #         )()\n",
        "\n",
        "    #     vec_env = DummyVecEnv([_env_fn])\n",
        "\n",
        "    #     video_env = VecVideoRecorder(\n",
        "    #         vec_env,\n",
        "    #         video_folder=str(video_folder),\n",
        "    #         record_video_trigger=lambda step: step == 0,\n",
        "    #         video_length=video_length,\n",
        "    #         name_prefix=f\"{algo}_{stage_name}_seed{seed}\",\n",
        "    #     )\n",
        "\n",
        "    #     obs = video_env.reset()\n",
        "    #     for step in range(video_length):\n",
        "    #         action, _ = model.predict(obs, deterministic=deterministic)\n",
        "    #         obs, rewards, dones, infos = video_env.step(action)\n",
        "    #         if np.any(dones):\n",
        "    #             break\n",
        "\n",
        "    #     video_env.close()\n",
        "    #     print(\"[visualize_policy] Video recording finished.\")\n",
        "    #     return\n",
        "\n",
        "    if record_video:\n",
        "      # Build env without render window\n",
        "      env_fn = lambda: make_metadrive_env(\n",
        "          stage=stage,\n",
        "          use_discrete=use_discrete,\n",
        "          seed=seed + 999,\n",
        "          render=False\n",
        "      )()\n",
        "\n",
        "      if video_folder is None:\n",
        "          video_folder = (\n",
        "              Path(experiments_root)\n",
        "              / algo\n",
        "              / curriculum_mode\n",
        "              / f\"seed_{seed}\"\n",
        "              / stage_name\n",
        "              / \"videos\"\n",
        "          )\n",
        "      video_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "      out_path = video_folder / f\"{algo}_{stage_name}_seed{seed}.mp4\"\n",
        "\n",
        "      print(f\"[visualize_policy] Recording video → {out_path}\")\n",
        "\n",
        "      record_metadrive_video(\n",
        "          env_fn,\n",
        "          model,\n",
        "          out_path=str(out_path),\n",
        "          n_steps=video_length,\n",
        "          deterministic=deterministic,\n",
        "      )\n",
        "      return\n",
        "\n",
        "    else:\n",
        "        # -------- LIVE RENDER MODE ----------\n",
        "        print(\"[visualize_policy] Creating render-enabled env...\")\n",
        "\n",
        "        make_env = make_metadrive_env(\n",
        "            stage=stage,\n",
        "            use_discrete=use_discrete,\n",
        "            seed=seed + 999,\n",
        "            render=True,  # open Panda3D window\n",
        "        )\n",
        "        env = make_env()\n",
        "\n",
        "        # -------------------------------\n",
        "        # 3. Rollout a single episode\n",
        "        # -------------------------------\n",
        "        print(f\"[visualize_policy] Rolling out one episode on stage '{stage_name}'...\")\n",
        "        try:\n",
        "            obs, info = env.reset()\n",
        "        except ValueError:\n",
        "            # If your MetaDrive wrapper returns only obs\n",
        "            obs = env.reset()\n",
        "            info = {}\n",
        "\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        positions = []\n",
        "        speeds = []\n",
        "\n",
        "        ep_reward = 0.0\n",
        "        step_count = 0\n",
        "        crashed = False\n",
        "        reached_dest = False\n",
        "\n",
        "        while not (done or truncated) and step_count < max_steps:\n",
        "            action, _ = model.predict(obs, deterministic=deterministic)\n",
        "            obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            ep_reward += float(reward)\n",
        "            step_count += 1\n",
        "\n",
        "            # log meta info for later analysis\n",
        "            if isinstance(info, dict):\n",
        "                pos = info.get(\"position\", None)\n",
        "                if pos is not None and len(pos) >= 2:\n",
        "                    positions.append((pos[0], pos[1]))\n",
        "                if \"speed\" in info:\n",
        "                    speeds.append(float(info[\"speed\"]))\n",
        "                elif \"avg_speed\" in info:\n",
        "                    speeds.append(float(info[\"avg_speed\"]))\n",
        "\n",
        "                # crash / success flags\n",
        "                if (\n",
        "                    info.get(\"crash_vehicle\", False)\n",
        "                    or info.get(\"crash_object\", False)\n",
        "                    or info.get(\"crash_building\", False)\n",
        "                    or info.get(\"out_of_road\", False)\n",
        "                ):\n",
        "                    crashed = True\n",
        "                if info.get(\"arrive_dest\", False) or info.get(\"success\", False):\n",
        "                    reached_dest = True\n",
        "\n",
        "            env.render()\n",
        "            # small sleep so it doesn't burn CPU + you can actually see it\n",
        "            time.sleep(0.01)\n",
        "\n",
        "        print(f\"[visualize_policy] Episode finished after {step_count} steps.\")\n",
        "        print(f\"  Total reward      : {ep_reward:.2f}\")\n",
        "        print(f\"  Reached destination: {reached_dest}\")\n",
        "        print(f\"  Crashed/out-of-road: {crashed}\")\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        # -------------------------------\n",
        "        # 4. Optional trajectory plot\n",
        "        # -------------------------------\n",
        "        if plot_trajectory and positions:\n",
        "            xs, ys = zip(*positions)\n",
        "            plt.figure(figsize=(5, 5))\n",
        "            plt.plot(xs, ys, marker=\".\", linewidth=1)\n",
        "            plt.title(f\"Trajectory: {algo} on {stage_name}\")\n",
        "            plt.xlabel(\"x\")\n",
        "            plt.ylabel(\"y\")\n",
        "            plt.axis(\"equal\")\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        # Optional: speed curve\n",
        "        if plot_trajectory and speeds:\n",
        "            plt.figure(figsize=(6, 3))\n",
        "            plt.plot(speeds)\n",
        "            plt.title(f\"Speed over time: {algo} on {stage_name}\")\n",
        "            plt.xlabel(\"step\")\n",
        "            plt.ylabel(\"speed\")\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "MYq3K2TgTqYs"
      },
      "id": "MYq3K2TgTqYs",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # visualize DQN on C2_LightTraffic (seed 0)\n",
        "# stage_C2 = next(s for s in STAGES if s[\"name\"] == \"C2_LightTraffic\")\n",
        "\n",
        "# visualize_policy(\n",
        "#     algo=\"DQN\",\n",
        "#     stage=stage_C2,\n",
        "#     seed=0,\n",
        "#     experiments_root=\"experiments\",\n",
        "#     curriculum_mode=\"noncurriculum\",\n",
        "#     max_steps=600,          # keep it light\n",
        "#     deterministic=True,\n",
        "#     record_video=True,     # live window\n",
        "#     plot_trajectory=True,\n",
        "#     device=\"cuda\",           # CPU is fine for rollout\n",
        "# )\n"
      ],
      "metadata": {
        "id": "UhwEcVDbTvUg"
      },
      "id": "UhwEcVDbTvUg",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## zip results"
      ],
      "metadata": {
        "id": "IYiG8pFJVZTg"
      },
      "id": "IYiG8pFJVZTg"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"experiments_DQN\", \"zip\", \"experiments\")\n",
        "\n",
        "print(\"Zipped to experiments_DQN.zip\")"
      ],
      "metadata": {
        "id": "i0oLMRwtFbhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4689b807-2860-4929-cebe-971e580b0a14"
      },
      "id": "i0oLMRwtFbhQ",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipped to experiments_DQN.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYGcHpMI02D4"
      },
      "id": "aYGcHpMI02D4",
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qpsW4W5NI9LG"
      ],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}