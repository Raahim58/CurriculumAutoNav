{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["qpsW4W5NI9LG"],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"15c876cf","cell_type":"markdown","source":"# NonCurriculum_MetaDrive_SB3_Experiments\n\nThis notebook contains a full, reproducible experiment pipeline for **non-curriculum** based reinforcement learning for autonomous driving using **MetaDrive** and **Stable Baselines3 (SB3)**. It includes:\n\n- Full environment factory and wrappers (including a discrete-action wrapper for DQN).\n- Exact stage definitions (C0..C3) and matching budgets.\n- Non-curriculum runner (train each target map separately for the same total sample budget).\n- Evaluation harness (metrics logging, CSV saving, TensorBoard integration, video recording).\n- Hyperparameters and experiment folder conventions.\n\n","metadata":{"id":"15c876cf"}},{"id":"8f8b7750","cell_type":"markdown","source":"## 1. Setup\n","metadata":{"id":"8f8b7750"}},{"id":"9KG6c_nUg1EH","cell_type":"code","source":"!pip uninstall -y metadrive metadrive-simulator metadrive-simulator-py3-12 || true\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KG6c_nUg1EH","outputId":"d3761111-e616-405e-fee2-3b6601fe78c0","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:25:44.744779Z","iopub.execute_input":"2025-11-28T16:25:44.745494Z","iopub.status.idle":"2025-11-28T16:25:47.676108Z","shell.execute_reply.started":"2025-11-28T16:25:44.745471Z","shell.execute_reply":"2025-11-28T16:25:47.675378Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping metadrive as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator-py3-12 as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"bbe3fc75","cell_type":"code","source":"# !pip install stable-baselines3[extra] gymnasium metadrive numpy pandas matplotlib tensorboard opencv-python\n# !pip install stable-baselines3[extra] tensorboard opencv-python\n# !pip install -q \"stable-baselines3[extra]\" \"metadrive-simulator-py3-12\" tensorboard opencv-python\n\n# # Uninstall the incompatible numpy version first to be safe\n# !pip uninstall -y numpy\n\n# # Install everything in ONE command to enforce the version constraint\n# !pip install \"numpy<2.0\" \"metadrive-simulator\" \"stable-baselines3[extra]\" tensorboard opencv-python\n\n# Uninstall potential conflict packages first\n!pip uninstall -y numpy protobuf\n\n# Install with pinned versions for Numpy and Protobuf\n!pip install \"numpy<2.0\" \"protobuf==3.20.3\" \"metadrive-simulator\" \"stable-baselines3[extra]\" tensorboard opencv-python","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbe3fc75","outputId":"f12bdee8-7fe7-4a69-d401-b32eb7e90d50","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:25:47.677701Z","iopub.execute_input":"2025-11-28T16:25:47.677947Z","iopub.status.idle":"2025-11-28T16:27:41.314094Z","shell.execute_reply.started":"2025-11-28T16:25:47.677923Z","shell.execute_reply":"2025-11-28T16:27:41.313382Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: protobuf 6.33.0\nUninstalling protobuf-6.33.0:\n  Successfully uninstalled protobuf-6.33.0\nCollecting numpy<2.0\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nCollecting metadrive-simulator\n  Downloading metadrive_simulator-0.4.3-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\nRequirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.11/dist-packages (2.1.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (2.32.5)\nRequirement already satisfied: gymnasium>=0.28 in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (0.29.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (3.7.2)\nRequirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (2.6.1)\nCollecting yapf (from metadrive-simulator)\n  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (0.12.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (4.67.1)\nCollecting progressbar (from metadrive-simulator)\n  Downloading progressbar-2.5.tar.gz (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting panda3d==1.10.13 (from metadrive-simulator)\n  Downloading panda3d-1.10.13-cp311-cp311-manylinux2014_x86_64.whl.metadata (3.2 kB)\nCollecting panda3d-gltf==0.13 (from metadrive-simulator)\n  Downloading panda3d_gltf-0.13-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (11.3.0)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (5.4.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (1.15.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (7.1.3)\nRequirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (2.1.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (3.20.0)\nRequirement already satisfied: Pygments in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (2.19.2)\nCollecting mediapy (from metadrive-simulator)\n  Downloading mediapy-1.2.4-py3-none-any.whl.metadata (4.8 kB)\nCollecting panda3d-simplepbr>=0.6 (from panda3d-gltf==0.13->metadrive-simulator)\n  Downloading panda3d_simplepbr-0.13.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.0+cu124)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.1.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.2.3)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (14.2.0)\nCollecting shimmy~=1.1.0 (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra])\n  Downloading Shimmy-1.1.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.74.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\nINFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\nCollecting opencv-python\n  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.3.0)\nCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28->metadrive-simulator) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28->metadrive-simulator) (0.0.4)\nCollecting ale-py~=0.8.1 (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra])\n  Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (2.9.0.post0)\nRequirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from mediapy->metadrive-simulator) (7.34.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->metadrive-simulator) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->metadrive-simulator) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->metadrive-simulator) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->metadrive-simulator) (2025.10.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (4.0.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->metadrive-simulator) (4.5.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (6.5.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (3.0.51)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (4.9.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->mediapy->metadrive-simulator) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->mediapy->metadrive-simulator) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy->metadrive-simulator) (0.2.13)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading metadrive_simulator-0.4.3-py3-none-any.whl (55.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading panda3d-1.10.13-cp311-cp311-manylinux2014_x86_64.whl (55.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading panda3d_gltf-0.13-py3-none-any.whl (25 kB)\nDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\nDownloading Shimmy-1.1.0-py3-none-any.whl (37 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mediapy-1.2.4-py3-none-any.whl (26 kB)\nDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading panda3d_simplepbr-0.13.1-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: progressbar, AutoROM.accept-rom-license\n  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12065 sha256=e88d779b7f646f586161a6805423baa9f00ab73a5f102ea3fa5260ed5c5a1ae9\n  Stored in directory: /root/.cache/pip/wheels/8d/bb/b2/5353b966ac6f3c5e1000629a9a5f6aed41794487f551e32efc\n  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446709 sha256=9ae77c91080101242ff7c3437a8357f858d8d44b97daf6705a34c8a75d575b0a\n  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\nSuccessfully built progressbar AutoROM.accept-rom-license\nInstalling collected packages: progressbar, panda3d, yapf, protobuf, panda3d-simplepbr, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, panda3d-gltf, opencv-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, ale-py, shimmy, nvidia-cusolver-cu12, mediapy, metadrive-simulator\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: opencv-python\n    Found existing installation: opencv-python 4.12.0.88\n    Uninstalling opencv-python-4.12.0.88:\n      Successfully uninstalled opencv-python-4.12.0.88\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: ale-py\n    Found existing installation: ale-py 0.11.2\n    Uninstalling ale-py-0.11.2:\n      Successfully uninstalled ale-py-0.11.2\n  Attempting uninstall: shimmy\n    Found existing installation: Shimmy 1.3.0\n    Uninstalling Shimmy-1.3.0:\n      Successfully uninstalled Shimmy-1.3.0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nkaggle-environments 1.18.0 requires shimmy>=1.2.1, but you have shimmy 1.1.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 mediapy-1.2.4 metadrive-simulator-0.4.3 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencv-python-4.11.0.86 panda3d-1.10.13 panda3d-gltf-0.13 panda3d-simplepbr-0.13.1 progressbar-2.5 protobuf-3.20.3 shimmy-1.1.0 yapf-0.43.0\n","output_type":"stream"}],"execution_count":2},{"id":"MDpNIf_ug8-1","cell_type":"code","source":"import metadrive\n\nprint(\"metadrive.__file__ :\", getattr(metadrive, \"__file__\", None))\nprint(\"metadrive.__path__ :\", getattr(metadrive, \"__path__\", None))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDpNIf_ug8-1","outputId":"d207b93b-b387-4972-df8a-c6cb75630660","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:27:41.315021Z","iopub.execute_input":"2025-11-28T16:27:41.315301Z","iopub.status.idle":"2025-11-28T16:27:49.020722Z","shell.execute_reply.started":"2025-11-28T16:27:41.315263Z","shell.execute_reply":"2025-11-28T16:27:49.020041Z"}},"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n","output_type":"stream"},{"name":"stdout","text":"metadrive.__file__ : /usr/local/lib/python3.11/dist-packages/metadrive/__init__.py\nmetadrive.__path__ : ['/usr/local/lib/python3.11/dist-packages/metadrive']\n","output_type":"stream"}],"execution_count":3},{"id":"vAvVvccaKQJQ","cell_type":"code","source":"# !git clone https://github.com/metadriverse/metadrive.git\n# %cd metadrive\n# !pip install -e .\n","metadata":{"id":"vAvVvccaKQJQ","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:27:49.022553Z","iopub.execute_input":"2025-11-28T16:27:49.022962Z","iopub.status.idle":"2025-11-28T16:27:49.026765Z","shell.execute_reply.started":"2025-11-28T16:27:49.022941Z","shell.execute_reply":"2025-11-28T16:27:49.026093Z"}},"outputs":[],"execution_count":4},{"id":"yD7V6MWtKbUn","cell_type":"code","source":"# !python -m metadrive.pull_asset\n","metadata":{"id":"yD7V6MWtKbUn","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:27:49.027529Z","iopub.execute_input":"2025-11-28T16:27:49.027797Z","iopub.status.idle":"2025-11-28T16:27:52.812819Z","shell.execute_reply.started":"2025-11-28T16:27:49.027771Z","shell.execute_reply":"2025-11-28T16:27:52.812045Z"}},"outputs":[],"execution_count":5},{"id":"-zr38W64KdZd","cell_type":"code","source":"# !python -m metadrive.examples.profile_metadrive\n","metadata":{"id":"-zr38W64KdZd","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:27:52.813627Z","iopub.execute_input":"2025-11-28T16:27:52.813859Z","iopub.status.idle":"2025-11-28T16:27:52.828096Z","shell.execute_reply.started":"2025-11-28T16:27:52.813832Z","shell.execute_reply":"2025-11-28T16:27:52.827427Z"}},"outputs":[],"execution_count":6},{"id":"Kf1vBH1BKfpt","cell_type":"code","source":"# %cd /content\n","metadata":{"id":"Kf1vBH1BKfpt","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:27:52.828955Z","iopub.execute_input":"2025-11-28T16:27:52.829353Z","iopub.status.idle":"2025-11-28T16:27:52.843261Z","shell.execute_reply.started":"2025-11-28T16:27:52.829310Z","shell.execute_reply":"2025-11-28T16:27:52.842633Z"}},"outputs":[],"execution_count":7},{"id":"ZXDForLMfJsl","cell_type":"code","source":"import metadrive, inspect, os\nprint(\"Using metadrive from:\", metadrive.__file__)\nprint(\"Contents:\", os.listdir(os.path.dirname(metadrive.__file__)))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZXDForLMfJsl","outputId":"d81d547a-2171-4461-c7dc-c2948c73a10a","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:27:52.844030Z","iopub.execute_input":"2025-11-28T16:27:52.844294Z","iopub.status.idle":"2025-11-28T16:27:52.858787Z","shell.execute_reply.started":"2025-11-28T16:27:52.844264Z","shell.execute_reply":"2025-11-28T16:27:52.858162Z"}},"outputs":[{"name":"stdout","text":"Using metadrive from: /usr/local/lib/python3.11/dist-packages/metadrive/__init__.py\nContents: ['policy', 'type.py', '__init__.py', 'component', 'manager', 'render_pipeline', 'version.py', 'pull_asset.py', 'tests', '__pycache__', 'third_party', 'examples', 'utils', 'obs', 'scenario', 'engine', 'envs', 'base_class', 'constants.py', 'shaders']\n","output_type":"stream"}],"execution_count":8},{"id":"kKG8RN5zKYtb","cell_type":"markdown","source":"## 2. Imports","metadata":{"id":"kKG8RN5zKYtb"}},{"id":"e81fb189","cell_type":"code","source":"import os\nimport time\nimport json\nimport math\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\nimport logging\n\n# SB3 imports\nfrom metadrive.envs.metadrive_env import MetaDriveEnv\nfrom metadrive.envs.varying_dynamics_env import VaryingDynamicsEnv\nfrom stable_baselines3 import PPO, SAC, DQN\nfrom stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n# MetaDrive import guard\n# try:\n# from metadrive.envs.metadrive_env import MetaDriveEnv\n# except Exception as e:\n    # MetaDriveEnv = None\n    # print('MetaDrive import failed.')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e81fb189","outputId":"083943d8-cb76-4273-8ffb-818c9a8eb7f5","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:27:52.859581Z","iopub.execute_input":"2025-11-28T16:27:52.859864Z","iopub.status.idle":"2025-11-28T16:28:07.794550Z","shell.execute_reply.started":"2025-11-28T16:27:52.859847Z","shell.execute_reply":"2025-11-28T16:28:07.793791Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 16:27:55.317757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764347275.498266      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764347275.549195      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"id":"PpACB8RcK8m8","cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")                       # ignore everything\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", module=\"stable_baselines3\")\nwarnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n","metadata":{"id":"PpACB8RcK8m8","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.797159Z","iopub.execute_input":"2025-11-28T16:28:07.797745Z","iopub.status.idle":"2025-11-28T16:28:07.802215Z","shell.execute_reply.started":"2025-11-28T16:28:07.797726Z","shell.execute_reply":"2025-11-28T16:28:07.801475Z"}},"outputs":[],"execution_count":10},{"id":"cc3da0a2","cell_type":"markdown","source":"## 3. Experiment configuration\nsetup: maps, stages, budgets, hyperparameters","metadata":{"id":"cc3da0a2"}},{"id":"e386723b","cell_type":"code","source":"### Maps, stages and budgets\n# STAGES = [\n#     (\"C0_Straight\", \"Straight\", 0.0, 200_000),\n#     (\"C1_Curve\", \"Curve\", 0.0, 300_000),\n#     (\"C2_Roundabout\",\"Roundabout\",0.0,400_000),\n#     (\"C3_Dynamic\", \"20-block\", 0.3, 400_000),\n# ]\n\n### Maps, curriculum stages and budgets (with reward configs)\n\nSTAGES = [\n    # C0: straight road, no traffic – just learn to go forward safely.\n    {\n        \"id\": \"C0\",\n        \"name\": \"C0_Straight\",\n        \"env_type\": \"general\",\n        \"map\": \"S\", # Straight\n        \"traffic\": 0.0,\n        \"budget\": 100_000,\n        \"reward\": {\n            \"base_w\": 1.0,              # scale MetaDrive base reward\n            \"speed_w\": 0.05,            # weak speed shaping\n            \"max_speed_kmh\": 80.0,\n            \"collision_penalty\": -5.0,\n            \"offroad_penalty\": -3.0,\n            \"traffic_violation_penalty\": -2.0,\n            \"success_bonus\": 10.0,\n            \"step_penalty\": -0.001,\n        },\n    },\n\n    # C1: roundabout, no traffic – topology harder, still single-ego.\n    {\n        \"id\": \"C1\",\n        \"name\": \"C1_Roundabout\",\n        \"env_type\": \"general\",\n        \"map\": \"O\", # Roundabout\n        \"traffic\": 0.0,\n        \"budget\": 150_000,\n        \"reward\": {\n            \"base_w\": 1.0,\n            \"speed_w\": 0.05,\n            \"max_speed_kmh\": 80.0,\n            \"collision_penalty\": -6.0,  # slightly harsher for bad manoeuvres\n            \"offroad_penalty\": -4.0,\n            \"traffic_violation_penalty\": -3.0,\n            \"success_bonus\": 10.0,\n            \"step_penalty\": -0.005,\n        },\n    },\n\n    # C2: 20-block PG map with **light traffic** – first exposure to traffic.\n    {\n        \"id\": \"C2\",\n        \"name\": \"C2_LightTraffic\",\n        \"map\": 10, # 20-block\n        \"traffic\": 0.05,               # light traffic\n        \"budget\": 200_000,\n        \"reward\": {\n            \"base_w\": 1.0,\n            \"speed_w\": 0.08,            # encourage moving at speed in traffic\n            \"max_speed_kmh\": 80.0,\n            \"collision_penalty\": -8.0,\n            \"offroad_penalty\": -6.0,\n            \"traffic_violation_penalty\": -4.0,\n            \"success_bonus\": 12.0,\n            \"step_penalty\": -0.01,\n        },\n    },\n\n    # C3: same PG map with **dense traffic** – “multi-agent-ish” final stage.\n    # Still single learning ego, but many interacting vehicles (like CuRLA's\n    # higher-traffic final curriculum stage).\n    {\n        \"id\": \"C3\",\n        \"name\": \"C3_DenseTraffic\",\n        \"map\": 20,\n        \"traffic\": 0.30,               # dense traffic ≈ mild multi-agent\n        \"budget\": 200_000,\n        \"reward\": {\n            \"base_w\": 1.0,\n            \"speed_w\": 0.10,\n            \"max_speed_kmh\": 80.0,\n            \"collision_penalty\": -10.0, # strong safety pressure\n            \"offroad_penalty\": -8.0,\n            \"traffic_violation_penalty\": -5.0,\n            \"success_bonus\": 15.0,\n            \"step_penalty\": -0.05,\n        },\n    },\n]\n\nTOTAL_CURRICULUM_BUDGET = sum(s[\"budget\"] for s in STAGES)\nprint(\"Total curriculum budget (per algorithm) =\", TOTAL_CURRICULUM_BUDGET)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e386723b","outputId":"4d669274-6425-41ab-e2df-579021c5aabe","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.802971Z","iopub.execute_input":"2025-11-28T16:28:07.803162Z","iopub.status.idle":"2025-11-28T16:28:07.837978Z","shell.execute_reply.started":"2025-11-28T16:28:07.803147Z","shell.execute_reply":"2025-11-28T16:28:07.837359Z"}},"outputs":[{"name":"stdout","text":"Total curriculum budget (per algorithm) = 650000\n","output_type":"stream"}],"execution_count":11},{"id":"B-HaQNeltf-R","cell_type":"code","source":"# Held-out 1: fixed 6-block map SCrRXO with medium traffic\nHELDOUT_SCENARIO_STAGE = {\n    \"id\": \"HELDOUT_SCENARIO\",\n    \"name\": \"HELDOUT_SCrRXO_MedTraffic\",\n    \"map\": \"SCrRXO\", # Straight -> Circular -> in-ramp -> out-ramp -> intersection -> roundabout\n    \"traffic\": 0.30, # medium-ish traffic\n    \"budget\": 0, # no training budget, eval only\n    \"reward\": STAGES[-1][\"reward\"],  # reuse hardest-stage shaping for comparability\n}\n\n# Held-out 2: VaryingDynamics environment (dynamics robustness test)\nVARYING_DYNAMICS_CONFIG = dict(\n    num_scenarios=100,\n    map=5,                  # small PG map\n    log_level=logging.ERROR,\n    # random_dynamics uses default ranges from docs; that's enough to show robustness\n)","metadata":{"id":"B-HaQNeltf-R","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.838834Z","iopub.execute_input":"2025-11-28T16:28:07.839108Z","iopub.status.idle":"2025-11-28T16:28:07.855649Z","shell.execute_reply.started":"2025-11-28T16:28:07.839090Z","shell.execute_reply":"2025-11-28T16:28:07.855003Z"}},"outputs":[],"execution_count":12},{"id":"w_ndV2-1wbQZ","cell_type":"code","source":"# Folder convention\nEXPERIMENT_ROOT = Path('experiments')\nEXPERIMENT_ROOT.mkdir(exist_ok=True)\n\n# Seeds and workers\nSEEDS = [0]\nN_ENVS = 1 # should be 8 but metadrive issues\nEVAL_FREQ = 10_000\nEVAL_EPISODES = 5\n\n# Held-out test map\n# HELDOUT_MAP = (\"Fork\", 0.2)","metadata":{"id":"w_ndV2-1wbQZ","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.856434Z","iopub.execute_input":"2025-11-28T16:28:07.856674Z","iopub.status.idle":"2025-11-28T16:28:07.869846Z","shell.execute_reply.started":"2025-11-28T16:28:07.856650Z","shell.execute_reply":"2025-11-28T16:28:07.869270Z"}},"outputs":[],"execution_count":13},{"id":"HjNj-DhhwaJR","cell_type":"code","source":"# Hyperparameters\nHYPERS = {\n    'PPO': {\n        'policy':'MlpPolicy',\n        'policy_kwargs':{'net_arch':[64,64]}, # can do 64, 64 as well\n        'learning_rate':3e-4,\n        'n_steps':1024, # compute issue, can do 2048 if faster\n        'batch_size':64,\n        'n_epochs':10,\n        'gamma':0.99,\n        'clip_range':0.2\n    },\n    # 'SAC': {\n    #     'policy':'MlpPolicy',\n    #     'policy_kwargs':{'net_arch':[256,256]},\n    #     'learning_rate':3e-4,\n    #     'batch_size':256,\n    #     'buffer_size':100_000,\n    #     'gamma':0.99\n    # },\n    'SAC':{\n        'policy':'MlpPolicy',\n        'policy_kwargs':{'net_arch':[128,128]},\n        'learning_rate':3e-4,\n        'batch_size':128,\n        'buffer_size':100_000,\n        'gamma':0.99\n    },\n    'DQN': { # Atari setup\n        'policy':'MlpPolicy',\n        'policy_kwargs':{'net_arch':[64,64]},\n        'learning_rate':1e-4,\n        'buffer_size':100_000, # can do 50,000 if needed\n        'batch_size':32,\n        'train_freq':4\n    }\n}","metadata":{"id":"HjNj-DhhwaJR","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.870715Z","iopub.execute_input":"2025-11-28T16:28:07.870893Z","iopub.status.idle":"2025-11-28T16:28:07.883947Z","shell.execute_reply.started":"2025-11-28T16:28:07.870879Z","shell.execute_reply":"2025-11-28T16:28:07.883281Z"}},"outputs":[],"execution_count":14},{"id":"0e299413","cell_type":"markdown","source":"Includes a discrete-action wrapper for DQN (maps discrete indices -> continuous steer/throttle).","metadata":{"id":"0e299413"}},{"id":"3e0cb97e","cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\n\nclass DiscreteActionWrapper(gym.ActionWrapper):\n    def __init__(self, env, mapping):\n        super().__init__(env)\n        self.mapping = mapping\n        self.action_space = spaces.Discrete(len(mapping))\n\n    def action(self, action):\n        return np.array(self.mapping[action], dtype=np.float32)","metadata":{"id":"3e0cb97e","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.884595Z","iopub.execute_input":"2025-11-28T16:28:07.884815Z","iopub.status.idle":"2025-11-28T16:28:07.898347Z","shell.execute_reply.started":"2025-11-28T16:28:07.884790Z","shell.execute_reply":"2025-11-28T16:28:07.897774Z"}},"outputs":[],"execution_count":15},{"id":"wLTVGUAADX1r","cell_type":"code","source":"class CurriculumRewardWrapper(gym.Wrapper):\n    \"\"\"\n    Stage-dependent reward shaping:\n    - Start from MetaDrive's base reward.\n    - Add speed term.\n    - Add collision / off-road / traffic-violation penalties.\n    - Add success bonus.\n\n    Uses the per-stage reward config from STAGES.\n    \"\"\"\n    def __init__(self, env, reward_cfg):\n        super().__init__(env)\n        self.cfg = reward_cfg\n        self.max_speed = self.cfg.get(\"max_speed_kmh\", 80.0)\n\n    def step(self, action):\n        # MetaDrive uses Gymnasium API: obs, reward, terminated, truncated, info\n        obs, base_r, terminated, truncated, info = self.env.step(action)\n\n        # --- speed term ---\n        # MetaDrive usually exposes speed either as 'speed' or 'velocity'\n        raw_speed = float(info.get(\"speed\", info.get(\"velocity\", 0.0)))\n        speed = max(0.0, min(raw_speed, self.max_speed))\n        speed_term = self.cfg.get(\"speed_w\", 0.0) * (speed / self.max_speed)\n\n        # --- start from scaled base reward + speed shaping ---\n        r = self.cfg.get(\"base_w\", 1.0) * base_r + speed_term\n\n        # --- per-step cost (encourage finishing sooner) ---\n        r += self.cfg.get(\"step_penalty\", 0.0)\n\n        # --- collision penalties ---\n        crashed = (\n            info.get(\"crash_vehicle\", False)\n            or info.get(\"crash_object\", False)\n            or info.get(\"crash_building\", False)\n        )\n        if crashed:\n            r += self.cfg.get(\"collision_penalty\", 0.0)\n        info[\"collision\"] = bool(crashed)\n\n        # --- off-road / traffic-violation penalties ---\n        offroad = info.get(\"out_of_road\", False)\n        if offroad:\n            r += self.cfg.get(\"offroad_penalty\", 0.0)\n\n        # generic \"traffic violation\" flag for your metrics callback\n        traffic_violation = bool(offroad or info.get(\"traffic_light_violation\", False))\n        if traffic_violation:\n            r += self.cfg.get(\"traffic_violation_penalty\", 0.0)\n        info[\"traffic_violation\"] = traffic_violation\n\n        # --- success bonus at terminal step ---\n        success = bool(info.get(\"arrive_dest\", False) or info.get(\"success\", False))\n        if terminated and success:\n            r += self.cfg.get(\"success_bonus\", 0.0)\n        info[\"success\"] = success\n\n        # For logging\n        info[\"avg_speed\"] = speed\n        info[\"shaped_reward\"] = r\n\n        return obs, r, terminated, truncated, info","metadata":{"id":"wLTVGUAADX1r","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.898988Z","iopub.execute_input":"2025-11-28T16:28:07.899614Z","iopub.status.idle":"2025-11-28T16:28:07.911642Z","shell.execute_reply.started":"2025-11-28T16:28:07.899589Z","shell.execute_reply":"2025-11-28T16:28:07.910981Z"}},"outputs":[],"execution_count":16},{"id":"75c2b81f","cell_type":"markdown","source":"## 4. Functions","metadata":{"id":"75c2b81f"}},{"id":"nGXJIcxPxSfh","cell_type":"markdown","source":"### 4.1 Environment factory","metadata":{"id":"nGXJIcxPxSfh"}},{"id":"vKLjHVxvkKCh","cell_type":"code","source":"import gymnasium as gym\n\nclass MetaDriveGymCompatibilityWrapper(gym.Wrapper):\n    \"\"\"\n    Makes MetaDriveEnv follow the Gymnasium reset() and step() signature.\n    Removes unsupported arguments like options.\n    \"\"\"\n    def reset(self, *, seed=None, options=None):\n        if seed is not None:\n            obs, info = self.env.reset(seed=seed)\n        else:\n            obs, info = self.env.reset()\n        return obs, info\n\n    def step(self, action):\n        obs, reward, done, truncated, info = self.env.step(action)\n        # MetaDrive uses done only; Gymnasium expects (terminated, truncated)\n        terminated = done\n        return obs, reward, terminated, truncated, info","metadata":{"id":"vKLjHVxvkKCh","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.912412Z","iopub.execute_input":"2025-11-28T16:28:07.912687Z","iopub.status.idle":"2025-11-28T16:28:07.927330Z","shell.execute_reply.started":"2025-11-28T16:28:07.912665Z","shell.execute_reply":"2025-11-28T16:28:07.926727Z"}},"outputs":[],"execution_count":17},{"id":"sv070jNaDnyR","cell_type":"code","source":"from functools import partial\nimport logging\n\ndef make_metadrive_env(stage, use_discrete=False, seed=0, render=False):\n    \"\"\"\n    stage: one of the dicts from STAGES. Uses:\n        stage[\"map\"], stage[\"traffic\"], stage[\"reward\"]\n    \"\"\"\n    map_name = stage[\"map\"]\n    traffic_density = stage[\"traffic\"]\n    reward_cfg = stage[\"reward\"]\n\n    def _init():\n        cfg = {\n            \"map\": map_name,\n            \"traffic_density\": traffic_density,\n            \"use_render\": render,\n            \"start_seed\": seed,\n            # \"random_spawn\": True,\n            \"debug\": False,\n            \"log_level\": logging.ERROR,\n            # cap episode length so eval can't run forever\n            \"horizon\": 1000, # max steps per episode\n            \"truncate_as_terminate\": True,   # treat horizon as done\n        }\n        env = MetaDriveEnv(cfg)\n\n        # Fix reset() signature mismatch\n        env = MetaDriveGymCompatibilityWrapper(env)\n\n        # CuRLA-style stage-dependent reward shaping\n        env = CurriculumRewardWrapper(env, reward_cfg)\n\n        # discrete-action wrapper for DQN: [steering, throttle]\n        if use_discrete:\n            mapping = [\n              (-1.0, 0.0), # hard left, no throttle\n              (-1.0, 0.3), # left + some accel\n              (0.0, 0.5), # go straight, accel\n              (1.0, 0.3), # right + some accel\n              (1.0, 0.0), # hard right, no throttle\n            ]\n            env = DiscreteActionWrapper(env, mapping)\n\n        return Monitor(env)\n\n    return _init\n","metadata":{"id":"sv070jNaDnyR","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.928086Z","iopub.execute_input":"2025-11-28T16:28:07.928305Z","iopub.status.idle":"2025-11-28T16:28:07.944374Z","shell.execute_reply.started":"2025-11-28T16:28:07.928282Z","shell.execute_reply":"2025-11-28T16:28:07.943678Z"}},"outputs":[],"execution_count":18},{"id":"v9TCh0Q_DpL4","cell_type":"code","source":"# def make_vec_env(stage, n_envs=8, use_discrete=False, seed=0, parallel=False):\n#     factories = [\n#         make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n#         for i in range(n_envs)\n#     ]\n#     if parallel:\n#         return SubprocVecEnv(factories)\n#     else:\n#         return DummyVecEnv(factories)\n\n# from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n\n# def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n#     \"\"\"\n#     Create vectorized MetaDrive envs.\n#     IMPORTANT: MetaDrive can only have one engine per process.\n#     So:\n#       - n_envs == 1  -> use DummyVecEnv (single process, single env)\n#       - n_envs > 1   -> use SubprocVecEnv (one env per process)\n#     \"\"\"\n#     if n_envs == 1:\n#         return DummyVecEnv([\n#             make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n#         ])\n#     else:\n#         env_fns = [\n#             make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n#             for i in range(n_envs)\n#         ]\n#         return SubprocVecEnv(env_fns)\n\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\ndef make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n    \"\"\"\n    Create vectorized MetaDrive envs.\n    FIX: We must ALWAYS use SubprocVecEnv for MetaDrive.\n    Using DummyVecEnv (single process) prevents creating a second env\n    (like eval_env) because MetaDrive allows only one engine per process.\n    \"\"\"\n    # env_fns = [\n    #     make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n    #     for i in range(n_envs)\n    # ]\n\n    # # FORCE SubprocVecEnv even if n_envs=1\n    # return SubprocVecEnv(env_fns)\n\n    return DummyVecEnv([\n        make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n    ])","metadata":{"id":"v9TCh0Q_DpL4","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.945113Z","iopub.execute_input":"2025-11-28T16:28:07.945377Z","iopub.status.idle":"2025-11-28T16:28:07.963267Z","shell.execute_reply.started":"2025-11-28T16:28:07.945350Z","shell.execute_reply":"2025-11-28T16:28:07.962455Z"}},"outputs":[],"execution_count":19},{"id":"dTkPoInXfp4F","cell_type":"markdown","source":"PPO/SAC use continous control (`Box`) from MetaDrive while DQN uses a manually defined discrete control space.","metadata":{"id":"dTkPoInXfp4F"}},{"id":"Q-OkesJEflDV","cell_type":"code","source":"vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=False, seed=0)\nbase_env = vec.envs[0]    # DummyVecEnv\nprint(\"PPO/SAC action space:\", base_env.action_space)\nprint(\"obs space:\", base_env.observation_space)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-OkesJEflDV","outputId":"2a120405-d933-4a83-ccbc-e1454ff5cd3e","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.964061Z","iopub.execute_input":"2025-11-28T16:28:07.964294Z","iopub.status.idle":"2025-11-28T16:28:07.985724Z","shell.execute_reply.started":"2025-11-28T16:28:07.964274Z","shell.execute_reply":"2025-11-28T16:28:07.985003Z"}},"outputs":[{"name":"stdout","text":"PPO/SAC action space: Box(-1.0, 1.0, (2,), float32)\nobs space: Box(-0.0, 1.0, (259,), float32)\n","output_type":"stream"}],"execution_count":20},{"id":"TA49LfETfmvF","cell_type":"code","source":"vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=True, seed=0)\nbase_env = vec.envs[0]    # DummyVecEnv\nprint(\"DQN action space:\", base_env.action_space)\nprint(\"obs space:\", base_env.observation_space)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TA49LfETfmvF","outputId":"f4a4b5da-67b3-422d-c4ef-23b67dc30b01","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:07.986411Z","iopub.execute_input":"2025-11-28T16:28:07.986627Z","iopub.status.idle":"2025-11-28T16:28:08.004850Z","shell.execute_reply.started":"2025-11-28T16:28:07.986601Z","shell.execute_reply":"2025-11-28T16:28:08.004232Z"}},"outputs":[{"name":"stdout","text":"DQN action space: Discrete(5)\nobs space: Box(-0.0, 1.0, (259,), float32)\n","output_type":"stream"}],"execution_count":21},{"id":"hJa0pGDE4Z1J","cell_type":"code","source":"def make_eval_vec_env(stage, use_discrete=False, seed=0):\n    \"\"\"\n    Eval env:\n    - Always uses SubprocVecEnv (even with 1 worker) so that MetaDriveEnv\n      is only ever created in subprocesses, not in the main process.\n    \"\"\"\n    env_fns = [make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)]\n    return SubprocVecEnv(env_fns)\n","metadata":{"id":"hJa0pGDE4Z1J","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.005538Z","iopub.execute_input":"2025-11-28T16:28:08.005862Z","iopub.status.idle":"2025-11-28T16:28:08.022580Z","shell.execute_reply.started":"2025-11-28T16:28:08.005841Z","shell.execute_reply":"2025-11-28T16:28:08.021884Z"}},"outputs":[],"execution_count":22},{"id":"34d56696","cell_type":"markdown","source":"### 4.2 log per-eval metrics (success rate, collisions, speed etc)","metadata":{"id":"34d56696"}},{"id":"zGjpX3W1FZsu","cell_type":"code","source":"class MetricsCallback(BaseCallback):\n    \"\"\"\n    Run a short evaluation every eval_freq steps and log:\n      - mean_reward\n      - success_rate\n      - collision_rate\n      - traffic_violation_rate\n      - avg_speed\n      - avg_episode_length\n\n    Saves to a CSV at csv_path.\n    \"\"\"\n\n    def __init__(self, eval_env, csv_path, eval_freq=50_000, eval_episodes=10, verbose=0):\n        super().__init__(verbose)\n        self.eval_env = eval_env\n        self.csv_path = csv_path\n        self.eval_freq = eval_freq\n        self.eval_episodes = eval_episodes\n\n        # Create dir if needed\n        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n\n        # Write header if file doesn't exist\n        if not os.path.exists(self.csv_path):\n            with open(self.csv_path, \"w\", newline=\"\") as f:\n                writer = csv.DictWriter(\n                    f,\n                    fieldnames=[\n                        \"timesteps\",\n                        \"mean_reward\",\n                        \"success_rate\",\n                        \"collision_rate\",\n                        \"traffic_violation_rate\",\n                        \"avg_speed\",\n                        \"avg_episode_length\",\n                    ],\n                )\n                writer.writeheader()\n\n    def _on_step(self) -> bool:\n        # Only evaluate every eval_freq calls\n        if self.n_calls % self.eval_freq != 0:\n            return True\n\n        episode_rewards = []\n        episode_successes = []\n        episode_collisions = []\n        episode_traffic_violations = []\n        episode_speeds = []\n        episode_lengths = []\n\n        for _ in range(self.eval_episodes):\n            # DummyVecEnv.reset() -> obs (no info, vec-batched)\n            obs = self.eval_env.reset()\n            info = {}\n            done = False\n\n            ep_reward = 0.0\n            ep_success = False\n            ep_collision = False\n            ep_traffic_violation = False\n            ep_steps = 0\n            ep_speeds = []\n\n            while not done:\n                # obs shape: (n_envs, obs_dim); n_envs = 1 here\n                action, _ = self.model.predict(obs, deterministic=True)\n                # DummyVecEnv.step() -> obs, rewards, dones, infos\n                obs, rewards, dones, infos = self.eval_env.step(action)\n\n                # unwrap vec env outputs for single env\n                if isinstance(rewards, (np.ndarray, list, tuple)):\n                    r = float(rewards[0])\n                else:\n                    r = float(rewards)\n\n                if isinstance(dones, (np.ndarray, list, tuple)):\n                    d = bool(dones[0])\n                else:\n                    d = bool(dones)\n\n                if isinstance(infos, (list, tuple)) and len(infos) > 0:\n                    info = infos[0]\n                else:\n                    info = infos\n\n                ep_reward += r\n                ep_steps += 1\n                done = d\n\n                # if isinstance(info, dict):\n                #     if info.get(\"success\", False):\n                #         ep_success = True\n                #     if info.get(\"collision\", False):\n                #         ep_collision = True\n                #     if info.get(\"traffic_violation\", False):\n                #         ep_traffic_violation = True\n\n                #     if \"avg_speed\" in info:\n                #         ep_speeds.append(float(info[\"avg_speed\"]))\n                #     elif \"speed\" in info:\n                #         ep_speeds.append(float(info[\"speed\"]))\n\n                # flags from MetaDrive info / our wrapper\n                if isinstance(info, dict):\n                    # success: either our wrapper's \"success\" OR MetaDrive's arrive_dest\n                    if info.get(\"success\", False) or info.get(\"arrive_dest\", False):\n                        ep_success = True\n\n                    # collision: either our wrapper's \"collision\" OR any crash/out-of-road\n                    if (\n                        info.get(\"collision\", False)\n                        or info.get(\"crash_vehicle\", False)\n                        or info.get(\"crash_object\", False)\n                        or info.get(\"crash_building\", False)\n                        or info.get(\"out_of_road\", False)\n                    ):\n                        ep_collision = True\n\n                    # traffic violation if we ever log it; otherwise this will stay 0\n                    if info.get(\"traffic_violation\", False):\n                        ep_traffic_violation = True\n\n                    # speed logging\n                    if \"avg_speed\" in info:\n                        ep_speeds.append(float(info[\"avg_speed\"]))\n                    elif \"speed\" in info:\n                        ep_speeds.append(float(info[\"speed\"]))\n\n\n            episode_rewards.append(ep_reward)\n            episode_successes.append(1.0 if ep_success else 0.0)\n            episode_collisions.append(1.0 if ep_collision else 0.0)\n            episode_traffic_violations.append(1.0 if ep_traffic_violation else 0.0)\n            episode_lengths.append(ep_steps)\n            if ep_speeds:\n                episode_speeds.append(sum(ep_speeds) / len(ep_speeds))\n\n        mean_reward = float(sum(episode_rewards) / len(episode_rewards)) if episode_rewards else 0.0\n        success_rate = float(sum(episode_successes) / len(episode_successes)) if episode_successes else 0.0\n        collision_rate = float(sum(episode_collisions) / len(episode_collisions)) if episode_collisions else 0.0\n        traffic_violation_rate = float(sum(episode_traffic_violations) / len(episode_traffic_violations)) if episode_traffic_violations else 0.0\n        avg_speed = float(sum(episode_speeds) / len(episode_speeds)) if episode_speeds else 0.0\n        avg_episode_length = float(sum(episode_lengths) / len(episode_lengths)) if episode_lengths else 0.0\n\n        row = {\n            \"timesteps\": int(self.num_timesteps),\n            \"mean_reward\": mean_reward,\n            \"success_rate\": success_rate,\n            \"collision_rate\": collision_rate,\n            \"traffic_violation_rate\": traffic_violation_rate,\n            \"avg_speed\": avg_speed,\n            \"avg_episode_length\": avg_episode_length,\n        }\n\n        with open(self.csv_path, \"a\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=row.keys())\n            writer.writerow(row)\n\n        if self.verbose > 0:\n            print(f\"[Metrics] t={self.num_timesteps}  succ={success_rate:.2f}  \"\n                  f\"coll={collision_rate:.2f}  len={avg_episode_length:.1f}\")\n\n        return True\n","metadata":{"id":"zGjpX3W1FZsu","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.023299Z","iopub.execute_input":"2025-11-28T16:28:08.023590Z","iopub.status.idle":"2025-11-28T16:28:08.040143Z","shell.execute_reply.started":"2025-11-28T16:28:08.023574Z","shell.execute_reply":"2025-11-28T16:28:08.039545Z"}},"outputs":[],"execution_count":23},{"id":"-eFLK2DKKDJR","cell_type":"code","source":"from stable_baselines3.common.callbacks import EvalCallback\n\nclass PrettyEvalCallback(EvalCallback):\n    \"\"\"\n    Clean pretty printing for eval:\n      - One separator before the first eval\n      - No spam between evals\n      - One separator at the end of training on this stage\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._started = False    # whether we have printed the first separator\n\n    def _on_step(self) -> bool:\n        # Check if it's time to evaluate\n        if self.n_calls % self.eval_freq == 0:\n            # On first eval, print top separator\n            if not self._started:\n                print(\"\\n\" + \"-\" * 60 + \"\\n\")\n                self._started = True\n\n        return super()._on_step()\n\n    def _on_training_end(self) -> None:\n        # After training for this stage: print final separator\n        if self._started:\n            print(\"\\n\" + \"-\" * 60 + \"\\n\")\n        return super()._on_training_end()\n","metadata":{"id":"-eFLK2DKKDJR","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.040855Z","iopub.execute_input":"2025-11-28T16:28:08.041132Z","iopub.status.idle":"2025-11-28T16:28:08.059768Z","shell.execute_reply.started":"2025-11-28T16:28:08.041116Z","shell.execute_reply":"2025-11-28T16:28:08.059080Z"}},"outputs":[],"execution_count":24},{"id":"rPOxRfNNHKbp","cell_type":"code","source":"def save_run_config(out_dir, algo, stage, seed):\n    \"\"\"\n    Save basic run config (algo, stage, hyperparams, seed) to config.json\n    so you can reproduce / inspect later.\n    \"\"\"\n    cfg = {\n        \"algo\": algo,\n        \"seed\": seed,\n        \"stage\": {\n            \"id\": stage[\"id\"],\n            \"name\": stage[\"name\"],\n            \"map\": stage[\"map\"],\n            \"traffic\": stage[\"traffic\"],\n            \"budget\": stage[\"budget\"],\n            \"reward\": stage[\"reward\"],\n        },\n        \"hyperparams\": HYPERS[algo],\n        \"heldout_map\": {\"map\": HELDOUT_SCENARIO_STAGE['map'], \"traffic\": HELDOUT_SCENARIO_STAGE['traffic'], \"reward\": HELDOUT_SCENARIO_STAGE['reward']},\n    }\n    with open(out_dir / \"config.json\", \"w\") as f:\n        json.dump(cfg, f, indent=2)","metadata":{"id":"rPOxRfNNHKbp","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.060554Z","iopub.execute_input":"2025-11-28T16:28:08.060821Z","iopub.status.idle":"2025-11-28T16:28:08.074745Z","shell.execute_reply.started":"2025-11-28T16:28:08.060795Z","shell.execute_reply":"2025-11-28T16:28:08.074081Z"}},"outputs":[],"execution_count":25},{"id":"soBC1415it33","cell_type":"code","source":"print(HELDOUT_SCENARIO_STAGE)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soBC1415it33","outputId":"a1c917f2-4a8f-40e5-e4a4-bde4a6ec4beb","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.075465Z","iopub.execute_input":"2025-11-28T16:28:08.075708Z","iopub.status.idle":"2025-11-28T16:28:08.095979Z","shell.execute_reply.started":"2025-11-28T16:28:08.075689Z","shell.execute_reply":"2025-11-28T16:28:08.095182Z"}},"outputs":[{"name":"stdout","text":"{'id': 'HELDOUT_SCENARIO', 'name': 'HELDOUT_SCrRXO_MedTraffic', 'map': 'SCrRXO', 'traffic': 0.3, 'budget': 0, 'reward': {'base_w': 1.0, 'speed_w': 0.1, 'max_speed_kmh': 80.0, 'collision_penalty': -10.0, 'offroad_penalty': -8.0, 'traffic_violation_penalty': -5.0, 'success_bonus': 15.0, 'step_penalty': -0.05}}\n","output_type":"stream"}],"execution_count":26},{"id":"0303bc4f","cell_type":"markdown","source":"### 4.3 Training functions\n\nThese functions create models, attach callbacks, and run training. Each saves checkpoint, best-model and CSV metrics.\n","metadata":{"id":"0303bc4f"}},{"id":"yLzoiNexEQ6n","cell_type":"code","source":"import torch\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef make_model(algo, env, hyperparams):\n    common_kwargs = dict(\n        verbose=0,  # make SB3 quiet in console\n        tensorboard_log=str(EXPERIMENT_ROOT / 'tensorboard'),\n        policy_kwargs=hyperparams['policy_kwargs'],\n        learning_rate=hyperparams['learning_rate'],\n    )\n\n    if algo == 'PPO':\n        model = PPO(\n            hyperparams['policy'],\n            env,\n            n_steps=hyperparams['n_steps'],\n            batch_size=hyperparams['batch_size'],\n            n_epochs=hyperparams['n_epochs'],\n            gamma=hyperparams['gamma'],\n            device=\"cpu\",\n            **common_kwargs,\n        )\n        return model\n\n    if algo == 'SAC':\n        model = SAC(\n            hyperparams['policy'],\n            env,\n            batch_size=hyperparams.get('batch_size', 256),\n            buffer_size=hyperparams.get('buffer_size', 100_000),\n            gamma=hyperparams.get('gamma', 0.99),\n            device=DEVICE,\n            **common_kwargs,\n        )\n        return model\n\n    if algo == 'DQN':\n        model = DQN(\n            hyperparams['policy'],\n            env,\n            buffer_size=hyperparams.get('buffer_size', 50_000),\n            batch_size=hyperparams.get('batch_size', 32),\n            train_freq=hyperparams.get('train_freq', 4),\n            device=DEVICE,\n            **common_kwargs,\n        )\n        return model\n\n    raise ValueError('Unknown algo')\n","metadata":{"id":"yLzoiNexEQ6n","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.096742Z","iopub.execute_input":"2025-11-28T16:28:08.096961Z","iopub.status.idle":"2025-11-28T16:28:08.110659Z","shell.execute_reply.started":"2025-11-28T16:28:08.096934Z","shell.execute_reply":"2025-11-28T16:28:08.109930Z"}},"outputs":[],"execution_count":27},{"id":"z5yA8SnOtpCg","cell_type":"code","source":"def evaluate_env_episodes(model, env, n_episodes=20, deterministic=True):\n    \"\"\"\n    Run n_episodes on a *single* (non-vec) env.\n    Returns (mean_reward, std_reward, mean_ep_len).\n    \"\"\"\n    rewards = []\n    lengths = []\n\n    for _ in range(n_episodes):\n        reset_out = env.reset()\n        # Handle reset() returning either obs or (obs, info)\n        if isinstance(reset_out, tuple):\n            obs, _info = reset_out\n        else:\n            obs = reset_out\n        info = {}\n        done = False\n        truncated = False\n        ep_r = 0.0\n        steps = 0\n\n        while not (done or truncated):\n            action, _ = model.predict(obs, deterministic=deterministic)\n            step_out = env.step(action)\n            # step could be:\n            #  - (obs, reward, done, info)  [old Gym]\n            #  - (obs, reward, terminated, truncated, info) [Gymnasium]\n            if len(step_out) == 5:\n                obs, r, terminated, truncated, info = step_out\n                done = bool(terminated or truncated)\n            elif len(step_out) == 4:\n                obs, r, done, info = step_out\n                truncated = False\n            else:\n                raise RuntimeError(f\"Unexpected env.step() output length: {len(step_out)}\")\n\n            truncated = False\n            ep_r += float(r)\n            steps += 1\n\n        rewards.append(ep_r)\n        lengths.append(steps)\n\n    rewards = np.array(rewards, dtype=np.float32)\n    lengths = np.array(lengths, dtype=np.float32)\n    return float(rewards.mean()), float(rewards.std()), float(lengths.mean())\n","metadata":{"id":"z5yA8SnOtpCg","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.114195Z","iopub.execute_input":"2025-11-28T16:28:08.114425Z","iopub.status.idle":"2025-11-28T16:28:08.128529Z","shell.execute_reply.started":"2025-11-28T16:28:08.114400Z","shell.execute_reply":"2025-11-28T16:28:08.127934Z"}},"outputs":[],"execution_count":28},{"id":"eCc8tf5CtVuA","cell_type":"code","source":"def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=1):\n    \"\"\"\n    Non-curriculum baseline:\n    - Train from scratch on a SINGLE stage for 'total_timesteps'.\n    - Eval during training on SAME env via PrettyEvalCallback (already set up).\n    - After training, eval on two held-out envs:\n        1) SCrRXO + medium traffic (MetaDriveEnv)\n        2) VaryingDynamicsEnv (randomized dynamics)\n    \"\"\"\n\n    out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    print(f\"Training NON-CURRICULUM: {algo} {stage['name']} seed {seed}\\n\")\n\n    use_discrete = (algo == \"DQN\")\n\n    # ---- training env (single vec env) ----\n    env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n\n    model = make_model(algo, env, HYPERS[algo])\n\n    # in-training eval (same env)\n    eval_cb = PrettyEvalCallback(\n        env,\n        best_model_save_path=str(out_dir / \"best_model\"),\n        log_path=str(out_dir / \"eval\"),\n        eval_freq=EVAL_FREQ,\n        n_eval_episodes=EVAL_EPISODES,\n        deterministic=True,\n        verbose=1,\n    )\n\n    ckpt_cb = CheckpointCallback(\n        save_freq=EVAL_FREQ,\n        save_path=str(out_dir / \"checkpoints\"),\n        name_prefix=\"ckpt\",\n    )\n\n    metrics_csv = out_dir / \"metrics.csv\"\n    metrics_cb = MetricsCallback(\n        eval_env=env,       # since you’re using n_envs=1 (DummyVecEnv)\n        csv_path=str(metrics_csv),\n        eval_freq=EVAL_FREQ,\n        eval_episodes=EVAL_EPISODES,\n        verbose=0,\n    )\n\n    save_run_config(out_dir, algo, stage, seed)\n\n    # ---- train ----\n    model.learn(\n        total_timesteps=total_timesteps,\n        callback=[eval_cb, ckpt_cb, metrics_cb],\n    )\n    model.save(str(out_dir / \"model.zip\"))\n\n    print(f\"\\nNon-curriculum training complete and saved to {out_dir}\")\n\n    # Close training vec env to avoid engine conflicts before new envs\n    env.close()\n\n    # =====================================================================\n    # HELD-OUT 1: SCrRXO + medium traffic\n    # =====================================================================\n    print(\"\\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\")\n\n    # build a *single* env instance using same wrappers\n    held1_make = make_metadrive_env(\n        HELDOUT_SCENARIO_STAGE,\n        use_discrete=use_discrete,\n        seed=seed + 1000,\n        render=False,\n    )\n    held1_env = held1_make()\n    held1_env = MetaDriveGymCompatibilityWrapper(held1_env)\n\n    h1_mean, h1_std, h1_len = evaluate_env_episodes( #, h1_len if manual function\n        model,\n        held1_env,\n        n_episodes=20,\n        deterministic=True,\n    )\n\n    print(\n        f\"HELD-OUT 1 (SCrRXO): mean_reward={h1_mean:.2f} ± {h1_std:.2f}, \"\n        f\"mean_ep_len={h1_len:.1f}\"\n    )\n\n    pd.DataFrame(\n        [{\n            \"algo\": algo,\n            \"train_stage\": stage[\"name\"],\n            \"heldout_name\": HELDOUT_SCENARIO_STAGE[\"name\"],\n            \"mean_reward\": h1_mean,\n            \"std_reward\": h1_std,\n            \"mean_ep_len\": h1_len,\n        }]\n    ).to_csv(out_dir / \"heldout_scrrxo_metrics.csv\", index=False)\n\n    held1_env.close()\n\n    # =====================================================================\n    # HELD-OUT 2: VaryingDynamicsEnv\n    # =====================================================================\n    # print(\"\\n[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\")\n\n    # # build varying dynamics env\n    # vd_env = VaryingDynamicsEnv(VARYING_DYNAMICS_CONFIG)\n    # vd_env = MetaDriveGymCompatibilityWrapper(vd_env)\n\n    # if use_discrete:\n    #     # same discrete mapping you used for training DQN\n    #     discrete_mapping = [(-1.0, 0.0), (-1.0, 0.3), (0.0, 0.5), (1.0, 0.3), (1.0, 0.0)]\n    #     vd_env = DiscreteActionWrapper(vd_env, discrete_mapping)\n\n    # h2_mean, h2_std, h2_len = evaluate_env_episodes(\n    #     model,\n    #     vd_env,\n    #     n_episodes=20,\n    #     deterministic=True,\n    # )\n\n    # print(\n    #     f\"HELD-OUT 2 (VaryingDynamics): mean_reward={h2_mean:.2f} ± {h2_std:.2f}, \"\n    #     f\"mean_ep_len={h2_len:.1f}\"\n    # )\n\n    # pd.DataFrame(\n    #     [{\n    #         \"algo\": algo,\n    #         \"train_stage\": stage[\"name\"],\n    #         \"heldout_name\": \"VaryingDynamicsEnv\",\n    #         \"mean_reward\": h2_mean,\n    #         \"std_reward\": h2_std,\n    #         \"mean_ep_len\": h2_len,\n    #     }]\n    # ).to_csv(out_dir / \"heldout_varying_metrics.csv\", index=False)\n\n    # vd_env.close()\n\n    return out_dir\n","metadata":{"id":"eCc8tf5CtVuA","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.129262Z","iopub.execute_input":"2025-11-28T16:28:08.129582Z","iopub.status.idle":"2025-11-28T16:28:08.146360Z","shell.execute_reply.started":"2025-11-28T16:28:08.129564Z","shell.execute_reply":"2025-11-28T16:28:08.145791Z"}},"outputs":[],"execution_count":29},{"id":"83889cd2","cell_type":"markdown","source":"### 4.4. Visualization (plot metrics, learning curves, and display videos)","metadata":{"id":"83889cd2"}},{"id":"57e24184","cell_type":"code","source":"def plot_metrics(csv_path, title=None):\n    if not os.path.exists(csv_path):\n        print('CSV not found:', csv_path); return\n    df = pd.read_csv(csv_path)\n    fig, axs = plt.subplots(2,2, figsize=(12,8))\n    axs = axs.flatten()\n    axs[0].plot(df['total_timesteps'], df['mean_reward'], marker='o'); axs[0].set_title('Mean reward')\n    axs[1].plot(df['total_timesteps'], df['success_rate'], marker='o'); axs[1].set_title('Success rate')\n    axs[2].plot(df['total_timesteps'], df['collision_rate'], marker='o'); axs[2].set_title('Collision rate')\n    axs[3].plot(df['total_timesteps'], df['avg_speed'], marker='o'); axs[3].set_title('Avg speed')\n    if title: fig.suptitle(title)\n    plt.tight_layout(); plt.show()\n\nprint('Plot helper ready')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57e24184","outputId":"f68ec532-040e-4f16-eb9a-e35e5fb4a11f","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.147110Z","iopub.execute_input":"2025-11-28T16:28:08.147366Z","iopub.status.idle":"2025-11-28T16:28:08.166238Z","shell.execute_reply.started":"2025-11-28T16:28:08.147342Z","shell.execute_reply":"2025-11-28T16:28:08.165643Z"}},"outputs":[{"name":"stdout","text":"Plot helper ready\n","output_type":"stream"}],"execution_count":30},{"id":"644f2875","cell_type":"markdown","source":"## 5. Toy pilot run\n\nSmall pilot run to validate pipeline.\n","metadata":{"id":"644f2875"}},{"id":"aef7a3ca","cell_type":"code","source":"# # run one tiny non-curriculum PPO for 2000 steps on Straight map\n\n# out = train_noncurriculum('SAC', STAGES[2], total_timesteps=2000, seed=0, n_envs=1)\n# print('Pilot saved at:', out)","metadata":{"id":"aef7a3ca","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.167054Z","iopub.execute_input":"2025-11-28T16:28:08.167723Z","iopub.status.idle":"2025-11-28T16:28:08.181384Z","shell.execute_reply.started":"2025-11-28T16:28:08.167695Z","shell.execute_reply":"2025-11-28T16:28:08.180701Z"}},"outputs":[],"execution_count":31},{"id":"95c6e5f6","cell_type":"markdown","source":"## 6. Full experiment\n\nFor each algorithm, for each seed.\n","metadata":{"id":"95c6e5f6"}},{"id":"xPZ5O3NLHg94","cell_type":"code","source":"algos = [\"SAC\"] # \"SAC\", \"DQN\",\n\nfor algo in algos:\n    for seed in SEEDS:\n        print(\"=\" * 80)\n        print(f\"ALGO={algo}  SEED={seed}\")\n\n        for stage in STAGES:\n            train_noncurriculum(\n                algo=algo,\n                stage=stage,\n                # total_timesteps=5000,\n                total_timesteps=stage[\"budget\"],  # use this stage's budget\n                seed=seed,\n                n_envs=N_ENVS,\n            )","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPZ5O3NLHg94","outputId":"83ed71a9-d8a4-4a32-ad49-001cbe625eaf","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:28:08.182256Z","iopub.execute_input":"2025-11-28T16:28:08.182587Z","iopub.status.idle":"2025-11-28T22:07:09.146921Z","shell.execute_reply.started":"2025-11-28T16:28:08.182559Z","shell.execute_reply":"2025-11-28T22:07:09.146243Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nALGO=SAC  SEED=0\nTraining NON-CURRICULUM: SAC C0_Straight seed 0\n\n\n------------------------------------------------------------\n\nEval num_timesteps=10000, episode_reward=10.08 +/- 0.00\nEpisode length: 119.00 +/- 0.00\nNew best mean reward!\nEval num_timesteps=20000, episode_reward=-12.81 +/- 0.00\nEpisode length: 941.00 +/- 0.00\nEval num_timesteps=30000, episode_reward=-11.19 +/- 0.00\nEpisode length: 325.00 +/- 0.00\nEval num_timesteps=40000, episode_reward=-3.19 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=50000, episode_reward=-3.97 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=60000, episode_reward=-3.09 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=70000, episode_reward=-3.23 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=80000, episode_reward=-0.73 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=90000, episode_reward=-1.70 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=100000, episode_reward=93.99 +/- 0.00\nEpisode length: 155.00 +/- 0.00\nNew best mean reward!\n\n------------------------------------------------------------\n\n\nNon-curriculum training complete and saved to experiments/SAC/noncurriculum/seed_0/C0_Straight\n\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\nHELD-OUT 1 (SCrRXO): mean_reward=19.02 ± 0.00, mean_ep_len=97.0\nTraining NON-CURRICULUM: SAC C1_Roundabout seed 0\n\n\n------------------------------------------------------------\n\nEval num_timesteps=10000, episode_reward=-5.05 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nNew best mean reward!\nEval num_timesteps=20000, episode_reward=-8.42 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=30000, episode_reward=-7.25 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=40000, episode_reward=-7.58 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=50000, episode_reward=-14.77 +/- 0.00\nEpisode length: 329.00 +/- 0.00\nEval num_timesteps=60000, episode_reward=-10.64 +/- 0.00\nEpisode length: 46.00 +/- 0.00\nEval num_timesteps=70000, episode_reward=-11.61 +/- 0.00\nEpisode length: 309.00 +/- 0.00\nEval num_timesteps=80000, episode_reward=-4.62 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nNew best mean reward!\nEval num_timesteps=90000, episode_reward=-7.17 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=100000, episode_reward=-1.81 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nNew best mean reward!\nEval num_timesteps=110000, episode_reward=-3.14 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=120000, episode_reward=-2.74 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=130000, episode_reward=-3.60 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=140000, episode_reward=-4.90 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=150000, episode_reward=-2.59 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\n\n------------------------------------------------------------\n\n\nNon-curriculum training complete and saved to experiments/SAC/noncurriculum/seed_0/C1_Roundabout\n\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\nHELD-OUT 1 (SCrRXO): mean_reward=-18.55 ± 0.00, mean_ep_len=24.0\nTraining NON-CURRICULUM: SAC C2_LightTraffic seed 0\n\n\n------------------------------------------------------------\n\nEval num_timesteps=10000, episode_reward=-10.70 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nNew best mean reward!\nEval num_timesteps=20000, episode_reward=-12.24 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=30000, episode_reward=-12.38 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=40000, episode_reward=-12.39 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=50000, episode_reward=-12.24 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=60000, episode_reward=-12.98 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=70000, episode_reward=-14.54 +/- 0.00\nEpisode length: 79.00 +/- 0.00\nEval num_timesteps=80000, episode_reward=-18.60 +/- 0.00\nEpisode length: 278.00 +/- 0.00\nEval num_timesteps=90000, episode_reward=-12.50 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=100000, episode_reward=-9.41 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nNew best mean reward!\nEval num_timesteps=110000, episode_reward=-9.62 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=120000, episode_reward=-9.77 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=130000, episode_reward=-9.84 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=140000, episode_reward=-9.86 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=150000, episode_reward=-9.90 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=160000, episode_reward=-9.99 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=170000, episode_reward=-9.93 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=180000, episode_reward=-9.95 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=190000, episode_reward=-9.89 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=200000, episode_reward=-9.97 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\n\n------------------------------------------------------------\n\n\nNon-curriculum training complete and saved to experiments/SAC/noncurriculum/seed_0/C2_LightTraffic\n\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\nHELD-OUT 1 (SCrRXO): mean_reward=-49.96 ± 0.00, mean_ep_len=1000.0\nTraining NON-CURRICULUM: SAC C3_DenseTraffic seed 0\n\n\n------------------------------------------------------------\n\nEval num_timesteps=10000, episode_reward=-8.38 +/- 0.00\nEpisode length: 53.00 +/- 0.00\nNew best mean reward!\nEval num_timesteps=20000, episode_reward=-52.17 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=30000, episode_reward=-53.74 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=40000, episode_reward=-52.06 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=50000, episode_reward=-49.98 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=60000, episode_reward=-49.34 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=70000, episode_reward=-49.44 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=80000, episode_reward=-49.70 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=90000, episode_reward=-49.32 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=100000, episode_reward=-49.30 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=110000, episode_reward=-49.71 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=120000, episode_reward=-52.27 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=130000, episode_reward=-52.17 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=140000, episode_reward=-52.13 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=150000, episode_reward=-52.16 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=160000, episode_reward=-50.79 +/- 0.00\nEpisode length: 634.00 +/- 0.00\nEval num_timesteps=170000, episode_reward=-70.87 +/- 0.02\nEpisode length: 972.20 +/- 0.40\nEval num_timesteps=180000, episode_reward=-49.31 +/- 0.00\nEpisode length: 1000.00 +/- 0.00\nEval num_timesteps=190000, episode_reward=-18.01 +/- 0.00\nEpisode length: 37.00 +/- 0.00\nEval num_timesteps=200000, episode_reward=-23.17 +/- 0.00\nEpisode length: 126.00 +/- 0.00\n\n------------------------------------------------------------\n\n\nNon-curriculum training complete and saved to experiments/SAC/noncurriculum/seed_0/C3_DenseTraffic\n\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\nHELD-OUT 1 (SCrRXO): mean_reward=-18.92 ± 0.00, mean_ep_len=31.0\n","output_type":"stream"}],"execution_count":32},{"id":"bde3ba34","cell_type":"code","source":"# algos = ['PPO','SAC','DQN']\n# for algo in algos:\n#     # non-curriculum: for each target map train for the total curriculum budget (to match sample budget)\n#     for seed in SEEDS:\n#         for (_, map_name, traffic, _) in STAGES:\n#             train_noncurriculum(algo, map_name, traffic, total_timesteps=TOTAL_CURRICULUM_BUDGET, seed=seed, n_envs=N_ENVS)","metadata":{"id":"bde3ba34","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:07:09.147986Z","iopub.execute_input":"2025-11-28T22:07:09.148182Z","iopub.status.idle":"2025-11-28T22:07:09.151602Z","shell.execute_reply.started":"2025-11-28T22:07:09.148168Z","shell.execute_reply":"2025-11-28T22:07:09.150826Z"}},"outputs":[],"execution_count":33},{"id":"qpsW4W5NI9LG","cell_type":"markdown","source":"## 5. Visualization","metadata":{"id":"qpsW4W5NI9LG"}},{"id":"-j-jWQJJgf8k","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport os\n\n# Adjust if your root is named differently\nEXPERIMENT_ROOT = Path(\"experiments\")\n\nCURVES_DIR = EXPERIMENT_ROOT / \"curves\"\nCURVES_DIR.mkdir(parents=True, exist_ok=True)\n\ndef load_eval_npz(eval_npz_path: Path):\n    \"\"\"\n    Load SB3 EvalCallback npz.\n    Returns: timesteps, mean_rewards, std_rewards, mean_ep_len\n    \"\"\"\n    data = np.load(eval_npz_path)\n    timesteps = data[\"timesteps\"].flatten()          # [n_eval]\n    results = data[\"results\"]                        # [n_eval, n_episodes]\n    ep_lengths = data[\"ep_lengths\"]                  # [n_eval, n_episodes]\n\n    mean_rewards = results.mean(axis=1)\n    std_rewards = results.std(axis=1)\n    mean_ep_len = ep_lengths.mean(axis=1)\n\n    return timesteps, mean_rewards, std_rewards, mean_ep_len\n\n\ndef plot_stage_curves(algo, seed_name, stage_name, eval_npz_path):\n    \"\"\"\n    Make per-stage plots:\n      - reward vs timesteps\n      - episode length vs timesteps\n    Save to experiments/curves\n    \"\"\"\n    t, mean_r, std_r, mean_len = load_eval_npz(eval_npz_path)\n\n    # 1) Reward curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(t, mean_r, marker=\"o\")\n    plt.fill_between(t, mean_r - std_r, mean_r + std_r, alpha=0.2)\n    plt.xlabel(\"Timesteps\")\n    plt.ylabel(\"Eval mean return\")\n    plt.title(f\"{algo} {seed_name} – {stage_name} (eval reward)\")\n    plt.grid(True)\n    plt.tight_layout()\n\n    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_reward.png\"\n    plt.savefig(out_path, dpi=150)\n    plt.close()\n\n    # 2) Episode length curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(t, mean_len, marker=\"o\")\n    plt.xlabel(\"Timesteps\")\n    plt.ylabel(\"Mean episode length\")\n    plt.title(f\"{algo} {seed_name} – {stage_name} (episode length)\")\n    plt.grid(True)\n    plt.tight_layout()\n\n    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_ep_len.png\"\n    plt.savefig(out_path, dpi=150)\n    plt.close()\n\n\ndef plot_combined_curves(algo, seed_name, stage_to_npz):\n    \"\"\"\n    Combined plots across stages for one algo+seed:\n      - all reward curves\n      - all episode length curves\n    stage_to_npz: dict {stage_name: eval_npz_path}\n    \"\"\"\n    # Reward\n    plt.figure(figsize=(7, 5))\n    for stage_name, npz_path in stage_to_npz.items():\n        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n        plt.plot(t, mean_r, marker=\"o\", label=stage_name)\n\n    plt.xlabel(\"Timesteps\")\n    plt.ylabel(\"Eval mean return\")\n    plt.title(f\"{algo} {seed_name} – eval reward (all stages)\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n\n    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_reward.png\"\n    plt.savefig(out_path, dpi=150)\n    plt.close()\n\n    # Episode length\n    plt.figure(figsize=(7, 5))\n    for stage_name, npz_path in stage_to_npz.items():\n        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n        plt.plot(t, mean_len, marker=\"o\", label=stage_name)\n\n    plt.xlabel(\"Timesteps\")\n    plt.ylabel(\"Mean episode length\")\n    plt.title(f\"{algo} {seed_name} – episode length (all stages)\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n\n    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_ep_len.png\"\n    plt.savefig(out_path, dpi=150)\n    plt.close()\n\n\ndef plot_heldout_bars(algo, seed_name, seed_dir: Path):\n    \"\"\"\n    If held-out CSVs exist for this algo+seed, make bar plots:\n      - heldout_scrrxo_metrics.csv across stages\n      - heldout_varying_metrics.csv across stages\n    \"\"\"\n    # Find all stages under this seed dir\n    stage_dirs = [\n        d for d in seed_dir.iterdir()\n        if d.is_dir() and not d.name.startswith(\".\")\n    ]\n\n    # ----- Held-out 1: SCrRXO -----\n    rows = []\n    for sd in stage_dirs:\n        csv_path = sd / \"heldout_scrrxo_metrics.csv\"\n        if csv_path.exists():\n            df = pd.read_csv(csv_path)\n            if not df.empty:\n                r = df.iloc[0].to_dict()\n                r[\"stage\"] = sd.name\n                rows.append(r)\n\n    if rows:\n        df_scr = pd.DataFrame(rows)\n        plt.figure(figsize=(6, 4))\n        plt.bar(df_scr[\"stage\"], df_scr[\"mean_reward\"])\n        plt.xlabel(\"Training stage\")\n        plt.ylabel(\"Held-out mean reward\")\n        plt.title(f\"{algo} {seed_name} – Held-out SCrRXO performance\")\n        plt.grid(axis=\"y\")\n        plt.tight_layout()\n        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_SCrRXO.png\"\n        plt.savefig(out_path, dpi=150)\n        plt.close()\n\n    # ----- Held-out 2: VaryingDynamics -----\n    rows = []\n    for sd in stage_dirs:\n        csv_path = sd / \"heldout_varying_metrics.csv\"\n        if csv_path.exists():\n            df = pd.read_csv(csv_path)\n            if not df.empty:\n                r = df.iloc[0].to_dict()\n                r[\"stage\"] = sd.name\n                rows.append(r)\n\n    if rows:\n        df_vd = pd.DataFrame(rows)\n        plt.figure(figsize=(6, 4))\n        plt.bar(df_vd[\"stage\"], df_vd[\"mean_reward\"])\n        plt.xlabel(\"Training stage\")\n        plt.ylabel(\"Held-out mean reward\")\n        plt.title(f\"{algo} {seed_name} – Held-out VaryingDynamics performance\")\n        plt.grid(axis=\"y\")\n        plt.tight_layout()\n        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_VaryingDynamics.png\"\n        plt.savefig(out_path, dpi=150)\n        plt.close()\n","metadata":{"id":"-j-jWQJJgf8k","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:07:09.152380Z","iopub.execute_input":"2025-11-28T22:07:09.152646Z","iopub.status.idle":"2025-11-28T22:07:09.174147Z","shell.execute_reply.started":"2025-11-28T22:07:09.152629Z","shell.execute_reply":"2025-11-28T22:07:09.173470Z"}},"outputs":[],"execution_count":34},{"id":"vHiU4aOEj-7z","cell_type":"code","source":"# ======================================================================\n# MAIN: walk experiments/ and generate all possible plots\n# ======================================================================\n\nfor algo_dir in EXPERIMENT_ROOT.iterdir():\n    if not algo_dir.is_dir():\n        continue\n    algo = algo_dir.name  # e.g. \"PPO\", \"SAC\", \"DQN\"\n\n    noncurr_dir = algo_dir / \"noncurriculum\"\n    if not noncurr_dir.exists():\n        continue\n\n    for seed_dir in noncurr_dir.iterdir():\n        if not seed_dir.is_dir() or not seed_dir.name.startswith(\"seed_\"):\n            continue\n        seed_name = seed_dir.name  # e.g. \"seed_0\"\n\n        print(f\"Processing {algo} / {seed_name} ...\")\n\n        # collect per-stage npz paths for combined plots\n        stage_to_npz = {}\n\n        # per-stage plots\n        for stage_dir in seed_dir.iterdir():\n            if not stage_dir.is_dir():\n                continue\n            stage_name = stage_dir.name  # e.g. \"C0_Straight\"\n\n            eval_npz_path = stage_dir / \"eval\" / \"evaluations.npz\"\n            if not eval_npz_path.exists():\n                continue\n\n            # individual plots\n            plot_stage_curves(algo, seed_name, stage_name, eval_npz_path)\n            stage_to_npz[stage_name] = eval_npz_path\n\n        # combined curves across stages\n        if stage_to_npz:\n            plot_combined_curves(algo, seed_name, stage_to_npz)\n\n        # held-out bar plots\n        plot_heldout_bars(algo, seed_name, seed_dir)\n\nprint(\"All curves saved under:\", CURVES_DIR)","metadata":{"id":"vHiU4aOEj-7z","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:07:09.174897Z","iopub.execute_input":"2025-11-28T22:07:09.175573Z","iopub.status.idle":"2025-11-28T22:07:11.318812Z","shell.execute_reply.started":"2025-11-28T22:07:09.175556Z","shell.execute_reply":"2025-11-28T22:07:11.318041Z"}},"outputs":[{"name":"stdout","text":"Processing SAC / seed_0 ...\nAll curves saved under: experiments/curves\n","output_type":"stream"}],"execution_count":35},{"id":"sZWIeaOMj_mO","cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\nfrom pathlib import Path\n\nBASE = Path(\"experiments/tensorboard\")\n\ndef load_tb_scalars(event_file):\n    event_file = str(event_file)   # <--- FIX HERE\n    ea = EventAccumulator(event_file)\n    ea.Reload()\n    scalars = {}\n    for tag in ea.Tags()[\"scalars\"]:\n        events = ea.Scalars(tag)\n        steps = [e.step for e in events]\n        values = [e.value for e in events]\n        scalars[tag] = (steps, values)\n    return scalars\n\n\ndef plot_scalar(steps, values, title, out_path):\n    plt.figure(figsize=(8,4))\n    plt.plot(steps, values)\n    plt.xlabel(\"Timesteps\")\n    plt.ylabel(title)\n    plt.title(title)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\ndef plot_stage(stage_path):\n    print(f\"\\nProcessing stage: {stage_path.name}\")\n    curves_dir = stage_path / \"curves\"\n    curves_dir.mkdir(exist_ok=True)\n\n    # Find tensorboard event file\n    event_files = list(stage_path.glob(\"**/events.out.tfevents.*\"))\n    if len(event_files)==0:\n        print(\"No TB file found for eval.\")\n        return\n    tb_file = event_files[0]\n    tb_scalars = load_tb_scalars(tb_file)\n\n    # Plot all general scalars\n    for tag, (steps, values) in tb_scalars.items():\n        clean_name = tag.replace(\"/\", \"_\")\n        out = curves_dir / f\"{clean_name}.png\"\n        plot_scalar(steps, values, f\"{stage_path.name}: {tag}\", out)\n\n    print(f\"Saved all curves for {stage_path.name} → {curves_dir}\")\n\n\n# =============================\n# PROCESS EACH STAGE\n# =============================\nall_stage_dirs = sorted([p for p in BASE.glob(\"*\") if p.is_dir()])\n\nfor stage_dir in all_stage_dirs:\n    plot_stage(stage_dir)\n\nprint(\"\\nFinished generating all plots.\")\n","metadata":{"id":"sZWIeaOMj_mO","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:07:11.319707Z","iopub.execute_input":"2025-11-28T22:07:11.320420Z","iopub.status.idle":"2025-11-28T22:07:17.715820Z","shell.execute_reply.started":"2025-11-28T22:07:11.320400Z","shell.execute_reply":"2025-11-28T22:07:17.715116Z"}},"outputs":[{"name":"stdout","text":"\nProcessing stage: SAC_1\nSaved all curves for SAC_1 → experiments/tensorboard/SAC_1/curves\n\nProcessing stage: SAC_2\nSaved all curves for SAC_2 → experiments/tensorboard/SAC_2/curves\n\nProcessing stage: SAC_3\nSaved all curves for SAC_3 → experiments/tensorboard/SAC_3/curves\n\nProcessing stage: SAC_4\nSaved all curves for SAC_4 → experiments/tensorboard/SAC_4/curves\n\nFinished generating all plots.\n","output_type":"stream"}],"execution_count":36},{"id":"i0oLMRwtFbhQ","cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"experiments_SAC\", \"zip\", \"experiments\")\n\nprint(\"Zipped to experiments_SAC.zip\")","metadata":{"id":"i0oLMRwtFbhQ","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:07:17.716565Z","iopub.execute_input":"2025-11-28T22:07:17.716859Z","iopub.status.idle":"2025-11-28T22:07:23.299438Z","shell.execute_reply.started":"2025-11-28T22:07:17.716831Z","shell.execute_reply":"2025-11-28T22:07:23.298624Z"}},"outputs":[{"name":"stdout","text":"Zipped to experiments_SAC.zip\n","output_type":"stream"}],"execution_count":37},{"id":"aYGcHpMI02D4","cell_type":"code","source":"","metadata":{"id":"aYGcHpMI02D4","trusted":true},"outputs":[],"execution_count":null}]}