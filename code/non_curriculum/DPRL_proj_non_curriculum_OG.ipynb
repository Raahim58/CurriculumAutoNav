{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15c876cf",
      "metadata": {
        "id": "15c876cf"
      },
      "source": [
        "# NonCurriculum_MetaDrive_SB3_Experiments\n",
        "\n",
        "This notebook contains a full, reproducible experiment pipeline for **non-curriculum** based reinforcement learning for autonomous driving using **MetaDrive** and **Stable Baselines3 (SB3)**. It includes:\n",
        "\n",
        "- Full environment factory and wrappers (including a discrete-action wrapper for DQN).\n",
        "- Exact stage definitions (C0..C3) and matching budgets.\n",
        "- Non-curriculum runner (train each target map separately for the same total sample budget).\n",
        "- Evaluation harness (metrics logging, CSV saving, TensorBoard integration, video recording).\n",
        "- Hyperparameters and experiment folder conventions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8b7750",
      "metadata": {
        "id": "8f8b7750"
      },
      "source": [
        "## 0. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y metadrive metadrive-simulator metadrive-simulator-py3-12 || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KG6c_nUg1EH",
        "outputId": "0582a0bb-12e6-448c-9820-4665b4ec0cd4"
      },
      "id": "9KG6c_nUg1EH",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping metadrive as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator-py3-12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bbe3fc75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe3fc75",
        "outputId": "fee91b90-788c-4832-93ae-35894a7a9b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# !pip install stable-baselines3[extra] gymnasium metadrive numpy pandas matplotlib tensorboard opencv-python\n",
        "# !pip install stable-baselines3[extra] tensorboard opencv-python\n",
        "!pip install -q \"stable-baselines3[extra]\" \"metadrive-simulator-py3-12\" tensorboard opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import metadrive\n",
        "\n",
        "print(\"metadrive.__file__ :\", getattr(metadrive, \"__file__\", None))\n",
        "print(\"metadrive.__path__ :\", getattr(metadrive, \"__path__\", None))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDpNIf_ug8-1",
        "outputId": "23ddf035-aa5c-4470-aa3e-63167627a1eb"
      },
      "id": "MDpNIf_ug8-1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadrive.__file__ : /usr/local/lib/python3.12/dist-packages/metadrive/__init__.py\n",
            "metadrive.__path__ : ['/usr/local/lib/python3.12/dist-packages/metadrive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/metadriverse/metadrive.git\n",
        "# %cd metadrive\n",
        "# !pip install -e .\n"
      ],
      "metadata": {
        "id": "vAvVvccaKQJQ"
      },
      "id": "vAvVvccaKQJQ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m metadrive.pull_asset\n"
      ],
      "metadata": {
        "id": "yD7V6MWtKbUn"
      },
      "id": "yD7V6MWtKbUn",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m metadrive.examples.profile_metadrive\n"
      ],
      "metadata": {
        "id": "-zr38W64KdZd"
      },
      "id": "-zr38W64KdZd",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content\n"
      ],
      "metadata": {
        "id": "Kf1vBH1BKfpt"
      },
      "id": "Kf1vBH1BKfpt",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import metadrive, inspect, os\n",
        "print(\"Using metadrive from:\", metadrive.__file__)\n",
        "print(\"Contents:\", os.listdir(os.path.dirname(metadrive.__file__)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXDForLMfJsl",
        "outputId": "8c392871-7533-434b-b9da-dfe73fb433df"
      },
      "id": "ZXDForLMfJsl",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using metadrive from: /usr/local/lib/python3.12/dist-packages/metadrive/__init__.py\n",
            "Contents: ['pull_asset.py', 'constants.py', 'type.py', '__init__.py', 'render_pipeline', 'component', 'obs', 'policy', 'manager', 'shaders', 'scenario', 'base_class', 'examples', '__pycache__', 'tests', 'version.py', 'utils', 'engine', 'envs', 'third_party']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports"
      ],
      "metadata": {
        "id": "kKG8RN5zKYtb"
      },
      "id": "kKG8RN5zKYtb"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e81fb189",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e81fb189",
        "outputId": "87871c6b-d062-4882-fe3a-d0f86926db23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# SB3 imports\n",
        "from metadrive.envs.metadrive_env import MetaDriveEnv\n",
        "from stable_baselines3 import PPO, SAC, DQN\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# MetaDrive import guard\n",
        "# try:\n",
        "# from metadrive.envs.metadrive_env import MetaDriveEnv\n",
        "# except Exception as e:\n",
        "    # MetaDriveEnv = None\n",
        "    # print('MetaDrive import failed.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")                       # ignore everything\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", module=\"stable_baselines3\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n"
      ],
      "metadata": {
        "id": "PpACB8RcK8m8"
      },
      "id": "PpACB8RcK8m8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cc3da0a2",
      "metadata": {
        "id": "cc3da0a2"
      },
      "source": [
        "## 2. Experiment configuration\n",
        "setup: maps, stages, budgets, hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e386723b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e386723b",
        "outputId": "87e032c1-2689-43f1-fa8e-0ddb6bd48009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total curriculum budget (per algorithm) = 1300000\n"
          ]
        }
      ],
      "source": [
        "### Maps, stages and budgets\n",
        "# STAGES = [\n",
        "#     (\"C0_Straight\", \"Straight\", 0.0, 200_000),\n",
        "#     (\"C1_Curve\", \"Curve\", 0.0, 300_000),\n",
        "#     (\"C2_Roundabout\",\"Roundabout\",0.0,400_000),\n",
        "#     (\"C3_Dynamic\", \"20-block\", 0.3, 400_000),\n",
        "# ]\n",
        "\n",
        "### Maps, curriculum stages and budgets (with reward configs)\n",
        "\n",
        "STAGES = [\n",
        "    # C0: straight road, no traffic – just learn to go forward safely.\n",
        "    {\n",
        "        \"id\": \"C0\",\n",
        "        \"name\": \"C0_Straight\",\n",
        "        \"env_type\": \"general\",\n",
        "        \"map\": \"S\", # Straight\n",
        "        \"traffic\": 0.0,\n",
        "        \"budget\": 200_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,              # scale MetaDrive base reward\n",
        "            \"speed_w\": 0.05,            # weak speed shaping\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -5.0,\n",
        "            \"offroad_penalty\": -3.0,\n",
        "            \"traffic_violation_penalty\": -2.0,\n",
        "            \"success_bonus\": 10.0,\n",
        "            \"step_penalty\": -0.001,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C1: roundabout, no traffic – topology harder, still single-ego.\n",
        "    {\n",
        "        \"id\": \"C1\",\n",
        "        \"name\": \"C1_Roundabout\",\n",
        "        \"env_type\": \"general\",\n",
        "        \"map\": \"O\", # Roundabout\n",
        "        \"traffic\": 0.0,\n",
        "        \"budget\": 300_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.05,\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -6.0,  # slightly harsher for bad manoeuvres\n",
        "            \"offroad_penalty\": -4.0,\n",
        "            \"traffic_violation_penalty\": -3.0,\n",
        "            \"success_bonus\": 10.0,\n",
        "            \"step_penalty\": -0.005,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C2: 20-block PG map with **light traffic** – first exposure to traffic.\n",
        "    {\n",
        "        \"id\": \"C2\",\n",
        "        \"name\": \"C2_LightTraffic\",\n",
        "        \"map\": \"20\", # 20-block\n",
        "        \"traffic\": 0.05,               # light traffic\n",
        "        \"budget\": 400_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.08,            # encourage moving at speed in traffic\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -8.0,\n",
        "            \"offroad_penalty\": -6.0,\n",
        "            \"traffic_violation_penalty\": -4.0,\n",
        "            \"success_bonus\": 12.0,\n",
        "            \"step_penalty\": -0.01,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C3: same PG map with **dense traffic** – “multi-agent-ish” final stage.\n",
        "    # Still single learning ego, but many interacting vehicles (like CuRLA's\n",
        "    # higher-traffic final curriculum stage).\n",
        "    {\n",
        "        \"id\": \"C3\",\n",
        "        \"name\": \"C3_DenseTraffic\",\n",
        "        \"map\": \"20\",\n",
        "        \"traffic\": 0.30,               # dense traffic ≈ mild multi-agent\n",
        "        \"budget\": 400_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.10,\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -10.0, # strong safety pressure\n",
        "            \"offroad_penalty\": -8.0,\n",
        "            \"traffic_violation_penalty\": -5.0,\n",
        "            \"success_bonus\": 15.0,\n",
        "            \"step_penalty\": -0.05,\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "TOTAL_CURRICULUM_BUDGET = sum(s[\"budget\"] for s in STAGES)\n",
        "print(\"Total curriculum budget (per algorithm) =\", TOTAL_CURRICULUM_BUDGET)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder convention\n",
        "EXPERIMENT_ROOT = Path('experiments')\n",
        "EXPERIMENT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "# Seeds and workers\n",
        "SEEDS = [0]\n",
        "N_ENVS = 1 # should be 8 but metadrive issues\n",
        "EVAL_FREQ = 20_000\n",
        "EVAL_EPISODES = 5\n",
        "\n",
        "# Held-out test map\n",
        "HELDOUT_MAP = (\"Fork\", 0.2)"
      ],
      "metadata": {
        "id": "w_ndV2-1wbQZ"
      },
      "id": "w_ndV2-1wbQZ",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "HYPERS = {\n",
        "    'PPO': {\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[128,128]}, # can do 64, 64 as well\n",
        "        'learning_rate':3e-4,\n",
        "        'n_steps':1024, # compute issue, can do 2048 if faster\n",
        "        'batch_size':64,\n",
        "        'n_epochs':10,\n",
        "        'gamma':0.99,\n",
        "        'clip_range':0.2\n",
        "    },\n",
        "    'SAC': {\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[256,256]},\n",
        "        'learning_rate':3e-4,\n",
        "        'batch_size':256,\n",
        "        'buffer_size':100_000,\n",
        "        'gamma':0.99\n",
        "    },\n",
        "    'DQN': { # Atari setup\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[64,64]},\n",
        "        'learning_rate':1e-4,\n",
        "        'buffer_size':100_000, # can do 50,000 if needed\n",
        "        'batch_size':32,\n",
        "        'train_freq':4\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "HjNj-DhhwaJR"
      },
      "id": "HjNj-DhhwaJR",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e299413",
      "metadata": {
        "id": "0e299413"
      },
      "source": [
        "Includes a discrete-action wrapper for DQN (maps discrete indices -> continuous steer/throttle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3e0cb97e",
      "metadata": {
        "id": "3e0cb97e"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class DiscreteActionWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env, mapping):\n",
        "        super().__init__(env)\n",
        "        self.mapping = mapping\n",
        "        self.action_space = spaces.Discrete(len(mapping))\n",
        "\n",
        "    def action(self, action):\n",
        "        return np.array(self.mapping[action], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CurriculumRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Stage-dependent reward shaping:\n",
        "    - Start from MetaDrive's base reward.\n",
        "    - Add speed term.\n",
        "    - Add collision / off-road / traffic-violation penalties.\n",
        "    - Add success bonus.\n",
        "\n",
        "    Uses the per-stage reward config from STAGES.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, reward_cfg):\n",
        "        super().__init__(env)\n",
        "        self.cfg = reward_cfg\n",
        "        self.max_speed = self.cfg.get(\"max_speed_kmh\", 80.0)\n",
        "\n",
        "    def step(self, action):\n",
        "        # MetaDrive uses Gymnasium API: obs, reward, terminated, truncated, info\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # --- speed term ---\n",
        "        # MetaDrive usually exposes speed either as 'speed' or 'velocity'\n",
        "        raw_speed = float(info.get(\"speed\", info.get(\"velocity\", 0.0)))\n",
        "        speed = max(0.0, min(raw_speed, self.max_speed))\n",
        "        speed_term = self.cfg.get(\"speed_w\", 0.0) * (speed / self.max_speed)\n",
        "\n",
        "        # --- start from scaled base reward + speed shaping ---\n",
        "        r = self.cfg.get(\"base_w\", 1.0) * base_r + speed_term\n",
        "\n",
        "        # --- per-step cost (encourage finishing sooner) ---\n",
        "        r += self.cfg.get(\"step_penalty\", 0.0)\n",
        "\n",
        "        # --- collision penalties ---\n",
        "        crashed = (\n",
        "            info.get(\"crash_vehicle\", False)\n",
        "            or info.get(\"crash_object\", False)\n",
        "            or info.get(\"crash_building\", False)\n",
        "        )\n",
        "        if crashed:\n",
        "            r += self.cfg.get(\"collision_penalty\", 0.0)\n",
        "        info[\"collision\"] = bool(crashed)\n",
        "\n",
        "        # --- off-road / traffic-violation penalties ---\n",
        "        offroad = info.get(\"out_of_road\", False)\n",
        "        if offroad:\n",
        "            r += self.cfg.get(\"offroad_penalty\", 0.0)\n",
        "        # generic \"traffic violation\" flag for your metrics callback\n",
        "        traffic_violation = bool(offroad or info.get(\"traffic_light_violation\", False))\n",
        "        if traffic_violation:\n",
        "            r += self.cfg.get(\"traffic_violation_penalty\", 0.0)\n",
        "        info[\"traffic_violation\"] = traffic_violation\n",
        "\n",
        "        # --- success bonus at terminal step ---\n",
        "        success = bool(info.get(\"arrive_dest\", False) or info.get(\"success\", False))\n",
        "        if terminated and success:\n",
        "            r += self.cfg.get(\"success_bonus\", 0.0)\n",
        "        info[\"success\"] = success\n",
        "\n",
        "        # For logging\n",
        "        info[\"avg_speed\"] = speed\n",
        "        info[\"shaped_reward\"] = r\n",
        "\n",
        "        return obs, r, terminated, truncated, info"
      ],
      "metadata": {
        "id": "wLTVGUAADX1r"
      },
      "id": "wLTVGUAADX1r",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "75c2b81f",
      "metadata": {
        "id": "75c2b81f"
      },
      "source": [
        "## 4. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Environment factory"
      ],
      "metadata": {
        "id": "nGXJIcxPxSfh"
      },
      "id": "nGXJIcxPxSfh"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "be7c260f",
      "metadata": {
        "id": "be7c260f"
      },
      "outputs": [],
      "source": [
        "# from functools import partial\n",
        "\n",
        "# def make_metadrive_env(map_name, traffic_density=0.0, use_discrete=False, seed=0, render=False):\n",
        "#     def _init():\n",
        "#         cfg = {\n",
        "#             'map': map_name,\n",
        "#             'traffic_density': traffic_density,\n",
        "#             'use_render': False,\n",
        "#             'start_seed': seed,\n",
        "#             'random_spawn': True,\n",
        "#             'debug': False,\n",
        "#         }\n",
        "#         env = MetaDriveEnv(cfg)\n",
        "#         # Optionally wrap for DQN\n",
        "#         if use_discrete:\n",
        "#             mapping = [(-1.0,0.0),(-1.0,0.3),(0.0,0.5),(1.0,0.3),(1.0,0.0)]\n",
        "#             env = DiscreteActionWrapper(env, mapping)\n",
        "#         return Monitor(env)\n",
        "#     return _init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_vec_env(map_name, traffic_density=0.0, n_envs=8, use_discrete=False, seed=0, parallel=False):\n",
        "#     factories = [make_metadrive_env(map_name, traffic_density, use_discrete, seed+i) for i in range(n_envs)]\n",
        "#     if parallel:\n",
        "#         return SubprocVecEnv(factories)\n",
        "#     else:\n",
        "#         return DummyVecEnv(factories)"
      ],
      "metadata": {
        "id": "Vz_Jdlgfw0a_"
      },
      "id": "Vz_Jdlgfw0a_",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "class MetaDriveGymCompatibilityWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Makes MetaDriveEnv follow the Gymnasium reset() and step() signature.\n",
        "    Removes unsupported arguments like options.\n",
        "    \"\"\"\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            obs, info = self.env.reset(seed=seed)\n",
        "        else:\n",
        "            obs, info = self.env.reset()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        # MetaDrive uses done only; Gymnasium expects (terminated, truncated)\n",
        "        terminated = done\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "vKLjHVxvkKCh"
      },
      "id": "vKLjHVxvkKCh",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import logging\n",
        "\n",
        "def make_metadrive_env(stage, use_discrete=False, seed=0, render=False):\n",
        "    \"\"\"\n",
        "    stage: one of the dicts from STAGES. Uses:\n",
        "        stage[\"map\"], stage[\"traffic\"], stage[\"reward\"]\n",
        "    \"\"\"\n",
        "    map_name = stage[\"map\"]\n",
        "    traffic_density = stage[\"traffic\"]\n",
        "    reward_cfg = stage[\"reward\"]\n",
        "\n",
        "    def _init():\n",
        "        cfg = {\n",
        "            \"map\": map_name,\n",
        "            \"traffic_density\": traffic_density,\n",
        "            \"use_render\": render,\n",
        "            \"start_seed\": seed,\n",
        "            # \"random_spawn\": True,\n",
        "            \"debug\": False,\n",
        "            \"log_level\": logging.ERROR,\n",
        "        }\n",
        "        env = MetaDriveEnv(cfg)\n",
        "\n",
        "        # Fix reset() signature mismatch\n",
        "        env = MetaDriveGymCompatibilityWrapper(env)\n",
        "\n",
        "        # CuRLA-style stage-dependent reward shaping\n",
        "        env = CurriculumRewardWrapper(env, reward_cfg)\n",
        "\n",
        "        # Optional discrete-action wrapper for DQN\n",
        "        if use_discrete:\n",
        "            mapping = [(-1.0, 0.0), (-1.0, 0.3), (0.0, 0.5), (1.0, 0.3), (1.0, 0.0)]\n",
        "            env = DiscreteActionWrapper(env, mapping)\n",
        "\n",
        "        return Monitor(env)\n",
        "\n",
        "    return _init\n"
      ],
      "metadata": {
        "id": "sv070jNaDnyR"
      },
      "id": "sv070jNaDnyR",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_vec_env(stage, n_envs=8, use_discrete=False, seed=0, parallel=False):\n",
        "#     factories = [\n",
        "#         make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "#         for i in range(n_envs)\n",
        "#     ]\n",
        "#     if parallel:\n",
        "#         return SubprocVecEnv(factories)\n",
        "#     else:\n",
        "#         return DummyVecEnv(factories)\n",
        "\n",
        "# from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "\n",
        "# def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n",
        "#     \"\"\"\n",
        "#     Create vectorized MetaDrive envs.\n",
        "#     IMPORTANT: MetaDrive can only have one engine per process.\n",
        "#     So:\n",
        "#       - n_envs == 1  -> use DummyVecEnv (single process, single env)\n",
        "#       - n_envs > 1   -> use SubprocVecEnv (one env per process)\n",
        "#     \"\"\"\n",
        "#     if n_envs == 1:\n",
        "#         return DummyVecEnv([\n",
        "#             make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n",
        "#         ])\n",
        "#     else:\n",
        "#         env_fns = [\n",
        "#             make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "#             for i in range(n_envs)\n",
        "#         ]\n",
        "#         return SubprocVecEnv(env_fns)\n",
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n",
        "    \"\"\"\n",
        "    Create vectorized MetaDrive envs.\n",
        "    FIX: We must ALWAYS use SubprocVecEnv for MetaDrive.\n",
        "    Using DummyVecEnv (single process) prevents creating a second env\n",
        "    (like eval_env) because MetaDrive allows only one engine per process.\n",
        "    \"\"\"\n",
        "    # env_fns = [\n",
        "    #     make_metadrive_env(stage, use_discrete=use_discrete, seed=seed + i)\n",
        "    #     for i in range(n_envs)\n",
        "    # ]\n",
        "\n",
        "    # # FORCE SubprocVecEnv even if n_envs=1\n",
        "    # return SubprocVecEnv(env_fns)\n",
        "\n",
        "    return DummyVecEnv([\n",
        "        make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n",
        "    ])"
      ],
      "metadata": {
        "id": "v9TCh0Q_DpL4"
      },
      "id": "v9TCh0Q_DpL4",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_eval_vec_env(stage, use_discrete=False, seed=0):\n",
        "    \"\"\"\n",
        "    Eval env:\n",
        "    - Always uses SubprocVecEnv (even with 1 worker) so that MetaDriveEnv\n",
        "      is only ever created in subprocesses, not in the main process.\n",
        "    \"\"\"\n",
        "    env_fns = [make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)]\n",
        "    return SubprocVecEnv(env_fns)\n"
      ],
      "metadata": {
        "id": "hJa0pGDE4Z1J"
      },
      "id": "hJa0pGDE4Z1J",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34d56696",
      "metadata": {
        "id": "34d56696"
      },
      "source": [
        "### 4.2 log per-eval metrics (success rate, collisions, speed etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "35b0feb6",
      "metadata": {
        "id": "35b0feb6"
      },
      "outputs": [],
      "source": [
        "# class MetricsCallback(BaseCallback):\n",
        "#     \"\"\"\n",
        "#     Custom callback to compute and append evaluation metrics to CSV on each eval.\n",
        "#     Assumes the eval_env yields episode info dicts with keys 'success','collision','avg_speed','traffic_violation'.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, eval_env, out_csv, eval_episodes=10, verbose=0):\n",
        "#         super().__init__(verbose)\n",
        "#         self.eval_env = eval_env\n",
        "#         self.out_csv = out_csv\n",
        "#         self.eval_episodes = eval_episodes\n",
        "#         self._cols = ['timestamp','total_timesteps','mean_reward','std_reward','success_rate','collision_rate','avg_speed','traffic_violations']\n",
        "#         if not os.path.exists(out_csv):\n",
        "#             pd.DataFrame(columns=self._cols).to_csv(out_csv, index=False)\n",
        "\n",
        "#     def _on_step(self) -> bool:\n",
        "#         return True\n",
        "\n",
        "#     def on_training_end(self):\n",
        "#         pass\n",
        "\n",
        "#     def record_eval(self, model, total_timesteps):\n",
        "#         # run evaluation episodes and compute metrics\n",
        "#         rewards = []\n",
        "#         successes = []\n",
        "#         collisions = []\n",
        "#         speeds = []\n",
        "#         traffic_viol = []\n",
        "#         for ep in range(self.eval_episodes):\n",
        "#             obs, _ = self.eval_env.reset()\n",
        "#             done = False\n",
        "#             ep_r = 0.0\n",
        "#             while not done:\n",
        "#                 action, _ = model.predict(obs, deterministic=True)\n",
        "#                 obs, reward, terminated, truncated, info = self.eval_env.step(action)\n",
        "#                 ep_r += reward\n",
        "#                 done = terminated or truncated\n",
        "#             rewards.append(ep_r)\n",
        "#             info_ep = info if isinstance(info, dict) else {}\n",
        "#             # fallback if env does not provide keys\n",
        "#             successes.append(info_ep.get('success', 1.0 if ep_r>0 else 0.0))\n",
        "#             collisions.append(info_ep.get('collision', 0.0))\n",
        "#             speeds.append(info_ep.get('avg_speed', info_ep.get('speed', 0.0)))\n",
        "#             traffic_viol.append(info_ep.get('traffic_violation', 0.0))\n",
        "\n",
        "#         mean_r = np.mean(rewards)\n",
        "#         std_r = np.std(rewards)\n",
        "#         success_rate = np.mean(successes)\n",
        "#         collision_rate = np.mean(collisions)\n",
        "#         avg_speed = np.mean(speeds)\n",
        "#         tv = np.mean(traffic_viol)\n",
        "#         row = {\n",
        "#             'timestamp': time.time(), 'total_timesteps': total_timesteps, 'mean_reward': mean_r, 'std_reward': std_r,\n",
        "#             'success_rate': success_rate, 'collision_rate': collision_rate, 'avg_speed': avg_speed, 'traffic_violations': tv\n",
        "#         }\n",
        "#         df = pd.read_csv(self.out_csv)\n",
        "#         df = df.append(row, ignore_index=True)\n",
        "#         df.to_csv(self.out_csv, index=False)\n",
        "#         return row"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class MetricsCallback(BaseCallback):\n",
        "#     \"\"\"\n",
        "#     Run a short evaluation every eval_freq steps and log:\n",
        "#       - mean_reward\n",
        "#       - success_rate\n",
        "#       - collision_rate\n",
        "#       - traffic_violation_rate\n",
        "#       - avg_speed\n",
        "#       - avg_episode_length\n",
        "\n",
        "#     Saves to a CSV at csv_path.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, eval_env, csv_path, eval_freq=50_000, eval_episodes=10, verbose=0):\n",
        "#         super().__init__(verbose)\n",
        "#         self.eval_env = eval_env\n",
        "#         self.csv_path = csv_path\n",
        "#         self.eval_freq = eval_freq\n",
        "#         self.eval_episodes = eval_episodes\n",
        "\n",
        "#         # Create dir if needed\n",
        "#         os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "#         # Write header if file doesn't exist\n",
        "#         if not os.path.exists(self.csv_path):\n",
        "#             with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "#                 writer = csv.DictWriter(\n",
        "#                     f,\n",
        "#                     fieldnames=[\n",
        "#                         \"timesteps\",\n",
        "#                         \"mean_reward\",\n",
        "#                         \"success_rate\",\n",
        "#                         \"collision_rate\",\n",
        "#                         \"traffic_violation_rate\",\n",
        "#                         \"avg_speed\",\n",
        "#                         \"avg_episode_length\",\n",
        "#                     ],\n",
        "#                 )\n",
        "#                 writer.writeheader()\n",
        "\n",
        "#     def _on_step(self) -> bool:\n",
        "#         # Only evaluate every eval_freq calls\n",
        "#         if self.n_calls % self.eval_freq != 0:\n",
        "#             return True\n",
        "\n",
        "#         rewards = []\n",
        "#         successes = []\n",
        "#         collisions = []\n",
        "#         traffic_violations = []\n",
        "#         speeds = []\n",
        "#         ep_lengths = []\n",
        "\n",
        "#         for _ in range(self.eval_episodes):\n",
        "#             obs, _ = self.eval_env.reset()\n",
        "#             done = False\n",
        "#             truncated = False\n",
        "\n",
        "#             ep_reward = 0.0\n",
        "#             ep_success = False\n",
        "#             ep_collision = False\n",
        "#             ep_traffic_violation = False\n",
        "#             ep_steps = 0\n",
        "#             ep_speeds = []\n",
        "\n",
        "#             while not (done or truncated):\n",
        "#                 action, _ = self.model.predict(obs, deterministic=True)\n",
        "#                 obs, r, done, truncated, info = self.eval_env.step(action)\n",
        "\n",
        "#                 ep_reward += float(r)\n",
        "#                 ep_steps += 1\n",
        "\n",
        "#                 # flags from CurriculumRewardWrapper / MetaDrive info\n",
        "#                 if info.get(\"success\", False):\n",
        "#                     ep_success = True\n",
        "#                 if info.get(\"collision\", False):\n",
        "#                     ep_collision = True\n",
        "#                 if info.get(\"traffic_violation\", False):\n",
        "#                     ep_traffic_violation = True\n",
        "\n",
        "#                 if \"avg_speed\" in info:\n",
        "#                     ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "#                 elif \"speed\" in info:\n",
        "#                     ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "#             rewards.append(ep_reward)\n",
        "#             successes.append(1.0 if ep_success else 0.0)\n",
        "#             collisions.append(1.0 if ep_collision else 0.0)\n",
        "#             traffic_violations.append(1.0 if ep_traffic_violation else 0.0)\n",
        "#             ep_lengths.append(ep_steps)\n",
        "#             if ep_speeds:\n",
        "#                 speeds.append(sum(ep_speeds) / len(ep_speeds))\n",
        "\n",
        "#         mean_reward = float(sum(rewards) / len(rewards)) if rewards else 0.0\n",
        "#         success_rate = float(sum(successes) / len(successes)) if successes else 0.0\n",
        "#         collision_rate = float(sum(collisions) / len(collisions)) if collisions else 0.0\n",
        "#         traffic_violation_rate = float(sum(traffic_violations) / len(traffic_violations)) if traffic_violations else 0.0\n",
        "#         avg_speed = float(sum(speeds) / len(speeds)) if speeds else 0.0\n",
        "#         avg_episode_length = float(sum(ep_lengths) / len(ep_lengths)) if ep_lengths else 0.0\n",
        "\n",
        "#         row = {\n",
        "#             \"timesteps\": int(self.num_timesteps),\n",
        "#             \"mean_reward\": mean_reward,\n",
        "#             \"success_rate\": success_rate,\n",
        "#             \"collision_rate\": collision_rate,\n",
        "#             \"traffic_violation_rate\": traffic_violation_rate,\n",
        "#             \"avg_speed\": avg_speed,\n",
        "#             \"avg_episode_length\": avg_episode_length,\n",
        "#         }\n",
        "\n",
        "#         with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "#             writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "#             writer.writerow(row)\n",
        "\n",
        "#         if self.verbose > 0:\n",
        "#             print(f\"[Metrics] t={self.num_timesteps}  succ={success_rate:.2f}  \"\n",
        "#                   f\"coll={collision_rate:.2f}  len={avg_episode_length:.1f}\")\n",
        "\n",
        "#         return True"
      ],
      "metadata": {
        "id": "nqsUhrkZHHzw"
      },
      "id": "nqsUhrkZHHzw",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Run a short evaluation every eval_freq steps and log:\n",
        "      - mean_reward\n",
        "      - success_rate\n",
        "      - collision_rate\n",
        "      - traffic_violation_rate\n",
        "      - avg_speed\n",
        "      - avg_episode_length\n",
        "\n",
        "    Saves to a CSV at csv_path.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eval_env, csv_path, eval_freq=50_000, eval_episodes=10, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.eval_env = eval_env\n",
        "        self.csv_path = csv_path\n",
        "        self.eval_freq = eval_freq\n",
        "        self.eval_episodes = eval_episodes\n",
        "\n",
        "        # Create dir if needed\n",
        "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "        # Write header if file doesn't exist\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "                writer = csv.DictWriter(\n",
        "                    f,\n",
        "                    fieldnames=[\n",
        "                        \"timesteps\",\n",
        "                        \"mean_reward\",\n",
        "                        \"success_rate\",\n",
        "                        \"collision_rate\",\n",
        "                        \"traffic_violation_rate\",\n",
        "                        \"avg_speed\",\n",
        "                        \"avg_episode_length\",\n",
        "                    ],\n",
        "                )\n",
        "                writer.writeheader()\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Only evaluate every eval_freq calls\n",
        "        if self.n_calls % self.eval_freq != 0:\n",
        "            return True\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_successes = []\n",
        "        episode_collisions = []\n",
        "        episode_traffic_violations = []\n",
        "        episode_speeds = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for _ in range(self.eval_episodes):\n",
        "            # DummyVecEnv.reset() -> obs (no info, vec-batched)\n",
        "            obs = self.eval_env.reset()\n",
        "            done = False\n",
        "\n",
        "            ep_reward = 0.0\n",
        "            ep_success = False\n",
        "            ep_collision = False\n",
        "            ep_traffic_violation = False\n",
        "            ep_steps = 0\n",
        "            ep_speeds = []\n",
        "\n",
        "            while not done:\n",
        "                # obs shape: (n_envs, obs_dim); n_envs = 1 here\n",
        "                action, _ = self.model.predict(obs, deterministic=True)\n",
        "                # DummyVecEnv.step() -> obs, rewards, dones, infos\n",
        "                obs, rewards, dones, infos = self.eval_env.step(action)\n",
        "\n",
        "                # unwrap vec env outputs for single env\n",
        "                if isinstance(rewards, (np.ndarray, list, tuple)):\n",
        "                    r = float(rewards[0])\n",
        "                else:\n",
        "                    r = float(rewards)\n",
        "\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    d = bool(dones[0])\n",
        "                else:\n",
        "                    d = bool(dones)\n",
        "\n",
        "                if isinstance(infos, (list, tuple)) and len(infos) > 0:\n",
        "                    info = infos[0]\n",
        "                else:\n",
        "                    info = infos\n",
        "\n",
        "                ep_reward += r\n",
        "                ep_steps += 1\n",
        "                done = d\n",
        "\n",
        "                # if isinstance(info, dict):\n",
        "                #     if info.get(\"success\", False):\n",
        "                #         ep_success = True\n",
        "                #     if info.get(\"collision\", False):\n",
        "                #         ep_collision = True\n",
        "                #     if info.get(\"traffic_violation\", False):\n",
        "                #         ep_traffic_violation = True\n",
        "\n",
        "                #     if \"avg_speed\" in info:\n",
        "                #         ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "                #     elif \"speed\" in info:\n",
        "                #         ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "                # flags from MetaDrive info / our wrapper\n",
        "                if isinstance(info, dict):\n",
        "                    # success: either our wrapper's \"success\" OR MetaDrive's arrive_dest\n",
        "                    if info.get(\"success\", False) or info.get(\"arrive_dest\", False):\n",
        "                        ep_success = True\n",
        "\n",
        "                    # collision: either our wrapper's \"collision\" OR any crash/out-of-road\n",
        "                    if (\n",
        "                        info.get(\"collision\", False)\n",
        "                        or info.get(\"crash_vehicle\", False)\n",
        "                        or info.get(\"crash_object\", False)\n",
        "                        or info.get(\"crash_building\", False)\n",
        "                        or info.get(\"out_of_road\", False)\n",
        "                    ):\n",
        "                        ep_collision = True\n",
        "\n",
        "                    # traffic violation if we ever log it; otherwise this will stay 0\n",
        "                    if info.get(\"traffic_violation\", False):\n",
        "                        ep_traffic_violation = True\n",
        "\n",
        "                    # speed logging\n",
        "                    if \"avg_speed\" in info:\n",
        "                        ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "                    elif \"speed\" in info:\n",
        "                        ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "\n",
        "            episode_rewards.append(ep_reward)\n",
        "            episode_successes.append(1.0 if ep_success else 0.0)\n",
        "            episode_collisions.append(1.0 if ep_collision else 0.0)\n",
        "            episode_traffic_violations.append(1.0 if ep_traffic_violation else 0.0)\n",
        "            episode_lengths.append(ep_steps)\n",
        "            if ep_speeds:\n",
        "                episode_speeds.append(sum(ep_speeds) / len(ep_speeds))\n",
        "\n",
        "        mean_reward = float(sum(episode_rewards) / len(episode_rewards)) if episode_rewards else 0.0\n",
        "        success_rate = float(sum(episode_successes) / len(episode_successes)) if episode_successes else 0.0\n",
        "        collision_rate = float(sum(episode_collisions) / len(episode_collisions)) if episode_collisions else 0.0\n",
        "        traffic_violation_rate = float(sum(episode_traffic_violations) / len(episode_traffic_violations)) if episode_traffic_violations else 0.0\n",
        "        avg_speed = float(sum(episode_speeds) / len(episode_speeds)) if episode_speeds else 0.0\n",
        "        avg_episode_length = float(sum(episode_lengths) / len(episode_lengths)) if episode_lengths else 0.0\n",
        "\n",
        "        row = {\n",
        "            \"timesteps\": int(self.num_timesteps),\n",
        "            \"mean_reward\": mean_reward,\n",
        "            \"success_rate\": success_rate,\n",
        "            \"collision_rate\": collision_rate,\n",
        "            \"traffic_violation_rate\": traffic_violation_rate,\n",
        "            \"avg_speed\": avg_speed,\n",
        "            \"avg_episode_length\": avg_episode_length,\n",
        "        }\n",
        "\n",
        "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "            writer.writerow(row)\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print(f\"[Metrics] t={self.num_timesteps}  succ={success_rate:.2f}  \"\n",
        "                  f\"coll={collision_rate:.2f}  len={avg_episode_length:.1f}\")\n",
        "\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "zGjpX3W1FZsu"
      },
      "id": "zGjpX3W1FZsu",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class PrettyEvalCallback(EvalCallback):\n",
        "    \"\"\"\n",
        "    Clean pretty printing for eval:\n",
        "      - One separator before the first eval\n",
        "      - No spam between evals\n",
        "      - One separator at the end of training on this stage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._started = False    # whether we have printed the first separator\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Check if it's time to evaluate\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            # On first eval, print top separator\n",
        "            if not self._started:\n",
        "                print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "                self._started = True\n",
        "\n",
        "        return super()._on_step()\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        # After training for this stage: print final separator\n",
        "        if self._started:\n",
        "            print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "        return super()._on_training_end()\n"
      ],
      "metadata": {
        "id": "-eFLK2DKKDJR"
      },
      "id": "-eFLK2DKKDJR",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_run_config(out_dir, algo, stage, seed):\n",
        "    \"\"\"\n",
        "    Save basic run config (algo, stage, hyperparams, seed) to config.json\n",
        "    so you can reproduce / inspect later.\n",
        "    \"\"\"\n",
        "    cfg = {\n",
        "        \"algo\": algo,\n",
        "        \"seed\": seed,\n",
        "        \"stage\": {\n",
        "            \"id\": stage[\"id\"],\n",
        "            \"name\": stage[\"name\"],\n",
        "            \"map\": stage[\"map\"],\n",
        "            \"traffic\": stage[\"traffic\"],\n",
        "            \"budget\": stage[\"budget\"],\n",
        "            \"reward\": stage[\"reward\"],\n",
        "        },\n",
        "        \"hyperparams\": HYPERS[algo],\n",
        "        \"heldout_map\": {\"map\": HELDOUT_MAP[0], \"traffic\": HELDOUT_MAP[1]},\n",
        "    }\n",
        "    with open(out_dir / \"config.json\", \"w\") as f:\n",
        "        json.dump(cfg, f, indent=2)"
      ],
      "metadata": {
        "id": "rPOxRfNNHKbp"
      },
      "id": "rPOxRfNNHKbp",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0303bc4f",
      "metadata": {
        "id": "0303bc4f"
      },
      "source": [
        "### 4.3 Training functions\n",
        "\n",
        "These functions create models, attach callbacks, and run training. Each saves checkpoint, best-model and CSV metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def make_model(algo, env, hyperparams):\n",
        "    common_kwargs = dict(\n",
        "        verbose=0,  # make SB3 quiet in console\n",
        "        tensorboard_log=str(EXPERIMENT_ROOT / 'tensorboard'),\n",
        "        policy_kwargs=hyperparams['policy_kwargs'],\n",
        "        learning_rate=hyperparams['learning_rate'],\n",
        "    )\n",
        "\n",
        "    if algo == 'PPO':\n",
        "        model = PPO(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            n_steps=hyperparams['n_steps'],\n",
        "            batch_size=hyperparams['batch_size'],\n",
        "            n_epochs=hyperparams['n_epochs'],\n",
        "            gamma=hyperparams['gamma'],\n",
        "            device=\"cpu\",\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if algo == 'SAC':\n",
        "        model = SAC(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            batch_size=hyperparams.get('batch_size', 256),\n",
        "            buffer_size=hyperparams.get('buffer_size', 100_000),\n",
        "            gamma=hyperparams.get('gamma', 0.99),\n",
        "            device=DEVICE,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if algo == 'DQN':\n",
        "        model = DQN(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            buffer_size=hyperparams.get('buffer_size', 50_000),\n",
        "            batch_size=hyperparams.get('batch_size', 32),\n",
        "            train_freq=hyperparams.get('train_freq', 4),\n",
        "            device=DEVICE,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    raise ValueError('Unknown algo')\n"
      ],
      "metadata": {
        "id": "yLzoiNexEQ6n"
      },
      "id": "yLzoiNexEQ6n",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "9a2ecb4e",
      "metadata": {
        "id": "9a2ecb4e"
      },
      "outputs": [],
      "source": [
        "# def make_model(algo, env, hyperparams):\n",
        "#     if algo == 'PPO':\n",
        "#         model = PPO(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
        "#                     policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
        "#                     n_steps=hyperparams['n_steps'], batch_size=hyperparams['batch_size'], n_epochs=hyperparams['n_epochs'], gamma=hyperparams['gamma'])\n",
        "#         return model\n",
        "#     if algo == 'SAC':\n",
        "#         model = SAC(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
        "#                     policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
        "#                     batch_size=hyperparams.get('batch_size',256), buffer_size=hyperparams.get('buffer_size',100000), gamma=hyperparams.get('gamma',0.99))\n",
        "#         return model\n",
        "#     if algo == 'DQN':\n",
        "#         model = DQN(hyperparams['policy'], env, verbose=1, tensorboard_log=str(EXPERIMENT_ROOT/'tensorboard'),\n",
        "#                     policy_kwargs=hyperparams['policy_kwargs'], learning_rate=hyperparams['learning_rate'],\n",
        "#                     buffer_size=hyperparams.get('buffer_size',50000), batch_size=hyperparams.get('batch_size',32), train_freq=hyperparams.get('train_freq',4))\n",
        "#         return model\n",
        "#     raise ValueError('Unknown algo')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_noncurriculum(algo, map_name, traffic, total_timesteps, seed, n_envs=N_ENVS):\n",
        "#     out_dir = EXPERIMENT_ROOT/f\"{algo}/noncurriculum/seed_{seed}/{map_name}\"\n",
        "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
        "#     print('Training non-curriculum:', algo, map_name, 'seed', seed)\n",
        "\n",
        "#     use_discrete = (algo=='DQN')\n",
        "#     env = make_vec_env(map_name, traffic_density=traffic, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
        "#     eval_env = make_vec_env(map_name, traffic_density=traffic, n_envs=1, use_discrete=use_discrete, seed=seed+100)\n",
        "#     eval_env = eval_env.env_fns[0]() if hasattr(eval_env, 'env_fns') else eval_env\n",
        "\n",
        "#     model = make_model(algo, env, HYPERS[algo])\n",
        "\n",
        "#     # callbacks\n",
        "#     eval_cb = EvalCallback(eval_env, best_model_save_path=str(out_dir/'best_model'), log_path=str(out_dir/'eval_logs'), eval_freq=EVAL_FREQ, n_eval_episodes=EVAL_EPISODES, deterministic=True)\n",
        "#     ckpt_cb = CheckpointCallback(save_freq=EVAL_FREQ, save_path=str(out_dir/'checkpoints'), name_prefix='ckpt')\n",
        "#     metrics_csv = out_dir/'metrics.csv'\n",
        "#     metrics_cb = MetricsCallback(eval_env, str(metrics_csv), eval_episodes=EVAL_EPISODES)\n",
        "\n",
        "#     # train\n",
        "#     model.learn(total_timesteps=total_timesteps, callback=[eval_cb, ckpt_cb])\n",
        "#     model.save(str(out_dir/'model.zip'))\n",
        "\n",
        "#     # held-out eval\n",
        "#     held_map, held_traffic = HELDOUT_MAP\n",
        "#     held_env = make_vec_env(held_map, traffic_density=held_traffic, n_envs=1, use_discrete=use_discrete, seed=seed+500)\n",
        "#     held_env = held_env.env_fns[0]() if hasattr(held_env, 'env_fns') else held_env\n",
        "#     mean_reward, std_reward = evaluate_policy(model, held_env, n_eval_episodes=100)\n",
        "#     pd.DataFrame([{'mean_reward':mean_reward,'std_reward':std_reward}]).to_csv(out_dir/'heldout_metrics.csv', index=False)\n",
        "\n",
        "#     print('Non-curriculum training complete and saved to', out_dir)\n",
        "#     return out_dir"
      ],
      "metadata": {
        "id": "cjxzJfrlxh1g"
      },
      "id": "cjxzJfrlxh1g",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=N_ENVS):\n",
        "#     \"\"\"\n",
        "#     Non-curriculum baseline:\n",
        "#     - Train *from scratch* on a single stage for TOTAL_CURRICULUM_BUDGET steps.\n",
        "#     - Stage carries map, traffic and reward config.\n",
        "#     \"\"\"\n",
        "#     map_name = stage[\"map\"]\n",
        "#     out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n",
        "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
        "#     print(\"Training non-curriculum:\", algo, stage[\"name\"], \"seed\", seed)\n",
        "\n",
        "#     use_discrete = (algo == \"DQN\")\n",
        "\n",
        "#     # training envs\n",
        "#     env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
        "\n",
        "#     # eval env (single copy)\n",
        "#     eval_env_vec = make_vec_env(stage, n_envs=1, use_discrete=use_discrete, seed=seed + 100)\n",
        "#     eval_env = eval_env_vec.env_fns[0]() if hasattr(eval_env_vec, \"env_fns\") else eval_env_vec\n",
        "\n",
        "#     model = make_model(algo, env, HYPERS[algo])\n",
        "\n",
        "#     # callbacks\n",
        "#     eval_cb = EvalCallback(\n",
        "#         eval_env,\n",
        "#         best_model_save_path=str(out_dir / \"best_model\"),\n",
        "#         log_path=str(out_dir / \"eval\"),\n",
        "#         eval_freq=EVAL_FREQ,\n",
        "#         n_eval_episodes=EVAL_EPISODES,\n",
        "#         deterministic=True,\n",
        "#     )\n",
        "#     ckpt_cb = CheckpointCallback(\n",
        "#         save_freq=EVAL_FREQ,\n",
        "#         save_path=str(out_dir / \"checkpoints\"),\n",
        "#         name_prefix=\"ckpt\",\n",
        "#     )\n",
        "#     metrics_csv = out_dir / \"metrics.csv\"\n",
        "#     metrics_cb = MetricsCallback(eval_env, str(metrics_csv), eval_episodes=EVAL_EPISODES)\n",
        "\n",
        "#     # train\n",
        "#     model.learn(total_timesteps=total_timesteps, callback=[eval_cb, ckpt_cb])\n",
        "#     model.save(str(out_dir / \"model.zip\"))\n",
        "\n",
        "#     # held-out evaluation (reuse same reward shaping config for fairness)\n",
        "#     held_stage = {\n",
        "#         \"id\": \"HELDOUT\",\n",
        "#         \"name\": f\"HELDOUT_{HELDOUT_MAP[0]}\",\n",
        "#         \"map\": HELDOUT_MAP[0],\n",
        "#         \"traffic\": HELDOUT_MAP[1],\n",
        "#         \"budget\": 0,\n",
        "#         \"reward\": stage[\"reward\"],  # same shaping as training stage\n",
        "#     }\n",
        "#     held_env_vec = make_vec_env(held_stage, n_envs=1, use_discrete=use_discrete, seed=seed + 500)\n",
        "#     held_env = held_env_vec.env_fns[0]() if hasattr(held_env_vec, \"env_fns\") else held_env_vec\n",
        "#     mean_reward, std_reward = evaluate_policy(model, held_env, n_eval_episodes=100)\n",
        "#     pd.DataFrame([{\"mean_reward\": mean_reward, \"std_reward\": std_reward}]).to_csv(\n",
        "#         out_dir / \"heldout_metrics.csv\", index=False\n",
        "#     )\n",
        "\n",
        "#     print(\"Non-curriculum training complete and saved to\", out_dir)\n",
        "#     return out_dir\n"
      ],
      "metadata": {
        "id": "rMVuVpGBDuw3"
      },
      "id": "rMVuVpGBDuw3",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=N_ENVS):\n",
        "    \"\"\"\n",
        "    Non-curriculum baseline:\n",
        "    - Train from scratch on a SINGLE (hard) stage for 'total_timesteps'.\n",
        "    - Evaluate on held-out map at the end.\n",
        "    \"\"\"\n",
        "\n",
        "    out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Training NON-CURRICULUM:\", algo, stage[\"name\"], \"seed\", seed)\n",
        "\n",
        "    use_discrete = (algo == \"DQN\")\n",
        "\n",
        "    # training envs\n",
        "    env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
        "\n",
        "    # eval env (for callbacks)\n",
        "    eval_env = env\n",
        "    # eval_env = make_eval_vec_env(stage, use_discrete=use_discrete, seed=seed + 100)\n",
        "    # eval_env = eval_env_vec.env_fns[0]() if hasattr(eval_env_vec, \"env_fns\") else eval_env_vec\n",
        "\n",
        "    model = make_model(algo, env, HYPERS[algo])\n",
        "\n",
        "    # callbacks\n",
        "    eval_cb = PrettyEvalCallback(\n",
        "        eval_env,\n",
        "        best_model_save_path=str(out_dir / \"best_model\"),\n",
        "        log_path=str(out_dir / \"eval\"),\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        n_eval_episodes=EVAL_EPISODES,\n",
        "        deterministic=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    ckpt_cb = CheckpointCallback(\n",
        "        save_freq=EVAL_FREQ,\n",
        "        save_path=str(out_dir / \"checkpoints\"),\n",
        "        name_prefix=\"ckpt\",\n",
        "    )\n",
        "    metrics_csv = out_dir / \"metrics.csv\"\n",
        "    metrics_cb = MetricsCallback(\n",
        "        eval_env,\n",
        "        csv_path=str(metrics_csv),\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        eval_episodes=EVAL_EPISODES,\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    # save run config\n",
        "    save_run_config(out_dir, algo, stage, seed)\n",
        "\n",
        "    # train\n",
        "    model.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=[eval_cb, ckpt_cb, metrics_cb],\n",
        "    )\n",
        "    model.save(str(out_dir / \"model.zip\"))\n",
        "\n",
        "    # held-out evaluation\n",
        "    held_stage = {\n",
        "        \"id\": \"HELDOUT\",\n",
        "        \"name\": f\"HELDOUT_{HELDOUT_MAP[0]}\",\n",
        "        \"map\": HELDOUT_MAP[0],\n",
        "        \"traffic\": HELDOUT_MAP[1],\n",
        "        \"budget\": 0,\n",
        "        \"reward\": stage[\"reward\"],  # use same shaping as training stage\n",
        "    }\n",
        "    # held_env = make_eval_vec_env(held_stage, use_discrete=use_discrete, seed=seed + 500)\n",
        "    # held_env = held_env_vec.env_fns[0]() if hasattr(held_env_vec, \"env_fns\") else held_env_vec\n",
        "\n",
        "    # mean_reward, std_reward = evaluate_policy(model, held_env, n_eval_episodes=100)\n",
        "    # pd.DataFrame([{\"mean_reward\": mean_reward, \"std_reward\": std_reward}]).to_csv(\n",
        "        # out_dir / \"heldout_metrics.csv\", index=False\n",
        "    # )\n",
        "\n",
        "    env.close()\n",
        "    eval_env.close()\n",
        "    # held_env.close()\n",
        "\n",
        "    print(\"Non-curriculum training complete and saved to\", out_dir)\n",
        "    return out_dir\n"
      ],
      "metadata": {
        "id": "QDgBvXRGHPwz"
      },
      "id": "QDgBvXRGHPwz",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83889cd2",
      "metadata": {
        "id": "83889cd2"
      },
      "source": [
        "### 4.4. Visualization (plot metrics, learning curves, and display videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "57e24184",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57e24184",
        "outputId": "45e2ca49-5231-482e-ce56-61224290e29a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot helper ready\n"
          ]
        }
      ],
      "source": [
        "def plot_metrics(csv_path, title=None):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print('CSV not found:', csv_path); return\n",
        "    df = pd.read_csv(csv_path)\n",
        "    fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
        "    axs = axs.flatten()\n",
        "    axs[0].plot(df['total_timesteps'], df['mean_reward'], marker='o'); axs[0].set_title('Mean reward')\n",
        "    axs[1].plot(df['total_timesteps'], df['success_rate'], marker='o'); axs[1].set_title('Success rate')\n",
        "    axs[2].plot(df['total_timesteps'], df['collision_rate'], marker='o'); axs[2].set_title('Collision rate')\n",
        "    axs[3].plot(df['total_timesteps'], df['avg_speed'], marker='o'); axs[3].set_title('Avg speed')\n",
        "    if title: fig.suptitle(title)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "print('Plot helper ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644f2875",
      "metadata": {
        "id": "644f2875"
      },
      "source": [
        "## 5. Toy pilot run\n",
        "\n",
        "Small pilot run to validate pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "aef7a3ca",
      "metadata": {
        "id": "aef7a3ca"
      },
      "outputs": [],
      "source": [
        "# run one tiny non-curriculum PPO for 2000 steps on Straight map\n",
        "\n",
        "# out = train_noncurriculum('PPO', STAGES[0], total_timesteps=2000, seed=0, n_envs=1)\n",
        "# print('Pilot saved at:', out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c6e5f6",
      "metadata": {
        "id": "95c6e5f6"
      },
      "source": [
        "## 6. Full experiment\n",
        "\n",
        "For each algorithm, for each seed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algos = [\"PPO\", \"SAC\", \"DQN\"]\n",
        "\n",
        "# Define hardest stage as the last curriculum stage (C3)\n",
        "HARDEST_STAGE = STAGES[-1]\n",
        "\n",
        "for algo in algos:\n",
        "    for seed in SEEDS:\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"ALGO={algo}  SEED={seed}\")\n",
        "\n",
        "        for stage in STAGES:\n",
        "            train_noncurriculum(\n",
        "                algo=algo,\n",
        "                stage=stage,\n",
        "                total_timesteps=stage[\"budget\"],  # use this stage's budget\n",
        "                seed=seed,\n",
        "                n_envs=N_ENVS,\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "xPZ5O3NLHg94",
        "outputId": "6c5adf9e-82f7-49b0-e70b-a26c0a7524f5"
      },
      "id": "xPZ5O3NLHg94",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ALGO=PPO  SEED=0\n",
            "Training NON-CURRICULUM: PPO C0_Straight seed 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Can not call this API after engine initialization!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2906221617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTAGES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             train_noncurriculum(\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1804171146.py\u001b[0m in \u001b[0;36mtrain_noncurriculum\u001b[0;34m(algo, stage, total_timesteps, seed, n_envs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# training envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vec_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_discrete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_discrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# eval env (for callbacks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1149387457.py\u001b[0m in \u001b[0;36mmake_vec_env\u001b[0;34m(stage, n_envs, use_discrete, seed)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# return SubprocVecEnv(env_fns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     return DummyVecEnv([\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mmake_metadrive_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_discrete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_discrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     ])\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_patch_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/tmp/ipython-input-3751961072.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;34m\"log_level\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         }\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetaDriveEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Fix reset() signature mismatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/metadrive/envs/metadrive_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_config_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munchangeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMetaDriveEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# scenario setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/metadrive/envs/base_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0minitialize_global_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# agent check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/metadrive/engine/engine_utils.py\u001b[0m in \u001b[0;36minitialize_global_config\u001b[0;34m(global_config)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mYou\u001b[0m \u001b[0mcan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcourse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreset\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mengine\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mlaunching\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mengine_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Can not call this API after engine initialization!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mEngineCore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Can not call this API after engine initialization!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde3ba34",
      "metadata": {
        "id": "bde3ba34"
      },
      "outputs": [],
      "source": [
        "# algos = ['PPO','SAC','DQN']\n",
        "# for algo in algos:\n",
        "#     # non-curriculum: for each target map train for the total curriculum budget (to match sample budget)\n",
        "#     for seed in SEEDS:\n",
        "#         for (_, map_name, traffic, _) in STAGES:\n",
        "#             train_noncurriculum(algo, map_name, traffic, total_timesteps=TOTAL_CURRICULUM_BUDGET, seed=seed, n_envs=N_ENVS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i0oLMRwtFbhQ"
      },
      "id": "i0oLMRwtFbhQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}