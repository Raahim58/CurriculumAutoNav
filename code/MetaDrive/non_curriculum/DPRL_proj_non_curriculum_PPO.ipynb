{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15c876cf",
      "metadata": {
        "id": "15c876cf"
      },
      "source": [
        "# NonCurriculum_MetaDrive_SB3_Experiments\n",
        "\n",
        "This notebook contains a full, reproducible experiment pipeline for **non-curriculum** based reinforcement learning for autonomous driving using **MetaDrive** and **Stable Baselines3 (SB3)**. It includes:\n",
        "\n",
        "- Full environment factory and wrappers (including a discrete-action wrapper for DQN).\n",
        "- Exact stage definitions (C0..C3) and matching budgets.\n",
        "- Non-curriculum runner (train each target map separately for the same total sample budget).\n",
        "- Evaluation harness (metrics logging, CSV saving, TensorBoard integration, video recording).\n",
        "- Hyperparameters and experiment folder conventions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8b7750",
      "metadata": {
        "id": "8f8b7750"
      },
      "source": [
        "## 0. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9KG6c_nUg1EH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KG6c_nUg1EH",
        "outputId": "0f185698-778b-4070-f0e6-f3818f276b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping metadrive as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator-py3-12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y metadrive metadrive-simulator metadrive-simulator-py3-12 || true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe3fc75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe3fc75",
        "outputId": "053db2a5-dd25-4f69-c364-8cc790c6e4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"stable-baselines3[extra]\" \"metadrive-simulator-py3-12\" tensorboard opencv-python # for colab\n",
        "\n",
        "## for kaggle:\n",
        "# !pip install \"numpy<2.0\" \"protobuf==3.20.3\" \"metadrive-simulator\" \"stable-baselines3[extra]\" tensorboard opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "MDpNIf_ug8-1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDpNIf_ug8-1",
        "outputId": "74a2510b-900a-4254-fbef-bedb28247c94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "metadrive.__file__ : /usr/local/lib/python3.12/dist-packages/metadrive/__init__.py\n",
            "metadrive.__path__ : ['/usr/local/lib/python3.12/dist-packages/metadrive']\n"
          ]
        }
      ],
      "source": [
        "import metadrive\n",
        "\n",
        "print(\"metadrive.__file__ :\", getattr(metadrive, \"__file__\", None))\n",
        "print(\"metadrive.__path__ :\", getattr(metadrive, \"__path__\", None))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kKG8RN5zKYtb",
      "metadata": {
        "id": "kKG8RN5zKYtb"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81fb189",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e81fb189",
        "outputId": "976babec-dae4-4a05-fd19-6a86cd1041ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import logging\n",
        "\n",
        "# SB3 imports\n",
        "from metadrive.envs.metadrive_env import MetaDriveEnv\n",
        "from metadrive.envs.varying_dynamics_env import VaryingDynamicsEnv\n",
        "from stable_baselines3 import PPO, SAC, DQN\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "PpACB8RcK8m8",
      "metadata": {
        "id": "PpACB8RcK8m8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")                       # ignore everything\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", module=\"stable_baselines3\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc3da0a2",
      "metadata": {
        "id": "cc3da0a2"
      },
      "source": [
        "## 2. Experiment configuration\n",
        "setup: maps, stages, budgets, hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e386723b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e386723b",
        "outputId": "5855e707-8f12-47fd-f1a4-7e9683e8d556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total curriculum budget (per algorithm) = 650000\n"
          ]
        }
      ],
      "source": [
        "STAGES = [\n",
        "    # C0: straight road, no traffic – just learn to go forward safely.\n",
        "    {\n",
        "        \"id\": \"C0\",\n",
        "        \"name\": \"C0_Straight\",\n",
        "        \"env_type\": \"general\",\n",
        "        \"map\": \"S\", # Straight\n",
        "        \"traffic\": 0.0,\n",
        "        \"budget\": 100_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,              # scale MetaDrive base reward\n",
        "            \"speed_w\": 0.05,            # weak speed shaping\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -5.0,\n",
        "            \"offroad_penalty\": -3.0,\n",
        "            \"traffic_violation_penalty\": -2.0,\n",
        "            \"success_bonus\": 10.0,\n",
        "            \"step_penalty\": -0.001,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C1: roundabout, no traffic – topology harder, still single-ego.\n",
        "    {\n",
        "        \"id\": \"C1\",\n",
        "        \"name\": \"C1_Roundabout\",\n",
        "        \"env_type\": \"general\",\n",
        "        \"map\": \"O\", # Roundabout\n",
        "        \"traffic\": 0.0,\n",
        "        \"budget\": 150_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.05,\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -6.0,  # slightly harsher for bad manoeuvres\n",
        "            \"offroad_penalty\": -4.0,\n",
        "            \"traffic_violation_penalty\": -3.0,\n",
        "            \"success_bonus\": 10.0,\n",
        "            \"step_penalty\": -0.005,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C2: 20-block PG map with **light traffic** – first exposure to traffic.\n",
        "    {\n",
        "        \"id\": \"C2\",\n",
        "        \"name\": \"C2_LightTraffic\",\n",
        "        \"map\": 10, # 20-block\n",
        "        \"traffic\": 0.05,               # light traffic\n",
        "        \"budget\": 200_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.08,            # encourage moving at speed in traffic\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -8.0,\n",
        "            \"offroad_penalty\": -6.0,\n",
        "            \"traffic_violation_penalty\": -4.0,\n",
        "            \"success_bonus\": 12.0,\n",
        "            \"step_penalty\": -0.01,\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # C3: same PG map with **dense traffic** – “multi-agent-ish” final stage.\n",
        "    # Still single learning ego, but many interacting vehicles (like CuRLA's\n",
        "    # higher-traffic final curriculum stage).\n",
        "    {\n",
        "        \"id\": \"C3\",\n",
        "        \"name\": \"C3_DenseTraffic\",\n",
        "        \"map\": 20,\n",
        "        \"traffic\": 0.30,               # dense traffic ≈ mild multi-agent\n",
        "        \"budget\": 200_000,\n",
        "        \"reward\": {\n",
        "            \"base_w\": 1.0,\n",
        "            \"speed_w\": 0.10,\n",
        "            \"max_speed_kmh\": 80.0,\n",
        "            \"collision_penalty\": -10.0, # strong safety pressure\n",
        "            \"offroad_penalty\": -8.0,\n",
        "            \"traffic_violation_penalty\": -5.0,\n",
        "            \"success_bonus\": 15.0,\n",
        "            \"step_penalty\": -0.05,\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "TOTAL_CURRICULUM_BUDGET = sum(s[\"budget\"] for s in STAGES)\n",
        "print(\"Total curriculum budget (per algorithm) =\", TOTAL_CURRICULUM_BUDGET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "B-HaQNeltf-R",
      "metadata": {
        "id": "B-HaQNeltf-R"
      },
      "outputs": [],
      "source": [
        "# Held-out 1: fixed 6-block map SCrRXO with medium traffic\n",
        "HELDOUT_SCENARIO_STAGE = {\n",
        "    \"id\": \"HELDOUT_SCENARIO\",\n",
        "    \"name\": \"HELDOUT_SCrRXO_MedTraffic\",\n",
        "    \"map\": \"SCrRXO\", # Straight -> Circular -> in-ramp -> out-ramp -> intersection -> roundabout\n",
        "    \"traffic\": 0.30, # medium-ish traffic\n",
        "    \"budget\": 0, # no training budget, eval only\n",
        "    \"reward\": STAGES[-1][\"reward\"],  # reuse hardest-stage shaping for comparability\n",
        "}\n",
        "\n",
        "# Held-out 2: VaryingDynamics environment (dynamics robustness test)\n",
        "VARYING_DYNAMICS_CONFIG = dict(\n",
        "    num_scenarios=100,\n",
        "    map=5,                  # small PG map\n",
        "    log_level=logging.ERROR,\n",
        "    # random_dynamics uses default ranges from docs; that's enough to show robustness\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w_ndV2-1wbQZ",
      "metadata": {
        "id": "w_ndV2-1wbQZ"
      },
      "outputs": [],
      "source": [
        "# Folder convention\n",
        "EXPERIMENT_ROOT = Path('experiments')\n",
        "EXPERIMENT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "# Seeds and workers\n",
        "SEEDS = [0]\n",
        "N_ENVS = 1 # should be 8 but metadrive issues\n",
        "EVAL_FREQ = 10_000\n",
        "EVAL_EPISODES = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "HjNj-DhhwaJR",
      "metadata": {
        "id": "HjNj-DhhwaJR"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "HYPERS = {\n",
        "    'PPO': {\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[64,64]}, # can do 64, 64 as well\n",
        "        'learning_rate':3e-4,\n",
        "        'n_steps':1024, # compute issue, can do 2048 if faster\n",
        "        'batch_size':64,\n",
        "        'n_epochs':10,\n",
        "        'gamma':0.99,\n",
        "        'clip_range':0.2\n",
        "    },\n",
        "    'SAC': {\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[256,256]},\n",
        "        'learning_rate':3e-4,\n",
        "        'batch_size':256,\n",
        "        'buffer_size':100_000,\n",
        "        'gamma':0.99\n",
        "    },\n",
        "    'DQN': { # Atari setup\n",
        "        'policy':'MlpPolicy',\n",
        "        'policy_kwargs':{'net_arch':[64,64]},\n",
        "        'learning_rate':1e-4,\n",
        "        'buffer_size':100_000, # can do 50,000 if needed\n",
        "        'batch_size':32,\n",
        "        'train_freq':4\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e299413",
      "metadata": {
        "id": "0e299413"
      },
      "source": [
        "Includes a discrete-action wrapper for DQN (maps discrete indices -> continuous steer/throttle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3e0cb97e",
      "metadata": {
        "id": "3e0cb97e"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class DiscreteActionWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env, mapping):\n",
        "        super().__init__(env)\n",
        "        self.mapping = mapping\n",
        "        self.action_space = spaces.Discrete(len(mapping))\n",
        "\n",
        "    def action(self, action):\n",
        "        return np.array(self.mapping[action], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "wLTVGUAADX1r",
      "metadata": {
        "id": "wLTVGUAADX1r"
      },
      "outputs": [],
      "source": [
        "class CurriculumRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Stage-dependent reward shaping:\n",
        "    - Start from MetaDrive's base reward.\n",
        "    - Add speed term.\n",
        "    - Add collision / off-road / traffic-violation penalties.\n",
        "    - Add success bonus.\n",
        "\n",
        "    Uses the per-stage reward config from STAGES.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, reward_cfg):\n",
        "        super().__init__(env)\n",
        "        self.cfg = reward_cfg\n",
        "        self.max_speed = self.cfg.get(\"max_speed_kmh\", 80.0)\n",
        "\n",
        "    def step(self, action):\n",
        "        # MetaDrive uses Gymnasium API: obs, reward, terminated, truncated, info\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # --- speed term ---\n",
        "        # MetaDrive usually exposes speed either as 'speed' or 'velocity'\n",
        "        raw_speed = float(info.get(\"speed\", info.get(\"velocity\", 0.0)))\n",
        "        speed = max(0.0, min(raw_speed, self.max_speed))\n",
        "        speed_term = self.cfg.get(\"speed_w\", 0.0) * (speed / self.max_speed)\n",
        "\n",
        "        # --- start from scaled base reward + speed shaping ---\n",
        "        r = self.cfg.get(\"base_w\", 1.0) * base_r + speed_term\n",
        "\n",
        "        # --- per-step cost (encourage finishing sooner) ---\n",
        "        r += self.cfg.get(\"step_penalty\", 0.0)\n",
        "\n",
        "        # --- collision penalties ---\n",
        "        crashed = (\n",
        "            info.get(\"crash_vehicle\", False)\n",
        "            or info.get(\"crash_object\", False)\n",
        "            or info.get(\"crash_building\", False)\n",
        "        )\n",
        "        if crashed:\n",
        "            r += self.cfg.get(\"collision_penalty\", 0.0)\n",
        "        info[\"collision\"] = bool(crashed)\n",
        "\n",
        "        # --- off-road / traffic-violation penalties ---\n",
        "        offroad = info.get(\"out_of_road\", False)\n",
        "        if offroad:\n",
        "            r += self.cfg.get(\"offroad_penalty\", 0.0)\n",
        "\n",
        "        # generic \"traffic violation\" flag for your metrics callback\n",
        "        traffic_violation = bool(offroad or info.get(\"traffic_light_violation\", False))\n",
        "        if traffic_violation:\n",
        "            r += self.cfg.get(\"traffic_violation_penalty\", 0.0)\n",
        "        info[\"traffic_violation\"] = traffic_violation\n",
        "\n",
        "        # --- success bonus at terminal step ---\n",
        "        success = bool(info.get(\"arrive_dest\", False) or info.get(\"success\", False))\n",
        "        if terminated and success:\n",
        "            r += self.cfg.get(\"success_bonus\", 0.0)\n",
        "        info[\"success\"] = success\n",
        "\n",
        "        # For logging\n",
        "        info[\"avg_speed\"] = speed\n",
        "        info[\"shaped_reward\"] = r\n",
        "\n",
        "        return obs, r, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c2b81f",
      "metadata": {
        "id": "75c2b81f"
      },
      "source": [
        "## 4. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nGXJIcxPxSfh",
      "metadata": {
        "id": "nGXJIcxPxSfh"
      },
      "source": [
        "### 4.1 Environment factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "vKLjHVxvkKCh",
      "metadata": {
        "id": "vKLjHVxvkKCh"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "class MetaDriveGymCompatibilityWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Makes MetaDriveEnv follow the Gymnasium reset() and step() signature.\n",
        "    Removes unsupported arguments like options.\n",
        "    \"\"\"\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            obs, info = self.env.reset(seed=seed)\n",
        "        else:\n",
        "            obs, info = self.env.reset()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        # MetaDrive uses done only; Gymnasium expects (terminated, truncated)\n",
        "        terminated = done\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "sv070jNaDnyR",
      "metadata": {
        "id": "sv070jNaDnyR"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import logging\n",
        "\n",
        "def make_metadrive_env(stage, use_discrete=False, seed=0, render=False):\n",
        "    \"\"\"\n",
        "    stage: one of the dicts from STAGES. Uses:\n",
        "        stage[\"map\"], stage[\"traffic\"], stage[\"reward\"]\n",
        "    \"\"\"\n",
        "    map_name = stage[\"map\"]\n",
        "    traffic_density = stage[\"traffic\"]\n",
        "    reward_cfg = stage[\"reward\"]\n",
        "\n",
        "    def _init():\n",
        "        cfg = {\n",
        "            \"map\": map_name,\n",
        "            \"traffic_density\": traffic_density,\n",
        "            \"use_render\": render,\n",
        "            \"start_seed\": seed,\n",
        "            # \"random_spawn\": True,\n",
        "            \"debug\": False,\n",
        "            \"log_level\": logging.ERROR,\n",
        "            # cap episode length so eval can't run forever\n",
        "            \"horizon\": 1000, # max steps per episode\n",
        "            \"truncate_as_terminate\": True,   # treat horizon as done\n",
        "        }\n",
        "        env = MetaDriveEnv(cfg)\n",
        "\n",
        "        # Fix reset() signature mismatch\n",
        "        env = MetaDriveGymCompatibilityWrapper(env)\n",
        "\n",
        "        # CuRLA-style stage-dependent reward shaping\n",
        "        env = CurriculumRewardWrapper(env, reward_cfg)\n",
        "\n",
        "        # discrete-action wrapper for DQN: [steering, throttle]\n",
        "        if use_discrete:\n",
        "            mapping = [\n",
        "              (-1.0, 0.0), # hard left, no throttle\n",
        "              (-1.0, 0.3), # left + some accel\n",
        "              (0.0, 0.5), # go straight, accel\n",
        "              (1.0, 0.3), # right + some accel\n",
        "              (1.0, 0.0), # hard right, no throttle\n",
        "            ]\n",
        "            env = DiscreteActionWrapper(env, mapping)\n",
        "\n",
        "        return Monitor(env)\n",
        "\n",
        "    return _init\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v9TCh0Q_DpL4",
      "metadata": {
        "id": "v9TCh0Q_DpL4"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n",
        "    \"\"\"\n",
        "    Create vectorized MetaDrive envs.\n",
        "    FIX: We must ALWAYS use SubprocVecEnv for MetaDrive.\n",
        "    Using DummyVecEnv (single process) prevents creating a second env\n",
        "    (like eval_env) because MetaDrive allows only one engine per process.\n",
        "    \"\"\"\n",
        "    return DummyVecEnv([\n",
        "        make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dTkPoInXfp4F",
      "metadata": {
        "id": "dTkPoInXfp4F"
      },
      "source": [
        "PPO/SAC use continous control (`Box`) from MetaDrive while DQN uses a manually defined discrete control space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "Q-OkesJEflDV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-OkesJEflDV",
        "outputId": "ed252f27-5b7a-4eaf-be65-34037aa715d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPO/SAC action space: Box(-1.0, 1.0, (2,), float32)\n",
            "obs space: Box(-0.0, 1.0, (259,), float32)\n"
          ]
        }
      ],
      "source": [
        "vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=False, seed=0)\n",
        "base_env = vec.envs[0]    # DummyVecEnv\n",
        "print(\"PPO/SAC action space:\", base_env.action_space)\n",
        "print(\"obs space:\", base_env.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "TA49LfETfmvF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA49LfETfmvF",
        "outputId": "d1e64acf-4bd4-43df-f385-06207c8c3eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPO/SAC action space: Discrete(5)\n",
            "obs space: Box(-0.0, 1.0, (259,), float32)\n"
          ]
        }
      ],
      "source": [
        "vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=True, seed=0)\n",
        "base_env = vec.envs[0]    # DummyVecEnv\n",
        "print(\"PPO/SAC action space:\", base_env.action_space)\n",
        "print(\"obs space:\", base_env.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "hJa0pGDE4Z1J",
      "metadata": {
        "id": "hJa0pGDE4Z1J"
      },
      "outputs": [],
      "source": [
        "def make_eval_vec_env(stage, use_discrete=False, seed=0):\n",
        "    \"\"\"\n",
        "    Eval env:\n",
        "    - Always uses SubprocVecEnv (even with 1 worker) so that MetaDriveEnv\n",
        "      is only ever created in subprocesses, not in the main process.\n",
        "    \"\"\"\n",
        "    env_fns = [make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)]\n",
        "    return SubprocVecEnv(env_fns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d56696",
      "metadata": {
        "id": "34d56696"
      },
      "source": [
        "### 4.2 log per-eval metrics (success rate, collisions, speed etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zGjpX3W1FZsu",
      "metadata": {
        "id": "zGjpX3W1FZsu"
      },
      "outputs": [],
      "source": [
        "class MetricsCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Run a short evaluation every eval_freq steps and log:\n",
        "      - mean_reward\n",
        "      - success_rate\n",
        "      - collision_rate\n",
        "      - traffic_violation_rate\n",
        "      - avg_speed\n",
        "      - avg_episode_length\n",
        "\n",
        "    Saves to a CSV at csv_path.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eval_env, csv_path, eval_freq=50_000, eval_episodes=10, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.eval_env = eval_env\n",
        "        self.csv_path = csv_path\n",
        "        self.eval_freq = eval_freq\n",
        "        self.eval_episodes = eval_episodes\n",
        "\n",
        "        # Create dir if needed\n",
        "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "        # Write header if file doesn't exist\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "                writer = csv.DictWriter(\n",
        "                    f,\n",
        "                    fieldnames=[\n",
        "                        \"timesteps\",\n",
        "                        \"mean_reward\",\n",
        "                        \"success_rate\",\n",
        "                        \"collision_rate\",\n",
        "                        \"traffic_violation_rate\",\n",
        "                        \"avg_speed\",\n",
        "                        \"avg_episode_length\",\n",
        "                    ],\n",
        "                )\n",
        "                writer.writeheader()\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Only evaluate every eval_freq calls\n",
        "        if self.n_calls % self.eval_freq != 0:\n",
        "            return True\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_successes = []\n",
        "        episode_collisions = []\n",
        "        episode_traffic_violations = []\n",
        "        episode_speeds = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for _ in range(self.eval_episodes):\n",
        "            # DummyVecEnv.reset() -> obs (no info, vec-batched)\n",
        "            obs = self.eval_env.reset()\n",
        "            done = False\n",
        "\n",
        "            ep_reward = 0.0\n",
        "            ep_success = False\n",
        "            ep_collision = False\n",
        "            ep_traffic_violation = False\n",
        "            ep_steps = 0\n",
        "            ep_speeds = []\n",
        "\n",
        "            while not done:\n",
        "                # obs shape: (n_envs, obs_dim); n_envs = 1 here\n",
        "                action, _ = self.model.predict(obs, deterministic=True)\n",
        "                # DummyVecEnv.step() -> obs, rewards, dones, infos\n",
        "                obs, rewards, dones, infos = self.eval_env.step(action)\n",
        "\n",
        "                # unwrap vec env outputs for single env\n",
        "                if isinstance(rewards, (np.ndarray, list, tuple)):\n",
        "                    r = float(rewards[0])\n",
        "                else:\n",
        "                    r = float(rewards)\n",
        "\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    d = bool(dones[0])\n",
        "                else:\n",
        "                    d = bool(dones)\n",
        "\n",
        "                if isinstance(infos, (list, tuple)) and len(infos) > 0:\n",
        "                    info = infos[0]\n",
        "                else:\n",
        "                    info = infos\n",
        "\n",
        "                ep_reward += r\n",
        "                ep_steps += 1\n",
        "                done = d\n",
        "\n",
        "                # flags from MetaDrive info / our wrapper\n",
        "                if isinstance(info, dict):\n",
        "                    # success: either our wrapper's \"success\" OR MetaDrive's arrive_dest\n",
        "                    if info.get(\"success\", False) or info.get(\"arrive_dest\", False):\n",
        "                        ep_success = True\n",
        "\n",
        "                    # collision: either our wrapper's \"collision\" OR any crash/out-of-road\n",
        "                    if (\n",
        "                        info.get(\"collision\", False)\n",
        "                        or info.get(\"crash_vehicle\", False)\n",
        "                        or info.get(\"crash_object\", False)\n",
        "                        or info.get(\"crash_building\", False)\n",
        "                        or info.get(\"out_of_road\", False)\n",
        "                    ):\n",
        "                        ep_collision = True\n",
        "\n",
        "                    # traffic violation if we ever log it; otherwise this will stay 0\n",
        "                    if info.get(\"traffic_violation\", False):\n",
        "                        ep_traffic_violation = True\n",
        "\n",
        "                    # speed logging\n",
        "                    if \"avg_speed\" in info:\n",
        "                        ep_speeds.append(float(info[\"avg_speed\"]))\n",
        "                    elif \"speed\" in info:\n",
        "                        ep_speeds.append(float(info[\"speed\"]))\n",
        "\n",
        "\n",
        "            episode_rewards.append(ep_reward)\n",
        "            episode_successes.append(1.0 if ep_success else 0.0)\n",
        "            episode_collisions.append(1.0 if ep_collision else 0.0)\n",
        "            episode_traffic_violations.append(1.0 if ep_traffic_violation else 0.0)\n",
        "            episode_lengths.append(ep_steps)\n",
        "            if ep_speeds:\n",
        "                episode_speeds.append(sum(ep_speeds) / len(ep_speeds))\n",
        "\n",
        "        mean_reward = float(sum(episode_rewards) / len(episode_rewards)) if episode_rewards else 0.0\n",
        "        success_rate = float(sum(episode_successes) / len(episode_successes)) if episode_successes else 0.0\n",
        "        collision_rate = float(sum(episode_collisions) / len(episode_collisions)) if episode_collisions else 0.0\n",
        "        traffic_violation_rate = float(sum(episode_traffic_violations) / len(episode_traffic_violations)) if episode_traffic_violations else 0.0\n",
        "        avg_speed = float(sum(episode_speeds) / len(episode_speeds)) if episode_speeds else 0.0\n",
        "        avg_episode_length = float(sum(episode_lengths) / len(episode_lengths)) if episode_lengths else 0.0\n",
        "\n",
        "        row = {\n",
        "            \"timesteps\": int(self.num_timesteps),\n",
        "            \"mean_reward\": mean_reward,\n",
        "            \"success_rate\": success_rate,\n",
        "            \"collision_rate\": collision_rate,\n",
        "            \"traffic_violation_rate\": traffic_violation_rate,\n",
        "            \"avg_speed\": avg_speed,\n",
        "            \"avg_episode_length\": avg_episode_length,\n",
        "        }\n",
        "\n",
        "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "            writer.writerow(row)\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print(f\"[Metrics] t={self.num_timesteps}  succ={success_rate:.2f}  \"\n",
        "                  f\"coll={collision_rate:.2f}  len={avg_episode_length:.1f}\")\n",
        "\n",
        "        return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "-eFLK2DKKDJR",
      "metadata": {
        "id": "-eFLK2DKKDJR"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class PrettyEvalCallback(EvalCallback):\n",
        "    \"\"\"\n",
        "    Clean pretty printing for eval:\n",
        "      - One separator before the first eval\n",
        "      - No spam between evals\n",
        "      - One separator at the end of training on this stage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._started = False    # whether we have printed the first separator\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Check if it's time to evaluate\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            # On first eval, print top separator\n",
        "            if not self._started:\n",
        "                print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "                self._started = True\n",
        "\n",
        "        return super()._on_step()\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        # After training for this stage: print final separator\n",
        "        if self._started:\n",
        "            print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "        return super()._on_training_end()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "rPOxRfNNHKbp",
      "metadata": {
        "id": "rPOxRfNNHKbp"
      },
      "outputs": [],
      "source": [
        "def save_run_config(out_dir, algo, stage, seed):\n",
        "    \"\"\"\n",
        "    Save basic run config (algo, stage, hyperparams, seed) to config.json\n",
        "    so you can reproduce / inspect later.\n",
        "    \"\"\"\n",
        "    cfg = {\n",
        "        \"algo\": algo,\n",
        "        \"seed\": seed,\n",
        "        \"stage\": {\n",
        "            \"id\": stage[\"id\"],\n",
        "            \"name\": stage[\"name\"],\n",
        "            \"map\": stage[\"map\"],\n",
        "            \"traffic\": stage[\"traffic\"],\n",
        "            \"budget\": stage[\"budget\"],\n",
        "            \"reward\": stage[\"reward\"],\n",
        "        },\n",
        "        \"hyperparams\": HYPERS[algo],\n",
        "        \"heldout_map\": {\"map\": HELDOUT_SCENARIO_STAGE['map'], \"traffic\": HELDOUT_SCENARIO_STAGE['traffic'], \"reward\": HELDOUT_SCENARIO_STAGE['reward']},\n",
        "    }\n",
        "    with open(out_dir / \"config.json\", \"w\") as f:\n",
        "        json.dump(cfg, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "soBC1415it33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soBC1415it33",
        "outputId": "79f26125-276a-4639-f9c8-ba5dd320dcb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'HELDOUT_SCENARIO', 'name': 'HELDOUT_SCrRXO_MedTraffic', 'map': 'SCrRXO', 'traffic': 0.3, 'budget': 0, 'reward': {'base_w': 1.0, 'speed_w': 0.1, 'max_speed_kmh': 80.0, 'collision_penalty': -10.0, 'offroad_penalty': -8.0, 'traffic_violation_penalty': -5.0, 'success_bonus': 15.0, 'step_penalty': -0.05}}\n"
          ]
        }
      ],
      "source": [
        "print(HELDOUT_SCENARIO_STAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0303bc4f",
      "metadata": {
        "id": "0303bc4f"
      },
      "source": [
        "### 4.3 Training functions\n",
        "\n",
        "These functions create models, attach callbacks, and run training. Each saves checkpoint, best-model and CSV metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "yLzoiNexEQ6n",
      "metadata": {
        "id": "yLzoiNexEQ6n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def make_model(algo, env, hyperparams):\n",
        "    common_kwargs = dict(\n",
        "        verbose=0,  # make SB3 quiet in console\n",
        "        tensorboard_log=str(EXPERIMENT_ROOT / 'tensorboard'),\n",
        "        policy_kwargs=hyperparams['policy_kwargs'],\n",
        "        learning_rate=hyperparams['learning_rate'],\n",
        "    )\n",
        "\n",
        "    if algo == 'PPO':\n",
        "        model = PPO(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            n_steps=hyperparams['n_steps'],\n",
        "            batch_size=hyperparams['batch_size'],\n",
        "            n_epochs=hyperparams['n_epochs'],\n",
        "            gamma=hyperparams['gamma'],\n",
        "            device=\"cpu\",\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if algo == 'SAC':\n",
        "        model = SAC(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            batch_size=hyperparams.get('batch_size', 256),\n",
        "            buffer_size=hyperparams.get('buffer_size', 100_000),\n",
        "            gamma=hyperparams.get('gamma', 0.99),\n",
        "            device=DEVICE,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if algo == 'DQN':\n",
        "        model = DQN(\n",
        "            hyperparams['policy'],\n",
        "            env,\n",
        "            buffer_size=hyperparams.get('buffer_size', 50_000),\n",
        "            batch_size=hyperparams.get('batch_size', 32),\n",
        "            train_freq=hyperparams.get('train_freq', 4),\n",
        "            device=DEVICE,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    raise ValueError('Unknown algo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "z5yA8SnOtpCg",
      "metadata": {
        "id": "z5yA8SnOtpCg"
      },
      "outputs": [],
      "source": [
        "def evaluate_env_episodes(model, env, n_episodes=20, deterministic=True):\n",
        "    \"\"\"\n",
        "    Run n_episodes on a *single* (non-vec) env.\n",
        "    Returns (mean_reward, std_reward, mean_ep_len).\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        truncated = False\n",
        "        ep_r = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        while not (done or truncated):\n",
        "            action, _ = model.predict(obs, deterministic=deterministic)\n",
        "            obs, r, done, truncated, info = env.step(action)\n",
        "            ep_r += float(r)\n",
        "            steps += 1\n",
        "\n",
        "        rewards.append(ep_r)\n",
        "        lengths.append(steps)\n",
        "\n",
        "    rewards = np.array(rewards, dtype=np.float32)\n",
        "    lengths = np.array(lengths, dtype=np.float32)\n",
        "    return float(rewards.mean()), float(rewards.std()), float(lengths.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "eCc8tf5CtVuA",
      "metadata": {
        "id": "eCc8tf5CtVuA"
      },
      "outputs": [],
      "source": [
        "def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=1):\n",
        "    \"\"\"\n",
        "    Non-curriculum baseline:\n",
        "    - Train from scratch on a SINGLE stage for 'total_timesteps'.\n",
        "    - Eval during training on SAME env via PrettyEvalCallback (already set up).\n",
        "    - After training, eval on two held-out envs:\n",
        "        1) SCrRXO + medium traffic (MetaDriveEnv)\n",
        "        2) VaryingDynamicsEnv (randomized dynamics)\n",
        "    \"\"\"\n",
        "\n",
        "    out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Training NON-CURRICULUM: {algo} {stage['name']} seed {seed}\\n\")\n",
        "\n",
        "    use_discrete = (algo == \"DQN\")\n",
        "\n",
        "    # ---- training env (single vec env) ----\n",
        "    env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
        "\n",
        "    model = make_model(algo, env, HYPERS[algo])\n",
        "\n",
        "    # in-training eval (same env)\n",
        "    eval_cb = PrettyEvalCallback(\n",
        "        env,\n",
        "        best_model_save_path=str(out_dir / \"best_model\"),\n",
        "        log_path=str(out_dir / \"eval\"),\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        n_eval_episodes=EVAL_EPISODES,\n",
        "        deterministic=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    ckpt_cb = CheckpointCallback(\n",
        "        save_freq=EVAL_FREQ,\n",
        "        save_path=str(out_dir / \"checkpoints\"),\n",
        "        name_prefix=\"ckpt\",\n",
        "    )\n",
        "\n",
        "    metrics_csv = out_dir / \"metrics.csv\"\n",
        "    metrics_cb = MetricsCallback(\n",
        "        eval_env=env.envs[0],       # since you’re using n_envs=1 (DummyVecEnv)\n",
        "        csv_path=str(metrics_csv),\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        eval_episodes=EVAL_EPISODES,\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    save_run_config(out_dir, algo, stage, seed)\n",
        "\n",
        "    # ---- train ----\n",
        "    model.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=[eval_cb, ckpt_cb, metrics_cb],\n",
        "    )\n",
        "    model.save(str(out_dir / \"model.zip\"))\n",
        "\n",
        "    print(f\"\\nNon-curriculum training complete and saved to {out_dir}\")\n",
        "\n",
        "    # Close training vec env to avoid engine conflicts before new envs\n",
        "    env.close()\n",
        "\n",
        "    # =====================================================================\n",
        "    # HELD-OUT 1: SCrRXO + medium traffic\n",
        "    # =====================================================================\n",
        "    print(\"\\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\")\n",
        "\n",
        "    # build a *single* env instance using same wrappers\n",
        "    held1_make = make_metadrive_env(\n",
        "        HELDOUT_SCENARIO_STAGE,\n",
        "        use_discrete=use_discrete,\n",
        "        seed=seed + 1000,\n",
        "        render=False,\n",
        "    )\n",
        "    held1_env = held1_make()\n",
        "\n",
        "    h1_mean, h1_std = evaluate_policy( #, h1_len if manual function\n",
        "        model,\n",
        "        held1_env,\n",
        "        n_eval_episodes=20,\n",
        "        deterministic=True,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"HELD-OUT 1 (SCrRXO): mean_reward={h1_mean:.2f} ± {h1_std:.2f}, \"\n",
        "        # f\"mean_ep_len={h1_len:.1f}\"\n",
        "    )\n",
        "\n",
        "    pd.DataFrame(\n",
        "        [{\n",
        "            \"algo\": algo,\n",
        "            \"train_stage\": stage[\"name\"],\n",
        "            \"heldout_name\": HELDOUT_SCENARIO_STAGE[\"name\"],\n",
        "            \"mean_reward\": h1_mean,\n",
        "            \"std_reward\": h1_std,\n",
        "            # \"mean_ep_len\": h1_len,\n",
        "        }]\n",
        "    ).to_csv(out_dir / \"heldout_scrrxo_metrics.csv\", index=False)\n",
        "\n",
        "    held1_env.close()\n",
        "\n",
        "    # =====================================================================\n",
        "    # HELD-OUT 2: VaryingDynamicsEnv\n",
        "    # =====================================================================\n",
        "    print(\"\\n[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\")\n",
        "\n",
        "    # build varying dynamics env\n",
        "    vd_env = VaryingDynamicsEnv(VARYING_DYNAMICS_CONFIG)\n",
        "    vd_env = MetaDriveGymCompatibilityWrapper(vd_env)\n",
        "\n",
        "    if use_discrete:\n",
        "        # same discrete mapping you used for training DQN\n",
        "        discrete_mapping = [(-1.0, 0.0), (-1.0, 0.3), (0.0, 0.5), (1.0, 0.3), (1.0, 0.0)]\n",
        "        vd_env = DiscreteActionWrapper(vd_env, discrete_mapping)\n",
        "\n",
        "    h2_mean, h2_std, h2_len = evaluate_env_episodes(\n",
        "        model,\n",
        "        vd_env,\n",
        "        n_episodes=20,\n",
        "        deterministic=True,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"HELD-OUT 2 (VaryingDynamics): mean_reward={h2_mean:.2f} ± {h2_std:.2f}, \"\n",
        "        f\"mean_ep_len={h2_len:.1f}\"\n",
        "    )\n",
        "\n",
        "    pd.DataFrame(\n",
        "        [{\n",
        "            \"algo\": algo,\n",
        "            \"train_stage\": stage[\"name\"],\n",
        "            \"heldout_name\": \"VaryingDynamicsEnv\",\n",
        "            \"mean_reward\": h2_mean,\n",
        "            \"std_reward\": h2_std,\n",
        "            \"mean_ep_len\": h2_len,\n",
        "        }]\n",
        "    ).to_csv(out_dir / \"heldout_varying_metrics.csv\", index=False)\n",
        "\n",
        "    vd_env.close()\n",
        "\n",
        "    return out_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83889cd2",
      "metadata": {
        "id": "83889cd2"
      },
      "source": [
        "### 4.4. Visualization (plot metrics, learning curves, and display videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "57e24184",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57e24184",
        "outputId": "7a7e8de7-ff8e-4f5a-d195-2a27a2f86a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plot helper ready\n"
          ]
        }
      ],
      "source": [
        "def plot_metrics(csv_path, title=None):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print('CSV not found:', csv_path); return\n",
        "    df = pd.read_csv(csv_path)\n",
        "    fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
        "    axs = axs.flatten()\n",
        "    axs[0].plot(df['total_timesteps'], df['mean_reward'], marker='o'); axs[0].set_title('Mean reward')\n",
        "    axs[1].plot(df['total_timesteps'], df['success_rate'], marker='o'); axs[1].set_title('Success rate')\n",
        "    axs[2].plot(df['total_timesteps'], df['collision_rate'], marker='o'); axs[2].set_title('Collision rate')\n",
        "    axs[3].plot(df['total_timesteps'], df['avg_speed'], marker='o'); axs[3].set_title('Avg speed')\n",
        "    if title: fig.suptitle(title)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "print('Plot helper ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c6e5f6",
      "metadata": {
        "id": "95c6e5f6"
      },
      "source": [
        "## 5. Full experiment\n",
        "\n",
        "For each algorithm, for each seed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xPZ5O3NLHg94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPZ5O3NLHg94",
        "outputId": "ce4cb02d-e87b-492f-95dd-2aab4bc358a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ALGO=PPO  SEED=0\n",
            "Training NON-CURRICULUM: PPO C0_Straight seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=0.87 +/- 0.00\n",
            "Episode length: 37.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=4.94 +/- 0.00\n",
            "Episode length: 35.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30000, episode_reward=16.71 +/- 0.00\n",
            "Episode length: 45.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=-1.86 +/- 0.00\n",
            "Episode length: 25.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=135.65 +/- 0.00\n",
            "Episode length: 92.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=60000, episode_reward=136.96 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=70000, episode_reward=135.86 +/- 0.00\n",
            "Episode length: 92.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=136.51 +/- 0.00\n",
            "Episode length: 94.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=135.48 +/- 0.00\n",
            "Episode length: 92.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=136.09 +/- 0.00\n",
            "Episode length: 93.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/PPO/noncurriculum/seed_0/C0_Straight\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=42.84 ± 0.01, \n",
            "\n",
            "[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\n",
            "HELD-OUT 2 (VaryingDynamics): mean_reward=117.35 ± 57.31, mean_ep_len=999.2\n",
            "Training NON-CURRICULUM: PPO C1_Roundabout seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=-2.51 +/- 0.00\n",
            "Episode length: 38.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-5.34 +/- 0.00\n",
            "Episode length: 33.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=25.84 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=4.38 +/- 0.00\n",
            "Episode length: 44.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=2.81 +/- 0.00\n",
            "Episode length: 41.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=46.77 +/- 0.00\n",
            "Episode length: 67.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=70000, episode_reward=11.98 +/- 0.00\n",
            "Episode length: 43.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=-6.68 +/- 0.00\n",
            "Episode length: 26.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=49.25 +/- 0.00\n",
            "Episode length: 71.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=100000, episode_reward=48.68 +/- 0.00\n",
            "Episode length: 70.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=49.04 +/- 0.00\n",
            "Episode length: 69.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=48.80 +/- 0.00\n",
            "Episode length: 67.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=0.99 +/- 0.00\n",
            "Episode length: 34.00 +/- 0.00\n",
            "Eval num_timesteps=140000, episode_reward=48.56 +/- 0.00\n",
            "Episode length: 68.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=49.63 +/- 0.00\n",
            "Episode length: 68.00 +/- 0.00\n",
            "New best mean reward!\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/PPO/noncurriculum/seed_0/C1_Roundabout\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=41.53 ± 0.00, \n",
            "\n",
            "[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\n",
            "HELD-OUT 2 (VaryingDynamics): mean_reward=134.03 ± 65.04, mean_ep_len=114.9\n",
            "Training NON-CURRICULUM: PPO C2_LightTraffic seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=50.29 +/- 0.00\n",
            "Episode length: 78.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=19.64 +/- 0.00\n",
            "Episode length: 51.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=7.59 +/- 0.00\n",
            "Episode length: 41.00 +/- 0.00\n",
            "Eval num_timesteps=40000, episode_reward=-4.89 +/- 0.00\n",
            "Episode length: 28.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=11.07 +/- 0.00\n",
            "Episode length: 45.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=10.07 +/- 0.00\n",
            "Episode length: 44.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=279.97 +/- 0.00\n",
            "Episode length: 170.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=80000, episode_reward=257.13 +/- 0.00\n",
            "Episode length: 160.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=148.47 +/- 0.00\n",
            "Episode length: 112.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=254.54 +/- 0.00\n",
            "Episode length: 163.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=272.20 +/- 0.00\n",
            "Episode length: 172.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=244.24 +/- 0.00\n",
            "Episode length: 159.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=255.54 +/- 0.74\n",
            "Episode length: 159.20 +/- 0.40\n",
            "Eval num_timesteps=140000, episode_reward=-13.09 +/- 0.00\n",
            "Episode length: 28.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=62.09 +/- 0.00\n",
            "Episode length: 76.00 +/- 0.00\n",
            "Eval num_timesteps=160000, episode_reward=268.74 +/- 11.58\n",
            "Episode length: 166.60 +/- 5.39\n",
            "Eval num_timesteps=170000, episode_reward=255.14 +/- 0.00\n",
            "Episode length: 159.00 +/- 0.00\n",
            "Eval num_timesteps=180000, episode_reward=251.03 +/- 0.00\n",
            "Episode length: 162.00 +/- 0.00\n",
            "Eval num_timesteps=190000, episode_reward=-11.50 +/- 0.00\n",
            "Episode length: 26.00 +/- 0.00\n",
            "Eval num_timesteps=200000, episode_reward=255.51 +/- 0.00\n",
            "Episode length: 162.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/PPO/noncurriculum/seed_0/C2_LightTraffic\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=43.49 ± 0.00, \n",
            "\n",
            "[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\n",
            "HELD-OUT 2 (VaryingDynamics): mean_reward=96.46 ± 52.44, mean_ep_len=143.8\n",
            "Training NON-CURRICULUM: PPO C3_DenseTraffic seed 0\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Eval num_timesteps=10000, episode_reward=-6.27 +/- 0.00\n",
            "Episode length: 38.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-0.42 +/- 0.00\n",
            "Episode length: 39.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30000, episode_reward=41.03 +/- 0.00\n",
            "Episode length: 65.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=-2.66 +/- 0.00\n",
            "Episode length: 37.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=58.21 +/- 0.01\n",
            "Episode length: 77.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=60000, episode_reward=56.91 +/- 0.07\n",
            "Episode length: 76.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=-3.31 +/- 0.00\n",
            "Episode length: 35.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=-10.34 +/- 0.00\n",
            "Episode length: 30.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=46.06 +/- 0.01\n",
            "Episode length: 70.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=55.00 +/- 0.89\n",
            "Episode length: 75.60 +/- 0.49\n",
            "Eval num_timesteps=110000, episode_reward=58.08 +/- 0.00\n",
            "Episode length: 76.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=-10.98 +/- 0.00\n",
            "Episode length: 29.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=73.02 +/- 0.16\n",
            "Episode length: 83.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=140000, episode_reward=56.13 +/- 0.00\n",
            "Episode length: 76.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=55.15 +/- 0.03\n",
            "Episode length: 73.00 +/- 0.00\n",
            "Eval num_timesteps=160000, episode_reward=55.72 +/- 0.00\n",
            "Episode length: 74.00 +/- 0.00\n",
            "Eval num_timesteps=170000, episode_reward=55.38 +/- 0.77\n",
            "Episode length: 74.20 +/- 0.40\n",
            "Eval num_timesteps=180000, episode_reward=56.18 +/- 0.00\n",
            "Episode length: 75.00 +/- 0.00\n",
            "Eval num_timesteps=190000, episode_reward=55.85 +/- 0.00\n",
            "Episode length: 75.00 +/- 0.00\n",
            "Eval num_timesteps=200000, episode_reward=55.58 +/- 0.05\n",
            "Episode length: 74.00 +/- 0.00\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Non-curriculum training complete and saved to experiments/PPO/noncurriculum/seed_0/C3_DenseTraffic\n",
            "\n",
            "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
            "HELD-OUT 1 (SCrRXO): mean_reward=43.51 ± 0.00, \n",
            "\n",
            "[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\n",
            "HELD-OUT 2 (VaryingDynamics): mean_reward=113.90 ± 48.70, mean_ep_len=117.9\n"
          ]
        }
      ],
      "source": [
        "algos = [\"PPO\"] # \"SAC\", \"DQN\",\n",
        "\n",
        "for algo in algos:\n",
        "    for seed in SEEDS:\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"ALGO={algo}  SEED={seed}\")\n",
        "\n",
        "        for stage in STAGES:\n",
        "            train_noncurriculum(\n",
        "                algo=algo,\n",
        "                stage=stage,\n",
        "                total_timesteps=stage[\"budget\"],  # use this stage's budget\n",
        "                seed=seed,\n",
        "                n_envs=N_ENVS,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qpsW4W5NI9LG",
      "metadata": {
        "id": "qpsW4W5NI9LG"
      },
      "source": [
        "## 6. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "-j-jWQJJgf8k",
      "metadata": {
        "id": "-j-jWQJJgf8k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Adjust if your root is named differently\n",
        "EXPERIMENT_ROOT = Path(\"experiments\")\n",
        "\n",
        "CURVES_DIR = EXPERIMENT_ROOT / \"curves\"\n",
        "CURVES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_eval_npz(eval_npz_path: Path):\n",
        "    \"\"\"\n",
        "    Load SB3 EvalCallback npz.\n",
        "    Returns: timesteps, mean_rewards, std_rewards, mean_ep_len\n",
        "    \"\"\"\n",
        "    data = np.load(eval_npz_path)\n",
        "    timesteps = data[\"timesteps\"].flatten()          # [n_eval]\n",
        "    results = data[\"results\"]                        # [n_eval, n_episodes]\n",
        "    ep_lengths = data[\"ep_lengths\"]                  # [n_eval, n_episodes]\n",
        "\n",
        "    mean_rewards = results.mean(axis=1)\n",
        "    std_rewards = results.std(axis=1)\n",
        "    mean_ep_len = ep_lengths.mean(axis=1)\n",
        "\n",
        "    return timesteps, mean_rewards, std_rewards, mean_ep_len\n",
        "\n",
        "\n",
        "def plot_stage_curves(algo, seed_name, stage_name, eval_npz_path):\n",
        "    \"\"\"\n",
        "    Make per-stage plots:\n",
        "      - reward vs timesteps\n",
        "      - episode length vs timesteps\n",
        "    Save to experiments/curves\n",
        "    \"\"\"\n",
        "    t, mean_r, std_r, mean_len = load_eval_npz(eval_npz_path)\n",
        "\n",
        "    # 1) Reward curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(t, mean_r, marker=\"o\")\n",
        "    plt.fill_between(t, mean_r - std_r, mean_r + std_r, alpha=0.2)\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Eval mean return\")\n",
        "    plt.title(f\"{algo} {seed_name} – {stage_name} (eval reward)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_reward.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # 2) Episode length curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(t, mean_len, marker=\"o\")\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Mean episode length\")\n",
        "    plt.title(f\"{algo} {seed_name} – {stage_name} (episode length)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_ep_len.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_combined_curves(algo, seed_name, stage_to_npz):\n",
        "    \"\"\"\n",
        "    Combined plots across stages for one algo+seed:\n",
        "      - all reward curves\n",
        "      - all episode length curves\n",
        "    stage_to_npz: dict {stage_name: eval_npz_path}\n",
        "    \"\"\"\n",
        "    # Reward\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    for stage_name, npz_path in stage_to_npz.items():\n",
        "        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n",
        "        plt.plot(t, mean_r, marker=\"o\", label=stage_name)\n",
        "\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Eval mean return\")\n",
        "    plt.title(f\"{algo} {seed_name} – eval reward (all stages)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_reward.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # Episode length\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    for stage_name, npz_path in stage_to_npz.items():\n",
        "        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n",
        "        plt.plot(t, mean_len, marker=\"o\", label=stage_name)\n",
        "\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(\"Mean episode length\")\n",
        "    plt.title(f\"{algo} {seed_name} – episode length (all stages)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_ep_len.png\"\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_heldout_bars(algo, seed_name, seed_dir: Path):\n",
        "    \"\"\"\n",
        "    If held-out CSVs exist for this algo+seed, make bar plots:\n",
        "      - heldout_scrrxo_metrics.csv across stages\n",
        "      - heldout_varying_metrics.csv across stages\n",
        "    \"\"\"\n",
        "    # Find all stages under this seed dir\n",
        "    stage_dirs = [\n",
        "        d for d in seed_dir.iterdir()\n",
        "        if d.is_dir() and not d.name.startswith(\".\")\n",
        "    ]\n",
        "\n",
        "    # ----- Held-out 1: SCrRXO -----\n",
        "    rows = []\n",
        "    for sd in stage_dirs:\n",
        "        csv_path = sd / \"heldout_scrrxo_metrics.csv\"\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            if not df.empty:\n",
        "                r = df.iloc[0].to_dict()\n",
        "                r[\"stage\"] = sd.name\n",
        "                rows.append(r)\n",
        "\n",
        "    if rows:\n",
        "        df_scr = pd.DataFrame(rows)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(df_scr[\"stage\"], df_scr[\"mean_reward\"])\n",
        "        plt.xlabel(\"Training stage\")\n",
        "        plt.ylabel(\"Held-out mean reward\")\n",
        "        plt.title(f\"{algo} {seed_name} – Held-out SCrRXO performance\")\n",
        "        plt.grid(axis=\"y\")\n",
        "        plt.tight_layout()\n",
        "        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_SCrRXO.png\"\n",
        "        plt.savefig(out_path, dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # ----- Held-out 2: VaryingDynamics -----\n",
        "    rows = []\n",
        "    for sd in stage_dirs:\n",
        "        csv_path = sd / \"heldout_varying_metrics.csv\"\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            if not df.empty:\n",
        "                r = df.iloc[0].to_dict()\n",
        "                r[\"stage\"] = sd.name\n",
        "                rows.append(r)\n",
        "\n",
        "    if rows:\n",
        "        df_vd = pd.DataFrame(rows)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(df_vd[\"stage\"], df_vd[\"mean_reward\"])\n",
        "        plt.xlabel(\"Training stage\")\n",
        "        plt.ylabel(\"Held-out mean reward\")\n",
        "        plt.title(f\"{algo} {seed_name} – Held-out VaryingDynamics performance\")\n",
        "        plt.grid(axis=\"y\")\n",
        "        plt.tight_layout()\n",
        "        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_VaryingDynamics.png\"\n",
        "        plt.savefig(out_path, dpi=150)\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "vHiU4aOEj-7z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHiU4aOEj-7z",
        "outputId": "51b68d0c-00cd-4349-f43d-bcea1c894c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing PPO / seed_0 ...\n",
            "All curves saved under: experiments/curves\n"
          ]
        }
      ],
      "source": [
        "# ======================================================================\n",
        "# MAIN: walk experiments/ and generate all possible plots\n",
        "# ======================================================================\n",
        "\n",
        "for algo_dir in EXPERIMENT_ROOT.iterdir():\n",
        "    if not algo_dir.is_dir():\n",
        "        continue\n",
        "    algo = algo_dir.name  # e.g. \"PPO\", \"SAC\", \"DQN\"\n",
        "\n",
        "    noncurr_dir = algo_dir / \"noncurriculum\"\n",
        "    if not noncurr_dir.exists():\n",
        "        continue\n",
        "\n",
        "    for seed_dir in noncurr_dir.iterdir():\n",
        "        if not seed_dir.is_dir() or not seed_dir.name.startswith(\"seed_\"):\n",
        "            continue\n",
        "        seed_name = seed_dir.name  # e.g. \"seed_0\"\n",
        "\n",
        "        print(f\"Processing {algo} / {seed_name} ...\")\n",
        "\n",
        "        # collect per-stage npz paths for combined plots\n",
        "        stage_to_npz = {}\n",
        "\n",
        "        # per-stage plots\n",
        "        for stage_dir in seed_dir.iterdir():\n",
        "            if not stage_dir.is_dir():\n",
        "                continue\n",
        "            stage_name = stage_dir.name  # e.g. \"C0_Straight\"\n",
        "\n",
        "            eval_npz_path = stage_dir / \"eval\" / \"evaluations.npz\"\n",
        "            if not eval_npz_path.exists():\n",
        "                continue\n",
        "\n",
        "            # individual plots\n",
        "            plot_stage_curves(algo, seed_name, stage_name, eval_npz_path)\n",
        "            stage_to_npz[stage_name] = eval_npz_path\n",
        "\n",
        "        # combined curves across stages\n",
        "        if stage_to_npz:\n",
        "            plot_combined_curves(algo, seed_name, stage_to_npz)\n",
        "\n",
        "        # held-out bar plots\n",
        "        plot_heldout_bars(algo, seed_name, seed_dir)\n",
        "\n",
        "print(\"All curves saved under:\", CURVES_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "sZWIeaOMj_mO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZWIeaOMj_mO",
        "outputId": "189969d8-5af5-47e8-d7d8-f9de4bd52be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing stage: PPO_1\n",
            "Saved all curves for PPO_1 → experiments/tensorboard/PPO_1/curves\n",
            "\n",
            "Processing stage: PPO_2\n",
            "Saved all curves for PPO_2 → experiments/tensorboard/PPO_2/curves\n",
            "\n",
            "Processing stage: PPO_3\n",
            "Saved all curves for PPO_3 → experiments/tensorboard/PPO_3/curves\n",
            "\n",
            "Processing stage: PPO_4\n",
            "Saved all curves for PPO_4 → experiments/tensorboard/PPO_4/curves\n",
            "\n",
            "Finished generating all plots.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"experiments/tensorboard\")\n",
        "\n",
        "def load_tb_scalars(event_file):\n",
        "    event_file = str(event_file)   # <--- FIX HERE\n",
        "    ea = EventAccumulator(event_file)\n",
        "    ea.Reload()\n",
        "    scalars = {}\n",
        "    for tag in ea.Tags()[\"scalars\"]:\n",
        "        events = ea.Scalars(tag)\n",
        "        steps = [e.step for e in events]\n",
        "        values = [e.value for e in events]\n",
        "        scalars[tag] = (steps, values)\n",
        "    return scalars\n",
        "\n",
        "\n",
        "def plot_scalar(steps, values, title, out_path):\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(steps, values)\n",
        "    plt.xlabel(\"Timesteps\")\n",
        "    plt.ylabel(title)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_stage(stage_path):\n",
        "    print(f\"\\nProcessing stage: {stage_path.name}\")\n",
        "    curves_dir = stage_path / \"curves\"\n",
        "    curves_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Find tensorboard event file\n",
        "    event_files = list(stage_path.glob(\"**/events.out.tfevents.*\"))\n",
        "    if len(event_files)==0:\n",
        "        print(\"No TB file found for eval.\")\n",
        "        return\n",
        "    tb_file = event_files[0]\n",
        "    tb_scalars = load_tb_scalars(tb_file)\n",
        "\n",
        "    # Plot all general scalars\n",
        "    for tag, (steps, values) in tb_scalars.items():\n",
        "        clean_name = tag.replace(\"/\", \"_\")\n",
        "        out = curves_dir / f\"{clean_name}.png\"\n",
        "        plot_scalar(steps, values, f\"{stage_path.name}: {tag}\", out)\n",
        "\n",
        "    print(f\"Saved all curves for {stage_path.name} → {curves_dir}\")\n",
        "\n",
        "\n",
        "# =============================\n",
        "# PROCESS EACH STAGE\n",
        "# =============================\n",
        "all_stage_dirs = sorted([p for p in BASE.glob(\"*\") if p.is_dir()])\n",
        "\n",
        "for stage_dir in all_stage_dirs:\n",
        "    plot_stage(stage_dir)\n",
        "\n",
        "print(\"\\nFinished generating all plots.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "i0oLMRwtFbhQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0oLMRwtFbhQ",
        "outputId": "177c628d-0679-4ded-e97b-f92ef1f3ceb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zipped to experiments_PPO.zip\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"experiments_PPO\", \"zip\", \"experiments\")\n",
        "\n",
        "print(\"Zipped to experiments_PPO.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aYGcHpMI02D4",
      "metadata": {
        "id": "aYGcHpMI02D4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
