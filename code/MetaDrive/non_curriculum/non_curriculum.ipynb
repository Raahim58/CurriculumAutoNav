{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c876cf",
   "metadata": {
    "id": "15c876cf"
   },
   "source": [
    "# NonCurriculum_MetaDrive_SB3_Experiments\n",
    "\n",
    "This notebook contains a full, reproducible experiment pipeline for **non-curriculum** based reinforcement learning for autonomous driving using **MetaDrive** and **Stable Baselines3 (SB3)**. It includes:\n",
    "\n",
    "- Full environment factory and wrappers (including a discrete-action wrapper for DQN).\n",
    "- Exact stage definitions (C0..C3) and matching budgets.\n",
    "- Non-curriculum runner (train each target map separately for the same total sample budget).\n",
    "- Evaluation harness (metrics logging, CSV saving, TensorBoard integration, video recording).\n",
    "- Hyperparameters and experiment folder conventions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b7750",
   "metadata": {
    "id": "8f8b7750"
   },
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9KG6c_nUg1EH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:25:44.745494Z",
     "iopub.status.busy": "2025-11-28T16:25:44.744779Z",
     "iopub.status.idle": "2025-11-28T16:25:47.676108Z",
     "shell.execute_reply": "2025-11-28T16:25:47.675378Z",
     "shell.execute_reply.started": "2025-11-28T16:25:44.745471Z"
    },
    "id": "9KG6c_nUg1EH",
    "outputId": "d3761111-e616-405e-fee2-3b6601fe78c0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping metadrive as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping metadrive-simulator-py3-12 as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y metadrive metadrive-simulator metadrive-simulator-py3-12 || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3fc75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:25:47.677947Z",
     "iopub.status.busy": "2025-11-28T16:25:47.677701Z",
     "iopub.status.idle": "2025-11-28T16:27:41.314094Z",
     "shell.execute_reply": "2025-11-28T16:27:41.313382Z",
     "shell.execute_reply.started": "2025-11-28T16:25:47.677923Z"
    },
    "id": "bbe3fc75",
    "outputId": "f12bdee8-7fe7-4a69-d401-b32eb7e90d50",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Found existing installation: protobuf 6.33.0\n",
      "Uninstalling protobuf-6.33.0:\n",
      "  Successfully uninstalled protobuf-6.33.0\n",
      "Collecting numpy<2.0\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf==3.20.3\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting metadrive-simulator\n",
      "  Downloading metadrive_simulator-0.4.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (2.32.5)\n",
      "Requirement already satisfied: gymnasium>=0.28 in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (0.29.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (3.7.2)\n",
      "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (2.6.1)\n",
      "Collecting yapf (from metadrive-simulator)\n",
      "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (0.12.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (4.67.1)\n",
      "Collecting progressbar (from metadrive-simulator)\n",
      "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting panda3d==1.10.13 (from metadrive-simulator)\n",
      "  Downloading panda3d-1.10.13-cp311-cp311-manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting panda3d-gltf==0.13 (from metadrive-simulator)\n",
      "  Downloading panda3d_gltf-0.13-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (11.3.0)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (5.4.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (1.15.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (7.1.3)\n",
      "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (2.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (3.20.0)\n",
      "Requirement already satisfied: Pygments in /usr/local/lib/python3.11/dist-packages (from metadrive-simulator) (2.19.2)\n",
      "Collecting mediapy (from metadrive-simulator)\n",
      "  Downloading mediapy-1.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting panda3d-simplepbr>=0.6 (from panda3d-gltf==0.13->metadrive-simulator)\n",
      "  Downloading panda3d_simplepbr-0.13.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.0+cu124)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (14.2.0)\n",
      "Collecting shimmy~=1.1.0 (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading Shimmy-1.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.3.0)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28->metadrive-simulator) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28->metadrive-simulator) (0.0.4)\n",
      "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (1.4.8)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->metadrive-simulator) (2.9.0.post0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from mediapy->metadrive-simulator) (7.34.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->metadrive-simulator) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->metadrive-simulator) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->metadrive-simulator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->metadrive-simulator) (2025.10.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (4.0.0)\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->metadrive-simulator) (4.5.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (6.5.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (0.19.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (3.0.51)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy->metadrive-simulator) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->mediapy->metadrive-simulator) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->mediapy->metadrive-simulator) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy->metadrive-simulator) (0.2.13)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading metadrive_simulator-0.4.3-py3-none-any.whl (55.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading panda3d-1.10.13-cp311-cp311-manylinux2014_x86_64.whl (55.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading panda3d_gltf-0.13-py3-none-any.whl (25 kB)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading Shimmy-1.1.0-py3-none-any.whl (37 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mediapy-1.2.4-py3-none-any.whl (26 kB)\n",
      "Downloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading panda3d_simplepbr-0.13.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: progressbar, AutoROM.accept-rom-license\n",
      "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12065 sha256=e88d779b7f646f586161a6805423baa9f00ab73a5f102ea3fa5260ed5c5a1ae9\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/bb/b2/5353b966ac6f3c5e1000629a9a5f6aed41794487f551e32efc\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446709 sha256=9ae77c91080101242ff7c3437a8357f858d8d44b97daf6705a34c8a75d575b0a\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
      "Successfully built progressbar AutoROM.accept-rom-license\n",
      "Installing collected packages: progressbar, panda3d, yapf, protobuf, panda3d-simplepbr, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, panda3d-gltf, opencv-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, ale-py, shimmy, nvidia-cusolver-cu12, mediapy, metadrive-simulator\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.12.0.88\n",
      "    Uninstalling opencv-python-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-4.12.0.88\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: ale-py\n",
      "    Found existing installation: ale-py 0.11.2\n",
      "    Uninstalling ale-py-0.11.2:\n",
      "      Successfully uninstalled ale-py-0.11.2\n",
      "  Attempting uninstall: shimmy\n",
      "    Found existing installation: Shimmy 1.3.0\n",
      "    Uninstalling Shimmy-1.3.0:\n",
      "      Successfully uninstalled Shimmy-1.3.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "kaggle-environments 1.18.0 requires shimmy>=1.2.1, but you have shimmy 1.1.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 mediapy-1.2.4 metadrive-simulator-0.4.3 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencv-python-4.11.0.86 panda3d-1.10.13 panda3d-gltf-0.13 panda3d-simplepbr-0.13.1 progressbar-2.5 protobuf-3.20.3 shimmy-1.1.0 yapf-0.43.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"stable-baselines3[extra]\" \"metadrive-simulator-py3-12\" tensorboard opencv-python # for colab\n",
    "\n",
    "## for kaggle:\n",
    "# !pip install \"numpy<2.0\" \"protobuf==3.20.3\" \"metadrive-simulator\" \"stable-baselines3[extra]\" tensorboard opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "MDpNIf_ug8-1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:27:41.315301Z",
     "iopub.status.busy": "2025-11-28T16:27:41.315021Z",
     "iopub.status.idle": "2025-11-28T16:27:49.020722Z",
     "shell.execute_reply": "2025-11-28T16:27:49.020041Z",
     "shell.execute_reply.started": "2025-11-28T16:27:41.315263Z"
    },
    "id": "MDpNIf_ug8-1",
    "outputId": "d207b93b-b387-4972-df8a-c6cb75630660",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadrive.__file__ : /usr/local/lib/python3.11/dist-packages/metadrive/__init__.py\n",
      "metadrive.__path__ : ['/usr/local/lib/python3.11/dist-packages/metadrive']\n"
     ]
    }
   ],
   "source": [
    "import metadrive\n",
    "\n",
    "print(\"metadrive.__file__ :\", getattr(metadrive, \"__file__\", None))\n",
    "print(\"metadrive.__path__ :\", getattr(metadrive, \"__path__\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kKG8RN5zKYtb",
   "metadata": {
    "id": "kKG8RN5zKYtb"
   },
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81fb189",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:27:52.859864Z",
     "iopub.status.busy": "2025-11-28T16:27:52.859581Z",
     "iopub.status.idle": "2025-11-28T16:28:07.794550Z",
     "shell.execute_reply": "2025-11-28T16:28:07.793791Z",
     "shell.execute_reply.started": "2025-11-28T16:27:52.859847Z"
    },
    "id": "e81fb189",
    "outputId": "083943d8-cb76-4273-8ffb-818c9a8eb7f5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 16:27:55.317757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764347275.498266      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764347275.549195      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "# SB3 imports\n",
    "from metadrive.envs.metadrive_env import MetaDriveEnv\n",
    "from metadrive.envs.varying_dynamics_env import VaryingDynamicsEnv\n",
    "from stable_baselines3 import PPO, SAC, DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# MetaDrive import guard\n",
    "# try:\n",
    "# from metadrive.envs.metadrive_env import MetaDriveEnv\n",
    "# except Exception as e:\n",
    "    # MetaDriveEnv = None\n",
    "    # print('MetaDrive import failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "PpACB8RcK8m8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.797745Z",
     "iopub.status.busy": "2025-11-28T16:28:07.797159Z",
     "iopub.status.idle": "2025-11-28T16:28:07.802215Z",
     "shell.execute_reply": "2025-11-28T16:28:07.801475Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.797726Z"
    },
    "id": "PpACB8RcK8m8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")                       # ignore everything\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"stable_baselines3\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3da0a2",
   "metadata": {
    "id": "cc3da0a2"
   },
   "source": [
    "## 3. Experiment configuration\n",
    "setup: maps, stages, budgets, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386723b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.803162Z",
     "iopub.status.busy": "2025-11-28T16:28:07.802971Z",
     "iopub.status.idle": "2025-11-28T16:28:07.837978Z",
     "shell.execute_reply": "2025-11-28T16:28:07.837359Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.803147Z"
    },
    "id": "e386723b",
    "outputId": "4d669274-6425-41ab-e2df-579021c5aabe",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total curriculum budget (per algorithm) = 650000\n"
     ]
    }
   ],
   "source": [
    "STAGES = [\n",
    "    # C0: straight road, no traffic – just learn to go forward safely.\n",
    "    {\n",
    "        \"id\": \"C0\",\n",
    "        \"name\": \"C0_Straight\",\n",
    "        \"env_type\": \"general\",\n",
    "        \"map\": \"S\", # Straight\n",
    "        \"traffic\": 0.0,\n",
    "        \"budget\": 100_000,\n",
    "        \"reward\": {\n",
    "            \"base_w\": 1.0,              # scale MetaDrive base reward\n",
    "            \"speed_w\": 0.05,            # weak speed shaping\n",
    "            \"max_speed_kmh\": 80.0,\n",
    "            \"collision_penalty\": -5.0,\n",
    "            \"offroad_penalty\": -3.0,\n",
    "            \"traffic_violation_penalty\": -2.0,\n",
    "            \"success_bonus\": 10.0,\n",
    "            \"step_penalty\": -0.001,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # C1: roundabout, no traffic – topology harder, still single-ego.\n",
    "    {\n",
    "        \"id\": \"C1\",\n",
    "        \"name\": \"C1_Roundabout\",\n",
    "        \"env_type\": \"general\",\n",
    "        \"map\": \"O\", # Roundabout\n",
    "        \"traffic\": 0.0,\n",
    "        \"budget\": 150_000,\n",
    "        \"reward\": {\n",
    "            \"base_w\": 1.0,\n",
    "            \"speed_w\": 0.05,\n",
    "            \"max_speed_kmh\": 80.0,\n",
    "            \"collision_penalty\": -6.0,  # slightly harsher for bad manoeuvres\n",
    "            \"offroad_penalty\": -4.0,\n",
    "            \"traffic_violation_penalty\": -3.0,\n",
    "            \"success_bonus\": 10.0,\n",
    "            \"step_penalty\": -0.005,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # C2: 20-block PG map with **light traffic** – first exposure to traffic.\n",
    "    {\n",
    "        \"id\": \"C2\",\n",
    "        \"name\": \"C2_LightTraffic\",\n",
    "        \"map\": 10, # 20-block\n",
    "        \"traffic\": 0.05,               # light traffic\n",
    "        \"budget\": 200_000,\n",
    "        \"reward\": {\n",
    "            \"base_w\": 1.0,\n",
    "            \"speed_w\": 0.08,            # encourage moving at speed in traffic\n",
    "            \"max_speed_kmh\": 80.0,\n",
    "            \"collision_penalty\": -8.0,\n",
    "            \"offroad_penalty\": -6.0,\n",
    "            \"traffic_violation_penalty\": -4.0,\n",
    "            \"success_bonus\": 12.0,\n",
    "            \"step_penalty\": -0.01,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # C3: same PG map with **dense traffic** – “multi-agent-ish” final stage.\n",
    "    # Still single learning ego, but many interacting vehicles (like CuRLA's\n",
    "    # higher-traffic final curriculum stage).\n",
    "    {\n",
    "        \"id\": \"C3\",\n",
    "        \"name\": \"C3_DenseTraffic\",\n",
    "        \"map\": 20,\n",
    "        \"traffic\": 0.30,               # dense traffic ≈ mild multi-agent\n",
    "        \"budget\": 200_000,\n",
    "        \"reward\": {\n",
    "            \"base_w\": 1.0,\n",
    "            \"speed_w\": 0.10,\n",
    "            \"max_speed_kmh\": 80.0,\n",
    "            \"collision_penalty\": -10.0, # strong safety pressure\n",
    "            \"offroad_penalty\": -8.0,\n",
    "            \"traffic_violation_penalty\": -5.0,\n",
    "            \"success_bonus\": 15.0,\n",
    "            \"step_penalty\": -0.05,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "TOTAL_CURRICULUM_BUDGET = sum(s[\"budget\"] for s in STAGES)\n",
    "print(\"Total curriculum budget (per algorithm) =\", TOTAL_CURRICULUM_BUDGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "B-HaQNeltf-R",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.839108Z",
     "iopub.status.busy": "2025-11-28T16:28:07.838834Z",
     "iopub.status.idle": "2025-11-28T16:28:07.855649Z",
     "shell.execute_reply": "2025-11-28T16:28:07.855003Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.839090Z"
    },
    "id": "B-HaQNeltf-R",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Held-out 1: fixed 6-block map SCrRXO with medium traffic\n",
    "HELDOUT_SCENARIO_STAGE = {\n",
    "    \"id\": \"HELDOUT_SCENARIO\",\n",
    "    \"name\": \"HELDOUT_SCrRXO_MedTraffic\",\n",
    "    \"map\": \"SCrRXO\", # Straight -> Circular -> in-ramp -> out-ramp -> intersection -> roundabout\n",
    "    \"traffic\": 0.30, # medium-ish traffic\n",
    "    \"budget\": 0, # no training budget, eval only\n",
    "    \"reward\": STAGES[-1][\"reward\"],  # reuse hardest-stage shaping for comparability\n",
    "}\n",
    "\n",
    "# Held-out 2: VaryingDynamics environment (dynamics robustness test)\n",
    "VARYING_DYNAMICS_CONFIG = dict(\n",
    "    num_scenarios=100,\n",
    "    map=5,                  # small PG map\n",
    "    log_level=logging.ERROR,\n",
    "    # random_dynamics uses default ranges from docs; that's enough to show robustness\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w_ndV2-1wbQZ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.856674Z",
     "iopub.status.busy": "2025-11-28T16:28:07.856434Z",
     "iopub.status.idle": "2025-11-28T16:28:07.869846Z",
     "shell.execute_reply": "2025-11-28T16:28:07.869270Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.856650Z"
    },
    "id": "w_ndV2-1wbQZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Folder convention\n",
    "EXPERIMENT_ROOT = Path('experiments')\n",
    "EXPERIMENT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# Seeds and workers\n",
    "SEEDS = [0]\n",
    "N_ENVS = 1 # should be 8 but metadrive issues\n",
    "EVAL_FREQ = 10_000\n",
    "EVAL_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "HjNj-DhhwaJR",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.870893Z",
     "iopub.status.busy": "2025-11-28T16:28:07.870715Z",
     "iopub.status.idle": "2025-11-28T16:28:07.883947Z",
     "shell.execute_reply": "2025-11-28T16:28:07.883281Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.870879Z"
    },
    "id": "HjNj-DhhwaJR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HYPERS = {\n",
    "    'PPO': {\n",
    "        'policy':'MlpPolicy',\n",
    "        'policy_kwargs':{'net_arch':[64,64]}, # can do 64, 64 as well\n",
    "        'learning_rate':3e-4,\n",
    "        'n_steps':1024, # compute issue, can do 2048 if faster\n",
    "        'batch_size':64,\n",
    "        'n_epochs':10,\n",
    "        'gamma':0.99,\n",
    "        'clip_range':0.2\n",
    "    },\n",
    "    # 'SAC': {\n",
    "    #     'policy':'MlpPolicy',\n",
    "    #     'policy_kwargs':{'net_arch':[256,256]},\n",
    "    #     'learning_rate':3e-4,\n",
    "    #     'batch_size':256,\n",
    "    #     'buffer_size':100_000,\n",
    "    #     'gamma':0.99\n",
    "    # },\n",
    "    'SAC':{\n",
    "        'policy':'MlpPolicy',\n",
    "        'policy_kwargs':{'net_arch':[128,128]},\n",
    "        'learning_rate':3e-4,\n",
    "        'batch_size':128,\n",
    "        'buffer_size':100_000,\n",
    "        'gamma':0.99\n",
    "    },\n",
    "    'DQN': { # Atari setup\n",
    "        'policy':'MlpPolicy',\n",
    "        'policy_kwargs':{'net_arch':[64,64]},\n",
    "        'learning_rate':1e-4,\n",
    "        'buffer_size':100_000, # can do 50,000 if needed\n",
    "        'batch_size':32,\n",
    "        'train_freq':4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e299413",
   "metadata": {
    "id": "0e299413"
   },
   "source": [
    "Includes a discrete-action wrapper for DQN (maps discrete indices -> continuous steer/throttle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e0cb97e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.884815Z",
     "iopub.status.busy": "2025-11-28T16:28:07.884595Z",
     "iopub.status.idle": "2025-11-28T16:28:07.898347Z",
     "shell.execute_reply": "2025-11-28T16:28:07.897774Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.884790Z"
    },
    "id": "3e0cb97e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, mapping):\n",
    "        super().__init__(env)\n",
    "        self.mapping = mapping\n",
    "        self.action_space = spaces.Discrete(len(mapping))\n",
    "\n",
    "    def action(self, action):\n",
    "        return np.array(self.mapping[action], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "wLTVGUAADX1r",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.899614Z",
     "iopub.status.busy": "2025-11-28T16:28:07.898988Z",
     "iopub.status.idle": "2025-11-28T16:28:07.911642Z",
     "shell.execute_reply": "2025-11-28T16:28:07.910981Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.899589Z"
    },
    "id": "wLTVGUAADX1r",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CurriculumRewardWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Stage-dependent reward shaping:\n",
    "    - Start from MetaDrive's base reward.\n",
    "    - Add speed term.\n",
    "    - Add collision / off-road / traffic-violation penalties.\n",
    "    - Add success bonus.\n",
    "\n",
    "    Uses the per-stage reward config from STAGES.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, reward_cfg):\n",
    "        super().__init__(env)\n",
    "        self.cfg = reward_cfg\n",
    "        self.max_speed = self.cfg.get(\"max_speed_kmh\", 80.0)\n",
    "\n",
    "    def step(self, action):\n",
    "        # MetaDrive uses Gymnasium API: obs, reward, terminated, truncated, info\n",
    "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # --- speed term ---\n",
    "        # MetaDrive usually exposes speed either as 'speed' or 'velocity'\n",
    "        raw_speed = float(info.get(\"speed\", info.get(\"velocity\", 0.0)))\n",
    "        speed = max(0.0, min(raw_speed, self.max_speed))\n",
    "        speed_term = self.cfg.get(\"speed_w\", 0.0) * (speed / self.max_speed)\n",
    "\n",
    "        # --- start from scaled base reward + speed shaping ---\n",
    "        r = self.cfg.get(\"base_w\", 1.0) * base_r + speed_term\n",
    "\n",
    "        # --- per-step cost (encourage finishing sooner) ---\n",
    "        r += self.cfg.get(\"step_penalty\", 0.0)\n",
    "\n",
    "        # --- collision penalties ---\n",
    "        crashed = (\n",
    "            info.get(\"crash_vehicle\", False)\n",
    "            or info.get(\"crash_object\", False)\n",
    "            or info.get(\"crash_building\", False)\n",
    "        )\n",
    "        if crashed:\n",
    "            r += self.cfg.get(\"collision_penalty\", 0.0)\n",
    "        info[\"collision\"] = bool(crashed)\n",
    "\n",
    "        # --- off-road / traffic-violation penalties ---\n",
    "        offroad = info.get(\"out_of_road\", False)\n",
    "        if offroad:\n",
    "            r += self.cfg.get(\"offroad_penalty\", 0.0)\n",
    "\n",
    "        # generic \"traffic violation\" flag for your metrics callback\n",
    "        traffic_violation = bool(offroad or info.get(\"traffic_light_violation\", False))\n",
    "        if traffic_violation:\n",
    "            r += self.cfg.get(\"traffic_violation_penalty\", 0.0)\n",
    "        info[\"traffic_violation\"] = traffic_violation\n",
    "\n",
    "        # --- success bonus at terminal step ---\n",
    "        success = bool(info.get(\"arrive_dest\", False) or info.get(\"success\", False))\n",
    "        if terminated and success:\n",
    "            r += self.cfg.get(\"success_bonus\", 0.0)\n",
    "        info[\"success\"] = success\n",
    "\n",
    "        # For logging\n",
    "        info[\"avg_speed\"] = speed\n",
    "        info[\"shaped_reward\"] = r\n",
    "\n",
    "        return obs, r, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2b81f",
   "metadata": {
    "id": "75c2b81f"
   },
   "source": [
    "## 4. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nGXJIcxPxSfh",
   "metadata": {
    "id": "nGXJIcxPxSfh"
   },
   "source": [
    "### 4.1 Environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vKLjHVxvkKCh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.912687Z",
     "iopub.status.busy": "2025-11-28T16:28:07.912412Z",
     "iopub.status.idle": "2025-11-28T16:28:07.927330Z",
     "shell.execute_reply": "2025-11-28T16:28:07.926727Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.912665Z"
    },
    "id": "vKLjHVxvkKCh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class MetaDriveGymCompatibilityWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Makes MetaDriveEnv follow the Gymnasium reset() and step() signature.\n",
    "    Removes unsupported arguments like options.\n",
    "    \"\"\"\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            obs, info = self.env.reset(seed=seed)\n",
    "        else:\n",
    "            obs, info = self.env.reset()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        # MetaDrive uses done only; Gymnasium expects (terminated, truncated)\n",
    "        terminated = done\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sv070jNaDnyR",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.928305Z",
     "iopub.status.busy": "2025-11-28T16:28:07.928086Z",
     "iopub.status.idle": "2025-11-28T16:28:07.944374Z",
     "shell.execute_reply": "2025-11-28T16:28:07.943678Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.928282Z"
    },
    "id": "sv070jNaDnyR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "def make_metadrive_env(stage, use_discrete=False, seed=0, render=False):\n",
    "    \"\"\"\n",
    "    stage: one of the dicts from STAGES. Uses:\n",
    "        stage[\"map\"], stage[\"traffic\"], stage[\"reward\"]\n",
    "    \"\"\"\n",
    "    map_name = stage[\"map\"]\n",
    "    traffic_density = stage[\"traffic\"]\n",
    "    reward_cfg = stage[\"reward\"]\n",
    "\n",
    "    def _init():\n",
    "        cfg = {\n",
    "            \"map\": map_name,\n",
    "            \"traffic_density\": traffic_density,\n",
    "            \"use_render\": render,\n",
    "            \"start_seed\": seed,\n",
    "            # \"random_spawn\": True,\n",
    "            \"debug\": False,\n",
    "            \"log_level\": logging.ERROR,\n",
    "            # cap episode length so eval can't run forever\n",
    "            \"horizon\": 1000, # max steps per episode\n",
    "            \"truncate_as_terminate\": True,   # treat horizon as done\n",
    "        }\n",
    "        env = MetaDriveEnv(cfg)\n",
    "\n",
    "        # Fix reset() signature mismatch\n",
    "        env = MetaDriveGymCompatibilityWrapper(env)\n",
    "\n",
    "        # CuRLA-style stage-dependent reward shaping\n",
    "        env = CurriculumRewardWrapper(env, reward_cfg)\n",
    "\n",
    "        # discrete-action wrapper for DQN: [steering, throttle]\n",
    "        if use_discrete:\n",
    "            mapping = [\n",
    "              (-1.0, 0.0), # hard left, no throttle\n",
    "              (-1.0, 0.3), # left + some accel\n",
    "              (0.0, 0.5), # go straight, accel\n",
    "              (1.0, 0.3), # right + some accel\n",
    "              (1.0, 0.0), # hard right, no throttle\n",
    "            ]\n",
    "            env = DiscreteActionWrapper(env, mapping)\n",
    "\n",
    "        return Monitor(env)\n",
    "\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675fffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "def make_vec_env(stage, n_envs=1, use_discrete=False, seed=0):\n",
    "    \"\"\"\n",
    "    Create vectorized MetaDrive envs.\n",
    "    FIX: We must ALWAYS use SubprocVecEnv for MetaDrive.\n",
    "    Using DummyVecEnv (single process) prevents creating a second env\n",
    "    (like eval_env) because MetaDrive allows only one engine per process.\n",
    "    \"\"\"\n",
    "\n",
    "    return DummyVecEnv([\n",
    "        make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dTkPoInXfp4F",
   "metadata": {
    "id": "dTkPoInXfp4F"
   },
   "source": [
    "PPO/SAC use continous control (`Box`) from MetaDrive while DQN uses a manually defined discrete control space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Q-OkesJEflDV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.964294Z",
     "iopub.status.busy": "2025-11-28T16:28:07.964061Z",
     "iopub.status.idle": "2025-11-28T16:28:07.985724Z",
     "shell.execute_reply": "2025-11-28T16:28:07.985003Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.964274Z"
    },
    "id": "Q-OkesJEflDV",
    "outputId": "2a120405-d933-4a83-ccbc-e1454ff5cd3e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO/SAC action space: Box(-1.0, 1.0, (2,), float32)\n",
      "obs space: Box(-0.0, 1.0, (259,), float32)\n"
     ]
    }
   ],
   "source": [
    "vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=False, seed=0)\n",
    "base_env = vec.envs[0]    # DummyVecEnv\n",
    "print(\"PPO/SAC action space:\", base_env.action_space)\n",
    "print(\"obs space:\", base_env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "TA49LfETfmvF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:07.986627Z",
     "iopub.status.busy": "2025-11-28T16:28:07.986411Z",
     "iopub.status.idle": "2025-11-28T16:28:08.004850Z",
     "shell.execute_reply": "2025-11-28T16:28:08.004232Z",
     "shell.execute_reply.started": "2025-11-28T16:28:07.986601Z"
    },
    "id": "TA49LfETfmvF",
    "outputId": "f4a4b5da-67b3-422d-c4ef-23b67dc30b01",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN action space: Discrete(5)\n",
      "obs space: Box(-0.0, 1.0, (259,), float32)\n"
     ]
    }
   ],
   "source": [
    "vec = make_vec_env(STAGES[2], n_envs=1, use_discrete=True, seed=0)\n",
    "base_env = vec.envs[0]    # DummyVecEnv\n",
    "print(\"DQN action space:\", base_env.action_space)\n",
    "print(\"obs space:\", base_env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hJa0pGDE4Z1J",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.005862Z",
     "iopub.status.busy": "2025-11-28T16:28:08.005538Z",
     "iopub.status.idle": "2025-11-28T16:28:08.022580Z",
     "shell.execute_reply": "2025-11-28T16:28:08.021884Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.005841Z"
    },
    "id": "hJa0pGDE4Z1J",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_eval_vec_env(stage, use_discrete=False, seed=0):\n",
    "    \"\"\"\n",
    "    Eval env:\n",
    "    - Always uses SubprocVecEnv (even with 1 worker) so that MetaDriveEnv\n",
    "      is only ever created in subprocesses, not in the main process.\n",
    "    \"\"\"\n",
    "    env_fns = [make_metadrive_env(stage, use_discrete=use_discrete, seed=seed)]\n",
    "    return SubprocVecEnv(env_fns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d56696",
   "metadata": {
    "id": "34d56696"
   },
   "source": [
    "### 4.2 log per-eval metrics (success rate, collisions, speed etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zGjpX3W1FZsu",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.023590Z",
     "iopub.status.busy": "2025-11-28T16:28:08.023299Z",
     "iopub.status.idle": "2025-11-28T16:28:08.040143Z",
     "shell.execute_reply": "2025-11-28T16:28:08.039545Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.023574Z"
    },
    "id": "zGjpX3W1FZsu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Run a short evaluation every eval_freq steps and log:\n",
    "      - mean_reward\n",
    "      - success_rate\n",
    "      - collision_rate\n",
    "      - traffic_violation_rate\n",
    "      - avg_speed\n",
    "      - avg_episode_length\n",
    "\n",
    "    Saves to a CSV at csv_path.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, csv_path, eval_freq=50_000, eval_episodes=10, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.csv_path = csv_path\n",
    "        self.eval_freq = eval_freq\n",
    "        self.eval_episodes = eval_episodes\n",
    "\n",
    "        # Create dir if needed\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "        # Write header if file doesn't exist\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "                writer = csv.DictWriter(\n",
    "                    f,\n",
    "                    fieldnames=[\n",
    "                        \"timesteps\",\n",
    "                        \"mean_reward\",\n",
    "                        \"success_rate\",\n",
    "                        \"collision_rate\",\n",
    "                        \"traffic_violation_rate\",\n",
    "                        \"avg_speed\",\n",
    "                        \"avg_episode_length\",\n",
    "                    ],\n",
    "                )\n",
    "                writer.writeheader()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Only evaluate every eval_freq calls\n",
    "        if self.n_calls % self.eval_freq != 0:\n",
    "            return True\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_successes = []\n",
    "        episode_collisions = []\n",
    "        episode_traffic_violations = []\n",
    "        episode_speeds = []\n",
    "        episode_lengths = []\n",
    "\n",
    "        for _ in range(self.eval_episodes):\n",
    "            # DummyVecEnv.reset() -> obs (no info, vec-batched)\n",
    "            obs = self.eval_env.reset()\n",
    "            info = {}\n",
    "            done = False\n",
    "\n",
    "            ep_reward = 0.0\n",
    "            ep_success = False\n",
    "            ep_collision = False\n",
    "            ep_traffic_violation = False\n",
    "            ep_steps = 0\n",
    "            ep_speeds = []\n",
    "\n",
    "            while not done:\n",
    "                # obs shape: (n_envs, obs_dim); n_envs = 1 here\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                # DummyVecEnv.step() -> obs, rewards, dones, infos\n",
    "                obs, rewards, dones, infos = self.eval_env.step(action)\n",
    "\n",
    "                # unwrap vec env outputs for single env\n",
    "                if isinstance(rewards, (np.ndarray, list, tuple)):\n",
    "                    r = float(rewards[0])\n",
    "                else:\n",
    "                    r = float(rewards)\n",
    "\n",
    "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
    "                    d = bool(dones[0])\n",
    "                else:\n",
    "                    d = bool(dones)\n",
    "\n",
    "                if isinstance(infos, (list, tuple)) and len(infos) > 0:\n",
    "                    info = infos[0]\n",
    "                else:\n",
    "                    info = infos\n",
    "\n",
    "                ep_reward += r\n",
    "                ep_steps += 1\n",
    "                done = d\n",
    "\n",
    "                # flags from MetaDrive info / our wrapper\n",
    "                if isinstance(info, dict):\n",
    "                    # success: either our wrapper's \"success\" OR MetaDrive's arrive_dest\n",
    "                    if info.get(\"success\", False) or info.get(\"arrive_dest\", False):\n",
    "                        ep_success = True\n",
    "\n",
    "                    # collision: either our wrapper's \"collision\" OR any crash/out-of-road\n",
    "                    if (\n",
    "                        info.get(\"collision\", False)\n",
    "                        or info.get(\"crash_vehicle\", False)\n",
    "                        or info.get(\"crash_object\", False)\n",
    "                        or info.get(\"crash_building\", False)\n",
    "                        or info.get(\"out_of_road\", False)\n",
    "                    ):\n",
    "                        ep_collision = True\n",
    "\n",
    "                    # traffic violation if we ever log it; otherwise this will stay 0\n",
    "                    if info.get(\"traffic_violation\", False):\n",
    "                        ep_traffic_violation = True\n",
    "\n",
    "                    # speed logging\n",
    "                    if \"avg_speed\" in info:\n",
    "                        ep_speeds.append(float(info[\"avg_speed\"]))\n",
    "                    elif \"speed\" in info:\n",
    "                        ep_speeds.append(float(info[\"speed\"]))\n",
    "\n",
    "\n",
    "            episode_rewards.append(ep_reward)\n",
    "            episode_successes.append(1.0 if ep_success else 0.0)\n",
    "            episode_collisions.append(1.0 if ep_collision else 0.0)\n",
    "            episode_traffic_violations.append(1.0 if ep_traffic_violation else 0.0)\n",
    "            episode_lengths.append(ep_steps)\n",
    "            if ep_speeds:\n",
    "                episode_speeds.append(sum(ep_speeds) / len(ep_speeds))\n",
    "\n",
    "        mean_reward = float(sum(episode_rewards) / len(episode_rewards)) if episode_rewards else 0.0\n",
    "        success_rate = float(sum(episode_successes) / len(episode_successes)) if episode_successes else 0.0\n",
    "        collision_rate = float(sum(episode_collisions) / len(episode_collisions)) if episode_collisions else 0.0\n",
    "        traffic_violation_rate = float(sum(episode_traffic_violations) / len(episode_traffic_violations)) if episode_traffic_violations else 0.0\n",
    "        avg_speed = float(sum(episode_speeds) / len(episode_speeds)) if episode_speeds else 0.0\n",
    "        avg_episode_length = float(sum(episode_lengths) / len(episode_lengths)) if episode_lengths else 0.0\n",
    "\n",
    "        row = {\n",
    "            \"timesteps\": int(self.num_timesteps),\n",
    "            \"mean_reward\": mean_reward,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"collision_rate\": collision_rate,\n",
    "            \"traffic_violation_rate\": traffic_violation_rate,\n",
    "            \"avg_speed\": avg_speed,\n",
    "            \"avg_episode_length\": avg_episode_length,\n",
    "        }\n",
    "\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "            writer.writerow(row)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(f\"[Metrics] t={self.num_timesteps}  succ={success_rate:.2f}  \"\n",
    "                  f\"coll={collision_rate:.2f}  len={avg_episode_length:.1f}\")\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "-eFLK2DKKDJR",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.041132Z",
     "iopub.status.busy": "2025-11-28T16:28:08.040855Z",
     "iopub.status.idle": "2025-11-28T16:28:08.059768Z",
     "shell.execute_reply": "2025-11-28T16:28:08.059080Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.041116Z"
    },
    "id": "-eFLK2DKKDJR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "class PrettyEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Clean pretty printing for eval:\n",
    "      - One separator before the first eval\n",
    "      - No spam between evals\n",
    "      - One separator at the end of training on this stage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._started = False    # whether we have printed the first separator\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if it's time to evaluate\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            # On first eval, print top separator\n",
    "            if not self._started:\n",
    "                print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "                self._started = True\n",
    "\n",
    "        return super()._on_step()\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # After training for this stage: print final separator\n",
    "        if self._started:\n",
    "            print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "        return super()._on_training_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rPOxRfNNHKbp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.060821Z",
     "iopub.status.busy": "2025-11-28T16:28:08.060554Z",
     "iopub.status.idle": "2025-11-28T16:28:08.074745Z",
     "shell.execute_reply": "2025-11-28T16:28:08.074081Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.060795Z"
    },
    "id": "rPOxRfNNHKbp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_run_config(out_dir, algo, stage, seed):\n",
    "    \"\"\"\n",
    "    Save basic run config (algo, stage, hyperparams, seed) to config.json\n",
    "    so you can reproduce / inspect later.\n",
    "    \"\"\"\n",
    "    cfg = {\n",
    "        \"algo\": algo,\n",
    "        \"seed\": seed,\n",
    "        \"stage\": {\n",
    "            \"id\": stage[\"id\"],\n",
    "            \"name\": stage[\"name\"],\n",
    "            \"map\": stage[\"map\"],\n",
    "            \"traffic\": stage[\"traffic\"],\n",
    "            \"budget\": stage[\"budget\"],\n",
    "            \"reward\": stage[\"reward\"],\n",
    "        },\n",
    "        \"hyperparams\": HYPERS[algo],\n",
    "        \"heldout_map\": {\"map\": HELDOUT_SCENARIO_STAGE['map'], \"traffic\": HELDOUT_SCENARIO_STAGE['traffic'], \"reward\": HELDOUT_SCENARIO_STAGE['reward']},\n",
    "    }\n",
    "    with open(out_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "soBC1415it33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.075708Z",
     "iopub.status.busy": "2025-11-28T16:28:08.075465Z",
     "iopub.status.idle": "2025-11-28T16:28:08.095979Z",
     "shell.execute_reply": "2025-11-28T16:28:08.095182Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.075689Z"
    },
    "id": "soBC1415it33",
    "outputId": "a1c917f2-4a8f-40e5-e4a4-bde4a6ec4beb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'HELDOUT_SCENARIO', 'name': 'HELDOUT_SCrRXO_MedTraffic', 'map': 'SCrRXO', 'traffic': 0.3, 'budget': 0, 'reward': {'base_w': 1.0, 'speed_w': 0.1, 'max_speed_kmh': 80.0, 'collision_penalty': -10.0, 'offroad_penalty': -8.0, 'traffic_violation_penalty': -5.0, 'success_bonus': 15.0, 'step_penalty': -0.05}}\n"
     ]
    }
   ],
   "source": [
    "print(HELDOUT_SCENARIO_STAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303bc4f",
   "metadata": {
    "id": "0303bc4f"
   },
   "source": [
    "### 4.3 Training functions\n",
    "\n",
    "These functions create models, attach callbacks, and run training. Each saves checkpoint, best-model and CSV metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "yLzoiNexEQ6n",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.096961Z",
     "iopub.status.busy": "2025-11-28T16:28:08.096742Z",
     "iopub.status.idle": "2025-11-28T16:28:08.110659Z",
     "shell.execute_reply": "2025-11-28T16:28:08.109930Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.096934Z"
    },
    "id": "yLzoiNexEQ6n",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def make_model(algo, env, hyperparams):\n",
    "    common_kwargs = dict(\n",
    "        verbose=0,  # make SB3 quiet in console\n",
    "        tensorboard_log=str(EXPERIMENT_ROOT / 'tensorboard'),\n",
    "        policy_kwargs=hyperparams['policy_kwargs'],\n",
    "        learning_rate=hyperparams['learning_rate'],\n",
    "    )\n",
    "\n",
    "    if algo == 'PPO':\n",
    "        model = PPO(\n",
    "            hyperparams['policy'],\n",
    "            env,\n",
    "            n_steps=hyperparams['n_steps'],\n",
    "            batch_size=hyperparams['batch_size'],\n",
    "            n_epochs=hyperparams['n_epochs'],\n",
    "            gamma=hyperparams['gamma'],\n",
    "            device=\"cpu\",\n",
    "            **common_kwargs,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    if algo == 'SAC':\n",
    "        model = SAC(\n",
    "            hyperparams['policy'],\n",
    "            env,\n",
    "            batch_size=hyperparams.get('batch_size', 256),\n",
    "            buffer_size=hyperparams.get('buffer_size', 100_000),\n",
    "            gamma=hyperparams.get('gamma', 0.99),\n",
    "            device=DEVICE,\n",
    "            **common_kwargs,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    if algo == 'DQN':\n",
    "        model = DQN(\n",
    "            hyperparams['policy'],\n",
    "            env,\n",
    "            buffer_size=hyperparams.get('buffer_size', 50_000),\n",
    "            batch_size=hyperparams.get('batch_size', 32),\n",
    "            train_freq=hyperparams.get('train_freq', 4),\n",
    "            device=DEVICE,\n",
    "            **common_kwargs,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    raise ValueError('Unknown algo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "z5yA8SnOtpCg",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.114425Z",
     "iopub.status.busy": "2025-11-28T16:28:08.114195Z",
     "iopub.status.idle": "2025-11-28T16:28:08.128529Z",
     "shell.execute_reply": "2025-11-28T16:28:08.127934Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.114400Z"
    },
    "id": "z5yA8SnOtpCg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_env_episodes(model, env, n_episodes=20, deterministic=True):\n",
    "    \"\"\"\n",
    "    Run n_episodes on a *single* (non-vec) env.\n",
    "    Returns (mean_reward, std_reward, mean_ep_len).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        reset_out = env.reset()\n",
    "        # Handle reset() returning either obs or (obs, info)\n",
    "        if isinstance(reset_out, tuple):\n",
    "            obs, _info = reset_out\n",
    "        else:\n",
    "            obs = reset_out\n",
    "        info = {}\n",
    "        done = False\n",
    "        truncated = False\n",
    "        ep_r = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            action, _ = model.predict(obs, deterministic=deterministic)\n",
    "            step_out = env.step(action)\n",
    "            # step could be:\n",
    "            #  - (obs, reward, done, info)  [old Gym]\n",
    "            #  - (obs, reward, terminated, truncated, info) [Gymnasium]\n",
    "            if len(step_out) == 5:\n",
    "                obs, r, terminated, truncated, info = step_out\n",
    "                done = bool(terminated or truncated)\n",
    "            elif len(step_out) == 4:\n",
    "                obs, r, done, info = step_out\n",
    "                truncated = False\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected env.step() output length: {len(step_out)}\")\n",
    "\n",
    "            truncated = False\n",
    "            ep_r += float(r)\n",
    "            steps += 1\n",
    "\n",
    "        rewards.append(ep_r)\n",
    "        lengths.append(steps)\n",
    "\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    lengths = np.array(lengths, dtype=np.float32)\n",
    "    return float(rewards.mean()), float(rewards.std()), float(lengths.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eCc8tf5CtVuA",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.129582Z",
     "iopub.status.busy": "2025-11-28T16:28:08.129262Z",
     "iopub.status.idle": "2025-11-28T16:28:08.146360Z",
     "shell.execute_reply": "2025-11-28T16:28:08.145791Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.129564Z"
    },
    "id": "eCc8tf5CtVuA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_noncurriculum(algo, stage, total_timesteps, seed, n_envs=1):\n",
    "    \"\"\"\n",
    "    Non-curriculum baseline:\n",
    "    - Train from scratch on a SINGLE stage for 'total_timesteps'.\n",
    "    - Eval during training on SAME env via PrettyEvalCallback (already set up).\n",
    "    - After training, eval on two held-out envs:\n",
    "        1) SCrRXO + medium traffic (MetaDriveEnv)\n",
    "        2) VaryingDynamicsEnv (randomized dynamics)\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = EXPERIMENT_ROOT / f\"{algo}/noncurriculum/seed_{seed}/{stage['name']}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Training NON-CURRICULUM: {algo} {stage['name']} seed {seed}\\n\")\n",
    "\n",
    "    use_discrete = (algo == \"DQN\")\n",
    "\n",
    "    # ---- training env (single vec env) ----\n",
    "    env = make_vec_env(stage, n_envs=n_envs, use_discrete=use_discrete, seed=seed)\n",
    "\n",
    "    model = make_model(algo, env, HYPERS[algo])\n",
    "\n",
    "    # in-training eval (same env)\n",
    "    eval_cb = PrettyEvalCallback(\n",
    "        env,\n",
    "        best_model_save_path=str(out_dir / \"best_model\"),\n",
    "        log_path=str(out_dir / \"eval\"),\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=EVAL_EPISODES,\n",
    "        deterministic=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    ckpt_cb = CheckpointCallback(\n",
    "        save_freq=EVAL_FREQ,\n",
    "        save_path=str(out_dir / \"checkpoints\"),\n",
    "        name_prefix=\"ckpt\",\n",
    "    )\n",
    "\n",
    "    metrics_csv = out_dir / \"metrics.csv\"\n",
    "    metrics_cb = MetricsCallback(\n",
    "        eval_env=env,       # since you’re using n_envs=1 (DummyVecEnv)\n",
    "        csv_path=str(metrics_csv),\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        eval_episodes=EVAL_EPISODES,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    save_run_config(out_dir, algo, stage, seed)\n",
    "\n",
    "    # ---- train ----\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=[eval_cb, ckpt_cb, metrics_cb],\n",
    "    )\n",
    "    model.save(str(out_dir / \"model.zip\"))\n",
    "\n",
    "    print(f\"\\nNon-curriculum training complete and saved to {out_dir}\")\n",
    "\n",
    "    # Close training vec env to avoid engine conflicts before new envs\n",
    "    env.close()\n",
    "\n",
    "    # =====================================================================\n",
    "    # HELD-OUT 1: SCrRXO + medium traffic\n",
    "    # =====================================================================\n",
    "    print(\"\\n[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\")\n",
    "\n",
    "    # build a *single* env instance using same wrappers\n",
    "    held1_make = make_metadrive_env(\n",
    "        HELDOUT_SCENARIO_STAGE,\n",
    "        use_discrete=use_discrete,\n",
    "        seed=seed + 1000,\n",
    "        render=False,\n",
    "    )\n",
    "    held1_env = held1_make()\n",
    "    held1_env = MetaDriveGymCompatibilityWrapper(held1_env)\n",
    "\n",
    "    h1_mean, h1_std, h1_len = evaluate_env_episodes( #, h1_len if manual function\n",
    "        model,\n",
    "        held1_env,\n",
    "        n_episodes=20,\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"HELD-OUT 1 (SCrRXO): mean_reward={h1_mean:.2f} ± {h1_std:.2f}, \"\n",
    "        f\"mean_ep_len={h1_len:.1f}\"\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(\n",
    "        [{\n",
    "            \"algo\": algo,\n",
    "            \"train_stage\": stage[\"name\"],\n",
    "            \"heldout_name\": HELDOUT_SCENARIO_STAGE[\"name\"],\n",
    "            \"mean_reward\": h1_mean,\n",
    "            \"std_reward\": h1_std,\n",
    "            \"mean_ep_len\": h1_len,\n",
    "        }]\n",
    "    ).to_csv(out_dir / \"heldout_scrrxo_metrics.csv\", index=False)\n",
    "\n",
    "    held1_env.close()\n",
    "\n",
    "    # =====================================================================\n",
    "    # HELD-OUT 2: VaryingDynamicsEnv\n",
    "    # =====================================================================\n",
    "    # print(\"\\n[HELD-OUT 2] Evaluating on VaryingDynamicsEnv (randomized dynamics)...\")\n",
    "\n",
    "    # # build varying dynamics env\n",
    "    # vd_env = VaryingDynamicsEnv(VARYING_DYNAMICS_CONFIG)\n",
    "    # vd_env = MetaDriveGymCompatibilityWrapper(vd_env)\n",
    "\n",
    "    # if use_discrete:\n",
    "    #     # same discrete mapping you used for training DQN\n",
    "    #     discrete_mapping = [(-1.0, 0.0), (-1.0, 0.3), (0.0, 0.5), (1.0, 0.3), (1.0, 0.0)]\n",
    "    #     vd_env = DiscreteActionWrapper(vd_env, discrete_mapping)\n",
    "\n",
    "    # h2_mean, h2_std, h2_len = evaluate_env_episodes(\n",
    "    #     model,\n",
    "    #     vd_env,\n",
    "    #     n_episodes=20,\n",
    "    #     deterministic=True,\n",
    "    # )\n",
    "\n",
    "    # print(\n",
    "    #     f\"HELD-OUT 2 (VaryingDynamics): mean_reward={h2_mean:.2f} ± {h2_std:.2f}, \"\n",
    "    #     f\"mean_ep_len={h2_len:.1f}\"\n",
    "    # )\n",
    "\n",
    "    # pd.DataFrame(\n",
    "    #     [{\n",
    "    #         \"algo\": algo,\n",
    "    #         \"train_stage\": stage[\"name\"],\n",
    "    #         \"heldout_name\": \"VaryingDynamicsEnv\",\n",
    "    #         \"mean_reward\": h2_mean,\n",
    "    #         \"std_reward\": h2_std,\n",
    "    #         \"mean_ep_len\": h2_len,\n",
    "    #     }]\n",
    "    # ).to_csv(out_dir / \"heldout_varying_metrics.csv\", index=False)\n",
    "\n",
    "    # vd_env.close()\n",
    "\n",
    "    return out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83889cd2",
   "metadata": {
    "id": "83889cd2"
   },
   "source": [
    "### 4.4. Visualization (plot metrics, learning curves, and display videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57e24184",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.147366Z",
     "iopub.status.busy": "2025-11-28T16:28:08.147110Z",
     "iopub.status.idle": "2025-11-28T16:28:08.166238Z",
     "shell.execute_reply": "2025-11-28T16:28:08.165643Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.147342Z"
    },
    "id": "57e24184",
    "outputId": "f68ec532-040e-4f16-eb9a-e35e5fb4a11f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot helper ready\n"
     ]
    }
   ],
   "source": [
    "def plot_metrics(csv_path, title=None):\n",
    "    if not os.path.exists(csv_path):\n",
    "        print('CSV not found:', csv_path); return\n",
    "    df = pd.read_csv(csv_path)\n",
    "    fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
    "    axs = axs.flatten()\n",
    "    axs[0].plot(df['total_timesteps'], df['mean_reward'], marker='o'); axs[0].set_title('Mean reward')\n",
    "    axs[1].plot(df['total_timesteps'], df['success_rate'], marker='o'); axs[1].set_title('Success rate')\n",
    "    axs[2].plot(df['total_timesteps'], df['collision_rate'], marker='o'); axs[2].set_title('Collision rate')\n",
    "    axs[3].plot(df['total_timesteps'], df['avg_speed'], marker='o'); axs[3].set_title('Avg speed')\n",
    "    if title: fig.suptitle(title)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "print('Plot helper ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6e5f6",
   "metadata": {
    "id": "95c6e5f6"
   },
   "source": [
    "## 5. Full experiment\n",
    "\n",
    "For each algorithm, for each seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xPZ5O3NLHg94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-28T16:28:08.182587Z",
     "iopub.status.busy": "2025-11-28T16:28:08.182256Z",
     "iopub.status.idle": "2025-11-28T22:07:09.146921Z",
     "shell.execute_reply": "2025-11-28T22:07:09.146243Z",
     "shell.execute_reply.started": "2025-11-28T16:28:08.182559Z"
    },
    "id": "xPZ5O3NLHg94",
    "outputId": "83ed71a9-d8a4-4a32-ad49-001cbe625eaf",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ALGO=SAC  SEED=0\n",
      "Training NON-CURRICULUM: SAC C0_Straight seed 0\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Eval num_timesteps=10000, episode_reward=10.08 +/- 0.00\n",
      "Episode length: 119.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-12.81 +/- 0.00\n",
      "Episode length: 941.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-11.19 +/- 0.00\n",
      "Episode length: 325.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-3.19 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-3.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-3.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-3.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-0.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=93.99 +/- 0.00\n",
      "Episode length: 155.00 +/- 0.00\n",
      "New best mean reward!\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "Non-curriculum training complete and saved to experiments/SAC/noncurriculum/seed_0/C0_Straight\n",
      "\n",
      "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
      "HELD-OUT 1 (SCrRXO): mean_reward=19.02 ± 0.00, mean_ep_len=97.0\n",
      "Training NON-CURRICULUM: SAC C1_Roundabout seed 0\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Eval num_timesteps=10000, episode_reward=-5.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-8.42 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-7.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-7.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-14.77 +/- 0.00\n",
      "Episode length: 329.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-10.64 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-11.61 +/- 0.00\n",
      "Episode length: 309.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-4.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-7.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-3.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-2.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-3.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-4.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "Non-curriculum training complete and saved to experiments/SAC/noncurriculum/seed_0/C1_Roundabout\n",
      "\n",
      "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
      "HELD-OUT 1 (SCrRXO): mean_reward=-18.55 ± 0.00, mean_ep_len=24.0\n",
      "Training NON-CURRICULUM: SAC C2_LightTraffic seed 0\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Eval num_timesteps=10000, episode_reward=-10.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-12.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-12.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-12.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-12.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-12.98 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-14.54 +/- 0.00\n",
      "Episode length: 79.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-18.60 +/- 0.00\n",
      "Episode length: 278.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-12.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-9.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-9.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-9.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-9.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-9.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-9.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-9.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-9.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-9.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-9.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-9.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "Non-curriculum training complete and saved to experiments/SAC/noncurriculum/seed_0/C2_LightTraffic\n",
      "\n",
      "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
      "HELD-OUT 1 (SCrRXO): mean_reward=-49.96 ± 0.00, mean_ep_len=1000.0\n",
      "Training NON-CURRICULUM: SAC C3_DenseTraffic seed 0\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Eval num_timesteps=10000, episode_reward=-8.38 +/- 0.00\n",
      "Episode length: 53.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-52.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-53.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-52.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-49.98 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-49.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-49.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-49.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-49.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-49.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-49.71 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-52.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-52.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-52.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-52.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-50.79 +/- 0.00\n",
      "Episode length: 634.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-70.87 +/- 0.02\n",
      "Episode length: 972.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-49.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-18.01 +/- 0.00\n",
      "Episode length: 37.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-23.17 +/- 0.00\n",
      "Episode length: 126.00 +/- 0.00\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "Non-curriculum training complete and saved to experiments/SAC/noncurriculum/seed_0/C3_DenseTraffic\n",
      "\n",
      "[HELD-OUT 1] Evaluating on SCrRXO (fixed 6-block) with medium traffic...\n",
      "HELD-OUT 1 (SCrRXO): mean_reward=-18.92 ± 0.00, mean_ep_len=31.0\n"
     ]
    }
   ],
   "source": [
    "algos = [\"SAC\"] # \"SAC\", \"DQN\",\n",
    "\n",
    "for algo in algos:\n",
    "    for seed in SEEDS:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ALGO={algo}  SEED={seed}\")\n",
    "\n",
    "        for stage in STAGES:\n",
    "            train_noncurriculum(\n",
    "                algo=algo,\n",
    "                stage=stage,\n",
    "                total_timesteps=stage[\"budget\"],  # use this stage's budget\n",
    "                seed=seed,\n",
    "                n_envs=N_ENVS,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qpsW4W5NI9LG",
   "metadata": {
    "id": "qpsW4W5NI9LG"
   },
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "-j-jWQJJgf8k",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T22:07:09.152646Z",
     "iopub.status.busy": "2025-11-28T22:07:09.152380Z",
     "iopub.status.idle": "2025-11-28T22:07:09.174147Z",
     "shell.execute_reply": "2025-11-28T22:07:09.173470Z",
     "shell.execute_reply.started": "2025-11-28T22:07:09.152629Z"
    },
    "id": "-j-jWQJJgf8k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Adjust if your root is named differently\n",
    "EXPERIMENT_ROOT = Path(\"experiments\")\n",
    "\n",
    "CURVES_DIR = EXPERIMENT_ROOT / \"curves\"\n",
    "CURVES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_eval_npz(eval_npz_path: Path):\n",
    "    \"\"\"\n",
    "    Load SB3 EvalCallback npz.\n",
    "    Returns: timesteps, mean_rewards, std_rewards, mean_ep_len\n",
    "    \"\"\"\n",
    "    data = np.load(eval_npz_path)\n",
    "    timesteps = data[\"timesteps\"].flatten()          # [n_eval]\n",
    "    results = data[\"results\"]                        # [n_eval, n_episodes]\n",
    "    ep_lengths = data[\"ep_lengths\"]                  # [n_eval, n_episodes]\n",
    "\n",
    "    mean_rewards = results.mean(axis=1)\n",
    "    std_rewards = results.std(axis=1)\n",
    "    mean_ep_len = ep_lengths.mean(axis=1)\n",
    "\n",
    "    return timesteps, mean_rewards, std_rewards, mean_ep_len\n",
    "\n",
    "\n",
    "def plot_stage_curves(algo, seed_name, stage_name, eval_npz_path):\n",
    "    \"\"\"\n",
    "    Make per-stage plots:\n",
    "      - reward vs timesteps\n",
    "      - episode length vs timesteps\n",
    "    Save to experiments/curves\n",
    "    \"\"\"\n",
    "    t, mean_r, std_r, mean_len = load_eval_npz(eval_npz_path)\n",
    "\n",
    "    # 1) Reward curve\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(t, mean_r, marker=\"o\")\n",
    "    plt.fill_between(t, mean_r - std_r, mean_r + std_r, alpha=0.2)\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Eval mean return\")\n",
    "    plt.title(f\"{algo} {seed_name} – {stage_name} (eval reward)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_reward.png\"\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Episode length curve\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(t, mean_len, marker=\"o\")\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Mean episode length\")\n",
    "    plt.title(f\"{algo} {seed_name} – {stage_name} (episode length)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_{stage_name}_ep_len.png\"\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_combined_curves(algo, seed_name, stage_to_npz):\n",
    "    \"\"\"\n",
    "    Combined plots across stages for one algo+seed:\n",
    "      - all reward curves\n",
    "      - all episode length curves\n",
    "    stage_to_npz: dict {stage_name: eval_npz_path}\n",
    "    \"\"\"\n",
    "    # Reward\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for stage_name, npz_path in stage_to_npz.items():\n",
    "        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n",
    "        plt.plot(t, mean_r, marker=\"o\", label=stage_name)\n",
    "\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Eval mean return\")\n",
    "    plt.title(f\"{algo} {seed_name} – eval reward (all stages)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_reward.png\"\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Episode length\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for stage_name, npz_path in stage_to_npz.items():\n",
    "        t, mean_r, std_r, mean_len = load_eval_npz(npz_path)\n",
    "        plt.plot(t, mean_len, marker=\"o\", label=stage_name)\n",
    "\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Mean episode length\")\n",
    "    plt.title(f\"{algo} {seed_name} – episode length (all stages)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = CURVES_DIR / f\"{algo}_{seed_name}_ALL_ep_len.png\"\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_heldout_bars(algo, seed_name, seed_dir: Path):\n",
    "    \"\"\"\n",
    "    If held-out CSVs exist for this algo+seed, make bar plots:\n",
    "      - heldout_scrrxo_metrics.csv across stages\n",
    "      - heldout_varying_metrics.csv across stages\n",
    "    \"\"\"\n",
    "    # Find all stages under this seed dir\n",
    "    stage_dirs = [\n",
    "        d for d in seed_dir.iterdir()\n",
    "        if d.is_dir() and not d.name.startswith(\".\")\n",
    "    ]\n",
    "\n",
    "    # ----- Held-out 1: SCrRXO -----\n",
    "    rows = []\n",
    "    for sd in stage_dirs:\n",
    "        csv_path = sd / \"heldout_scrrxo_metrics.csv\"\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            if not df.empty:\n",
    "                r = df.iloc[0].to_dict()\n",
    "                r[\"stage\"] = sd.name\n",
    "                rows.append(r)\n",
    "\n",
    "    if rows:\n",
    "        df_scr = pd.DataFrame(rows)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(df_scr[\"stage\"], df_scr[\"mean_reward\"])\n",
    "        plt.xlabel(\"Training stage\")\n",
    "        plt.ylabel(\"Held-out mean reward\")\n",
    "        plt.title(f\"{algo} {seed_name} – Held-out SCrRXO performance\")\n",
    "        plt.grid(axis=\"y\")\n",
    "        plt.tight_layout()\n",
    "        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_SCrRXO.png\"\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    # ----- Held-out 2: VaryingDynamics -----\n",
    "    rows = []\n",
    "    for sd in stage_dirs:\n",
    "        csv_path = sd / \"heldout_varying_metrics.csv\"\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            if not df.empty:\n",
    "                r = df.iloc[0].to_dict()\n",
    "                r[\"stage\"] = sd.name\n",
    "                rows.append(r)\n",
    "\n",
    "    if rows:\n",
    "        df_vd = pd.DataFrame(rows)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(df_vd[\"stage\"], df_vd[\"mean_reward\"])\n",
    "        plt.xlabel(\"Training stage\")\n",
    "        plt.ylabel(\"Held-out mean reward\")\n",
    "        plt.title(f\"{algo} {seed_name} – Held-out VaryingDynamics performance\")\n",
    "        plt.grid(axis=\"y\")\n",
    "        plt.tight_layout()\n",
    "        out_path = CURVES_DIR / f\"{algo}_{seed_name}_heldout_VaryingDynamics.png\"\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "vHiU4aOEj-7z",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T22:07:09.175573Z",
     "iopub.status.busy": "2025-11-28T22:07:09.174897Z",
     "iopub.status.idle": "2025-11-28T22:07:11.318812Z",
     "shell.execute_reply": "2025-11-28T22:07:11.318041Z",
     "shell.execute_reply.started": "2025-11-28T22:07:09.175556Z"
    },
    "id": "vHiU4aOEj-7z",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SAC / seed_0 ...\n",
      "All curves saved under: experiments/curves\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# MAIN: walk experiments/ and generate all possible plots\n",
    "# ======================================================================\n",
    "\n",
    "for algo_dir in EXPERIMENT_ROOT.iterdir():\n",
    "    if not algo_dir.is_dir():\n",
    "        continue\n",
    "    algo = algo_dir.name  # e.g. \"PPO\", \"SAC\", \"DQN\"\n",
    "\n",
    "    noncurr_dir = algo_dir / \"noncurriculum\"\n",
    "    if not noncurr_dir.exists():\n",
    "        continue\n",
    "\n",
    "    for seed_dir in noncurr_dir.iterdir():\n",
    "        if not seed_dir.is_dir() or not seed_dir.name.startswith(\"seed_\"):\n",
    "            continue\n",
    "        seed_name = seed_dir.name  # e.g. \"seed_0\"\n",
    "\n",
    "        print(f\"Processing {algo} / {seed_name} ...\")\n",
    "\n",
    "        # collect per-stage npz paths for combined plots\n",
    "        stage_to_npz = {}\n",
    "\n",
    "        # per-stage plots\n",
    "        for stage_dir in seed_dir.iterdir():\n",
    "            if not stage_dir.is_dir():\n",
    "                continue\n",
    "            stage_name = stage_dir.name  # e.g. \"C0_Straight\"\n",
    "\n",
    "            eval_npz_path = stage_dir / \"eval\" / \"evaluations.npz\"\n",
    "            if not eval_npz_path.exists():\n",
    "                continue\n",
    "\n",
    "            # individual plots\n",
    "            plot_stage_curves(algo, seed_name, stage_name, eval_npz_path)\n",
    "            stage_to_npz[stage_name] = eval_npz_path\n",
    "\n",
    "        # combined curves across stages\n",
    "        if stage_to_npz:\n",
    "            plot_combined_curves(algo, seed_name, stage_to_npz)\n",
    "\n",
    "        # held-out bar plots\n",
    "        plot_heldout_bars(algo, seed_name, seed_dir)\n",
    "\n",
    "print(\"All curves saved under:\", CURVES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sZWIeaOMj_mO",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T22:07:11.320420Z",
     "iopub.status.busy": "2025-11-28T22:07:11.319707Z",
     "iopub.status.idle": "2025-11-28T22:07:17.715820Z",
     "shell.execute_reply": "2025-11-28T22:07:17.715116Z",
     "shell.execute_reply.started": "2025-11-28T22:07:11.320400Z"
    },
    "id": "sZWIeaOMj_mO",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing stage: SAC_1\n",
      "Saved all curves for SAC_1 → experiments/tensorboard/SAC_1/curves\n",
      "\n",
      "Processing stage: SAC_2\n",
      "Saved all curves for SAC_2 → experiments/tensorboard/SAC_2/curves\n",
      "\n",
      "Processing stage: SAC_3\n",
      "Saved all curves for SAC_3 → experiments/tensorboard/SAC_3/curves\n",
      "\n",
      "Processing stage: SAC_4\n",
      "Saved all curves for SAC_4 → experiments/tensorboard/SAC_4/curves\n",
      "\n",
      "Finished generating all plots.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"experiments/tensorboard\")\n",
    "\n",
    "def load_tb_scalars(event_file):\n",
    "    event_file = str(event_file)   # <--- FIX HERE\n",
    "    ea = EventAccumulator(event_file)\n",
    "    ea.Reload()\n",
    "    scalars = {}\n",
    "    for tag in ea.Tags()[\"scalars\"]:\n",
    "        events = ea.Scalars(tag)\n",
    "        steps = [e.step for e in events]\n",
    "        values = [e.value for e in events]\n",
    "        scalars[tag] = (steps, values)\n",
    "    return scalars\n",
    "\n",
    "\n",
    "def plot_scalar(steps, values, title, out_path):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(steps, values)\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(title)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_stage(stage_path):\n",
    "    print(f\"\\nProcessing stage: {stage_path.name}\")\n",
    "    curves_dir = stage_path / \"curves\"\n",
    "    curves_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Find tensorboard event file\n",
    "    event_files = list(stage_path.glob(\"**/events.out.tfevents.*\"))\n",
    "    if len(event_files)==0:\n",
    "        print(\"No TB file found for eval.\")\n",
    "        return\n",
    "    tb_file = event_files[0]\n",
    "    tb_scalars = load_tb_scalars(tb_file)\n",
    "\n",
    "    # Plot all general scalars\n",
    "    for tag, (steps, values) in tb_scalars.items():\n",
    "        clean_name = tag.replace(\"/\", \"_\")\n",
    "        out = curves_dir / f\"{clean_name}.png\"\n",
    "        plot_scalar(steps, values, f\"{stage_path.name}: {tag}\", out)\n",
    "\n",
    "    print(f\"Saved all curves for {stage_path.name} → {curves_dir}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# PROCESS EACH STAGE\n",
    "# =============================\n",
    "all_stage_dirs = sorted([p for p in BASE.glob(\"*\") if p.is_dir()])\n",
    "\n",
    "for stage_dir in all_stage_dirs:\n",
    "    plot_stage(stage_dir)\n",
    "\n",
    "print(\"\\nFinished generating all plots.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "i0oLMRwtFbhQ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T22:07:17.716859Z",
     "iopub.status.busy": "2025-11-28T22:07:17.716565Z",
     "iopub.status.idle": "2025-11-28T22:07:23.299438Z",
     "shell.execute_reply": "2025-11-28T22:07:23.298624Z",
     "shell.execute_reply.started": "2025-11-28T22:07:17.716831Z"
    },
    "id": "i0oLMRwtFbhQ",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped to experiments_SAC.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"experiments_SAC\", \"zip\", \"experiments\")\n",
    "\n",
    "print(\"Zipped to experiments_SAC.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aYGcHpMI02D4",
   "metadata": {
    "id": "aYGcHpMI02D4",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qpsW4W5NI9LG"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
